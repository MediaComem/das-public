<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN" "JATS-archivearticle1-mathml3.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">J Biomed Opt</journal-id><journal-id journal-id-type="iso-abbrev">J Biomed Opt</journal-id><journal-id journal-id-type="coden">JBOPFO</journal-id><journal-id journal-id-type="publisher-id">JBO</journal-id><journal-title-group><journal-title>Journal of Biomedical Optics</journal-title></journal-title-group><issn pub-type="ppub">1083-3668</issn><issn pub-type="epub">1560-2281</issn><publisher><publisher-name>Society of Photo-Optical Instrumentation Engineers</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">31617336</article-id><article-id pub-id-type="pmc">PMC7000911</article-id><article-id pub-id-type="doi">10.1117/1.JBO.24.10.106501</article-id><article-id pub-id-type="publisher-manuscript">JBO-190163RR</article-id><article-id pub-id-type="publisher-id">190163RR</article-id><article-categories><subj-group subj-group-type="heading"><subject>Microscopy</subject></subj-group><subj-group subj-group-type="SPIE-art-type"><subject>Paper</subject></subj-group></article-categories><title-group><article-title>Digitalization versus immersion: performance and subjective evaluation of 3D perception with emulated accommodation and parallax in digital microsurgery</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-3437-6711</contrib-id><name><surname>Wahl</surname><given-names>Siegfried</given-names></name><xref ref-type="aff" rid="aff1">a</xref><xref ref-type="aff" rid="aff2">b</xref><email>Siegfried. Wahl@uni-tuebingen.de</email></contrib><contrib contrib-type="author"><name><surname>Dragneva</surname><given-names>Denitsa</given-names></name><xref ref-type="aff" rid="aff1">a</xref><email>denitsa.dragneva@googlemail.com</email></contrib><contrib contrib-type="author"><name><surname>Rifai</surname><given-names>Katharina</given-names></name><xref ref-type="aff" rid="aff1">a</xref><xref ref-type="aff" rid="aff2">b</xref><xref ref-type="corresp" rid="cor1">*</xref><email>katharina.rifai@zeiss.com</email></contrib><aff id="aff1"><label>a</label><institution>University Eye Clinics</institution>, Institute for Ophthalmic Research, Tuebingen, <country>Germany</country></aff><aff id="aff2"><label>b</label><institution>Carl Zeiss Vision International GmbH</institution>, Aalen, <country>Germany</country></aff></contrib-group><author-notes><corresp id="cor1"><label>*</label>Address all correspondence to Katharina Rifai, E-mail: <email>katharina.rifai@medizin.uni-tuebingen.de</email></corresp></author-notes><pub-date pub-type="epub"><day>15</day><month>10</month><year>2019</year></pub-date><pub-date pub-type="ppub"><month>10</month><year>2019</year></pub-date><pub-date pub-type="pmc-release"><day>15</day><month>10</month><year>2019</year></pub-date><!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. --><volume>24</volume><issue>10</issue><elocation-id>106501</elocation-id><history><date date-type="received"><day>20</day><month>5</month><year>2019</year></date><date date-type="accepted"><day>12</day><month>9</month><year>2019</year></date></history><permissions><copyright-statement>&#x000a9; The Authors. Published by SPIE under a Creative Commons Attribution 4.0 Unported License. Distribution or reproduction of this work in whole or in part requires full attribution of the original publication, including its DOI.</copyright-statement><copyright-holder>The Authors</copyright-holder></permissions><self-uri xlink:title="pdf" xlink:href="JBO_24_10_106501.pdf"/><abstract><title>Abstract.</title><p>In the visually challenging situation of microsurgery with many altered depth cues, digitalization of surgical systems disrupts two further depth cues, namely focus and parallax. Although in purely optical surgical systems accommodation and eye movements induce expected focus and parallax changes, they become statically fixed through digitalization. Our study evaluates the impact of static focus and parallax onto performance and subjective 3D perception. Subjects reported decreased depth realism under static parallax and focus. Thus surgeons&#x02019; depth perception is impacted further through digitalization of microsurgery, increasing the potential of artificial stereo-induced fatigue.</p></abstract><kwd-group><title>Keywords:</title><kwd>microsurgery</kwd><kwd>depth perception</kwd><kwd>digitalization</kwd><kwd>stereo images</kwd></kwd-group><funding-group><award-group id="sp1"><funding-source>BMBF</funding-source><award-id>FKZ 01GQ1002</award-id></award-group></funding-group><counts><fig-count count="4"/><table-count count="1"/><ref-count count="47"/><page-count count="7"/></counts><custom-meta-group><custom-meta><meta-name>running-head</meta-name><meta-value>Wahl, Dragneva, and Rifai: Digitalization versus immersion: performance and subjective evaluation of 3D perception&#x02026;</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="sec1"><label>1</label><title>Introduction</title><p>The work of a surgeon is one of the most challenging ones in today&#x02019;s professional world: high-cognitive load combines with high-acuity manual work over a long period of time. Technological advances have shaped the surgery room to a highly specialized surrounding, impacting the surgeon&#x02019;s perception as well as action in performed surgery. Specifically, high-precision surgery has advanced to a new level by the application of stereo surgical microscopes. Nonetheless, stereo microscopes impose challenges on the visual system, specifically in depth perception. Depth perception is known to make use of a variety of available information, called depth cues.<xref rid="r1" ref-type="bibr"><sup>1</sup></xref><sup>,</sup><xref rid="r2" ref-type="bibr"><sup>2</sup></xref> The most obvious, and in many cases dominant information, is the relative differences in the images from the two eyes, called binocular disparity.<xref rid="r3" ref-type="bibr"><sup>3</sup></xref> In addition, properties of the external world, containing information of the spatial arrangement of objects in space, such as shading, familiar size, occlusion, spatial frequency statistics, and others have been shown to contribute as well.<xref rid="r4" ref-type="bibr"><sup>4</sup></xref> Even the consequences of the optical transformation of the image onto the retina, by the optics of the eye, are used as depth cues: focus cues of blur and depth of focus.<xref rid="r5" ref-type="bibr"><sup>5</sup></xref><named-content content-type="online"><xref rid="r6" ref-type="bibr"/></named-content><named-content content-type="print"><sup>&#x02013;</sup></named-content><xref rid="r7" ref-type="bibr"><sup>7</sup></xref> Stereo vision in microscopes differs from natural vision in several aspects, and these differences affect a variety of depth cues. Differences depend on the optical design of the microscope, and in this article, common main objective (CMO) microscopes are considered. In stereo microscopes, the magnification of an observed object changes depth of focus compared to the unmagnified image and affects contrast content. Both image properties are known depth cues.<xref rid="r5" ref-type="bibr"><sup>5</sup></xref><sup>,</sup><xref rid="r7" ref-type="bibr"><sup>7</sup></xref><named-content content-type="online"><xref rid="r8" ref-type="bibr"/></named-content><named-content content-type="print"><sup>&#x02013;</sup></named-content><xref rid="r9" ref-type="bibr"><sup>9</sup></xref> Furthermore, viewing geometry, accommodation distance, and disparity are set by the optical design of the stereo microscope. CMO microscopes are designed for a parallel gaze, and focus planes are often set to infinity.<xref rid="r10" ref-type="bibr"><sup>10</sup></xref> In CMO microscopes, image projection geometry is disentangled from gaze geometry. Projection geometry is typically designed to have a viewing angle of typically 10 to 12&#x000a0;deg, leading to a disparity-based perceived distance of approximately arm distance.<xref rid="r10" ref-type="bibr"><sup>10</sup></xref> Thus in stereo microscopes, many of the three-dimensional (3D) cues differ from natural 3D vision in a purely optical configuration, leading to fatigue and visual strain.<xref rid="r11" ref-type="bibr"><sup>11</sup></xref> Fatigue induced by artificial stereo images is considered a consequence of the efficiency of the stereo system; even largely inaccurate stereo cues still lead to 3D perception.<xref rid="r12" ref-type="bibr"><sup>12</sup></xref> Evidently, 3D perception, despite erroneous image input, comes with the cost of an increased processing load, leading to fatigue.<xref rid="r9" ref-type="bibr"><sup>9</sup></xref><sup>,</sup><xref rid="r13" ref-type="bibr"><sup>13</sup></xref><named-content content-type="online"><xref rid="r14" ref-type="bibr"/><xref rid="r15" ref-type="bibr"/><xref rid="r16" ref-type="bibr"/><xref rid="r17" ref-type="bibr"/><xref rid="r18" ref-type="bibr"/><xref rid="r19" ref-type="bibr"/><xref rid="r20" ref-type="bibr"/><xref rid="r21" ref-type="bibr"/></named-content><named-content content-type="print"><sup>&#x02013;</sup></named-content><xref rid="r22" ref-type="bibr"><sup>22</sup></xref> Recent advances in digitalization in the medical sector show a new disruptive technology on the horizon of microsurgery: digital surgical microscopy. With its numerous advantages in data storage, data analysis, and augmentation, it has the potential to bring microsurgery to a new level of performance. Nonetheless, it challenges the surgeon&#x02019;s perceptual system by presenting errant focus and parallax cues. Visual information reaching an observer&#x02019;s retina in natural vision contains the fingerprint of the eye&#x02019;s status in both cues: the observer&#x02019;s accommodation shifts the focus plane, and eye and head movements induce parallax cues.<xref rid="r5" ref-type="bibr"><sup>5</sup></xref><sup>,</sup><xref rid="r23" ref-type="bibr"><sup>23</sup></xref><sup>,</sup><xref rid="r24" ref-type="bibr"><sup>24</sup></xref> Whereas in an optical microscope, accommodation of the surgeon&#x02019;s eye leads to a corresponding shift of the focus plane, in a digital stereo system the focus plane is predefined. Thus a change in accommodation is not accompanied with a corresponding change of the focus plane in the image any more. Furthermore, image parallax through either small head movements in front of an eyepiece, or even through eye movements, is disabled. The recording geometry is statically fixed in a standard digital stereo geometry. Thus additional demands are imposed onto the visual 3D system through digitalization. This study intends to evaluate the impact of disabling motion parallax cues as well as the dynamics of focus cues through digitalization in a digital stereo microscope setup. A thorough evaluation of the two aspects requires a constancy of all other image and stereo properties. But, when comparing purely optical and digital surgical microscope systems, digitalization imposes additional image degradations: pixilation-induced reduction of resolution and limited contrast, as well as a changed color space. Thus in this study, these degradations were consistent across all conditions evaluated and the impact of focus and parallax cues were analyzed entirely in one identical digital system. To do so, a dynamic stereo microscope (DSM) of the CMO type is developed, which is able to mimic eye-induced parallax and focus changes in real time.<xref rid="r25" ref-type="bibr"><sup>25</sup></xref> In this DSM, eye tracking is exploited to estimate the momentarily viewed depth plane as well as to estimate viewing geometry, while image content is adjusted accordingly.<xref rid="r26" ref-type="bibr"><sup>26</sup></xref> Focus plane adjustments are implemented as focus-adjustable (Optotune AG, Dietikon, Switzerland) lens elements, and image projection changes are implemented through a moving aperture.<xref rid="r25" ref-type="bibr"><sup>25</sup></xref></p><p>With this DSM, behavioral performance as well as subjective 3D immersion are assessed in a standard static stereo arrangement (STAT), a focus (FOC), and a parallax (PAR) condition. Furthermore, a COMB (combined) condition estimates the cumulative effect of focus and parallax.</p></sec><sec id="sec2"><label>2</label><title>Materials and Methods</title><sec id="sec2.1"><label>2.1</label><title>Study Approval</title><p>The study was approved by the Ethics Committee of the Medical Faculty of the Eberhard Karls University of T&#x000fc;bingen and the University Hospital.</p></sec><sec id="sec2.2"><label>2.2</label><title>Subjects</title><p>Twenty right-handed, eye-healthy subjects, aged between 19 and 38 years participated in the study with a prior written consent in adherence to the declaration of Helsinki. All participants were na&#x000ef;ve to the purpose of the study; none had prior experience in microsurgery.</p></sec><sec id="sec2.3"><label>2.3</label><title>Experimental Setup</title><p>The experimental setup consisted of a line-stereo video system (TRENION 3D HD, Carl Zeiss Meditec AG, Jena, Germany) with the <inline-formula><mml:math id="math1"><mml:mrow><mml:mn>930</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>523</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1067</mml:mn><mml:mtext>&#x02009;&#x02009;</mml:mtext><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula> stereo monitor (LMD-4251TD, Sony Corporation, Minato, Tokio, Japan), positioned at a height of 123&#x000a0;cm, and a head-worn eye tracker Dikablis Professional Glasses (Ergoneers GmbH), both connected to a prototype DSM. The setup is depicted in <xref ref-type="fig" rid="f1">Fig.&#x000a0;1(a)</xref>. The subject was standing in front of the 3D monitor at a distance of 100&#x000a0;cm, wearing polarized glasses enabling line-stereo-based 3D vision, as well as the head-worn eye tracker. In the subject&#x02019;s right hand, forceps were placed. The DSM was positioned to the right of the monitor, at approximate working distance from a subject. It was ensured that all subjects were able to reach the task object comfortably.</p><fig id="f1" orientation="portrait" position="float"><label>Fig. 1</label><caption><p>(a)&#x000a0;Labeled display of the setup with the microscope breadboard on the right, together with the steering unit, the screen in the center, and system computers on the right. (b)&#x000a0;Sample images demonstrating focus shift and ocular parallax. Circles indicate selected spatial positions in which image changes through accommodation and parallax become specifically obvious. On the top of the parallax images, a 3D representation of the object shown in the parallax example is shown. (c)&#x000a0;Schematic representation of the principle of the DSM. Eye movements of a subject viewing the object under the DSM on the screen are recorded. Based on the gaze position, autofocus and parallax-inducing aperture are adjusted, which adjust screen content.</p></caption><graphic xlink:href="JBO-024-106501-g001"/></fig></sec><sec id="sec2.4"><label>2.4</label><title>Dynamic Stereo Microscope</title><p>The DSM incorporated movable optical components into a CMO microscope to imitate accommodation and ocular parallax in real time, both on the basis of eye-tracking-based estimations of the gaze position on the screen. The accommodation mimicking autofocus was implemented by tuneable lenses (EL-10-30-C, Optotune AG, Dietikon, Switzerland), ocular parallax was mimicked by a translating circular aperture, realized through linear actuators (LS2818L0604-T5x5-75, Nanotec Electronic GmbH &#x00026; Co. KG, Feldkirchen, Germany). The aperture selects a sub-bundle of the light to be imaged to the camera sensor, just like the ocular pupil would in natural vision. Thereby, the image projection of a 3D object onto the camera sensor changes as it would through eye movements, restoring ocular parallax.<xref rid="r23" ref-type="bibr"><sup>23</sup></xref>
<xref ref-type="fig" rid="f1">Figure&#x000a0;1(b)</xref> shows examples of according focus adjustment and ocular parallax. Gaze positions of both eyes on screen were recorded by the Dikablis hardware, combined with customwritten software (EyeRecToo).<xref rid="r27" ref-type="bibr"><sup>27</sup></xref> Upon saccades, according new gaze directions were estimated, and focus planes for each eye were drawn from a predefined depth map of the object. The software adjusted the lens focus and shifted the motorized stage accordingly, so that the adjusted image content appeared on screen 205&#x000a0;ms after each saccadic eye movement in the autofocus condition and 295&#x000a0;ms after shifting the motor stage. The delay was measured end-to-end by videorecording a blinking LED together with the videostream of the same LED on the DSM screen on one video. Synchronization of eyetracking data and command logs of the steering system allowed an isolation of autofocus delay and translation stage, which equalled on 95&#x000a0;ms for the translation stage and 185&#x000a0;ms for the autofocus, whereas 110&#x000a0;ms of the overall delay was attributed to the pure video delay, present in a static scenario as well. These adjustments were based on the gaze estimation of the eye tracker, as well as a predetermined depth map estimation of the scene. Eye movements were recorded with the Dikablis Professional Glasses (Ergoneers GmbH) in combination with the EyeRecToo, which also took care of the pupil detection. Further descriptions of the system are published by Fuhl et&#x000a0;al.,<xref rid="r25" ref-type="bibr"><sup>25</sup></xref> a schematic representation of the function principle is shown in <xref ref-type="fig" rid="f1">Fig.&#x000a0;1(c)</xref>.</p></sec><sec id="sec2.5"><label>2.5</label><title>Task Design</title><p>A standard task of high-accuracy manual interaction was developed. The task included the placement of spheres into a topographically challenging sphere holder, shown in <xref ref-type="fig" rid="f2">Fig.&#x000a0;2</xref>. The sphere holder included different depths to mimic the multiple depth plane nature of surgical tasks. It was designed in Autodesk 123D Design (Autodesk, Inc., San Rafael) and 3D printed on an Ultimaker2 (Ultimaker, Geldermalsen, Netherlands) out of black polylactic acid. The 3D object for the haptic task contained 12 pockets of 3.4-mm diameter with a circular opening at 12 different planes, arranged in a <inline-formula><mml:math id="math2"><mml:mrow><mml:mn>4</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> grid. During the haptic task participants were asked to place small spheres out of white polyoxymethylen of the diameter of 3&#x000a0;mm (Hoch KG, Ha&#x000df;furt, Germany) into those pockets with the help of forceps. The pocket top surfaces were tilted by 20&#x000a0;deg in order to increase the difficulty of the manual task and enforce the use of visual information. It was ensured that the spheres fitted into the pockets comfortably, once placed correctly on top of the opening. With one attempt per pocket subjects were asked to fill the 12 pockets, row by row, from left to right starting in the bottom row, repeating the task, as often as possible, for 5&#x000a0;min. A sufficient supply of spheres was stored in the bowl-shaped socket of the 3D object.</p><fig id="f2" orientation="portrait" position="float"><label>Fig. 2</label><caption><p>Topographically challenging sphere holder for the manual task, together with measurement readings, and a subject&#x02019;s hand holding the forceps. On the bottom of the holder, a bowl-shaped socket for sphere supply is visible.</p></caption><graphic xlink:href="JBO-024-106501-g002"/></fig></sec><sec id="sec2.6"><label>2.6</label><title>Experimental Procedure</title><p>Prior to the experiment, subjects were introduced to the microscope and the task. After familiarization, subjects were positioned standing in front of the monitor at a distance of 100&#x000a0;cm and equipped with polarized 3D glasses and an eye-tracker on top. Viewing was binocular. Subsequently, a custom-made eye tracking calibration was performed, in which a calibration marker (ArUco) was moved over the screen.<xref rid="r28" ref-type="bibr"><sup>28</sup></xref> Subjects followed the marker with their gaze, and calibration was performed through the scene camera of the head-worn eye tracker. Calibration accuracy was tested in four reference positions in the corners of the microscope image. Thereafter, the training phase started. Participants were instructed on how to place the spheres, as well as the procedure of the task. Before the task was started, subjects were trained the task for 5&#x000a0;min to reach an initial saturation level of training effects of the manual task. The task started when the subjects had the forceps in the field of view of the microscope. After that they put as many spheres in the pockets as possible within 5&#x000a0;min. The number of spheres successfully placed was counted thereafter. At the end of each task, a task-specific questionnaire, aiming to evaluate their experience, was filled out by the participants (see <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1117/1.JBO.24.10.106501.s01">Supplemental Fig.&#x000a0;S1</ext-link>). Four separate conditions, consisting of three enhanced conditions (FOC, PAR, and COMB), as well as the static control condition (STAT) were tested in a random order on separate days, with one condition tested per day. In the STAT condition, variable focus and variable parallax were disabled, the focus plane was set to the average level of pocket openings, so that the highest and lowest pockets were blurred. In the FOC and the COMB conditions, the variable focus was activated, so that the system gaze-contingently kept the focus at fixation. In the PAR and COMB conditions, the moving aperture of the system mimicked the gaze direction changes induced through eye movements, and thus successfully imitated eye movement-induced parallax.</p></sec><sec id="sec2.7"><label>2.7</label><title>Data Analysis</title><p>Separately for each condition, task performance was estimated based on the number of spheres successfully placed within the 5&#x000a0;min. Average task performance was estimated in spheres per minute. In the questionnaire, different perceptual properties were evaluated with separate sets of questions: general perception, interaction, immersion, and depth. For details, refer to <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1117/1.JBO.24.10.106501.s01">Supplemental Fig.&#x000a0;S1</ext-link>. In each question, subjects rated a specific aspect on a Likert-type scale of 21 discrete steps. Individual questionnaire ratings were collected for each condition. Ratings were normalized to the maximum rating.</p><p>Data analysis of performance as well as questionnaire data focused on the difference of the three enhanced conditions (PAR, FOC, and COMB) to the baseline condition (STAT). Thus in the following, performance and ratings of any of the enhanced conditions are subtracted from condition STAT. As first step, an analysis of variance (ANOVA) is performed on relative performance changes as well as changes questionnaire ratings relative to baseline (STAT). Thereafter, in separate tests, the difference of each of the three enhanced conditions (PAR, FOC, and COMB) to the STAT condition was tested with student&#x02019;s <inline-formula><mml:math id="math3"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>-test.</p></sec></sec><sec id="sec3"><label>3</label><title>Results</title><sec id="sec3.1"><label>3.1</label><title>Task Performance</title><p><xref ref-type="fig" rid="f3">Figure&#x000a0;3</xref> summarizes task performance in the four measured conditions; error bars show standard errors. Average task performance is approximately three spheres per minute. An ANOVA indicated no significant differences in performance [<inline-formula><mml:math id="math4"><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>59</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1.3698</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="math5"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.26</mml:mn></mml:mrow></mml:math></inline-formula>]. When comparing performance between the different sockets of the object, no distinct pattern occurred. Thus there does not seem to be a large behavioral difference between the conditions.</p><fig id="f3" orientation="portrait" position="float"><label>Fig. 3</label><caption><p>Manual task performance in the four conditions: STAT, FOC, PAR, and COMB. Error bars indicate standard errors.</p></caption><graphic xlink:href="JBO-024-106501-g003"/></fig></sec><sec id="sec3.2"><label>3.2</label><title>Subjective Evaluation</title><p>In the next step, subjective ratings were evaluated. In <xref ref-type="fig" rid="f4">Fig.&#x000a0;4</xref>, questions were grouped into three categories &#x0201c;interaction,&#x0201d; &#x0201c;immersion,&#x0201d; and &#x0201c;depth.&#x0201d; Each subgraph shows average subjective ratings over all subjects, with zero indicating a maximally positive response, and one indicating a maximally negative response. Overall, enhancement conditions show significantly decreased rating scores, compared to the STAT condition in all fields: interaction, immersion, and depth. Decreased scores indicate a higher level of satisfaction. All but one of the questions showed significant differences in ratings to condition STAT in the ANOVA, results are summarized in <xref rid="t001" ref-type="table">Table&#x000a0;1</xref>. Solely the question related to immersion did not show a significant effect in the ANOVA, probably the term was difficult to interpret for the subjects [<inline-formula><mml:math id="math6"><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>2,59</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1.3171</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="math7"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2799</mml:mn></mml:mrow></mml:math></inline-formula>].</p><fig id="f4" orientation="portrait" position="float"><label>Fig. 4</label><caption><p>Subjective ratings of performance and perception, grouped into topics of interaction, immersion, and depth. Error bars indicate standard deviations. Significance levels are indicated by asterisks.</p></caption><graphic xlink:href="JBO-024-106501-g004"/></fig><table-wrap id="t001" orientation="portrait" position="float"><label>Table 1</label><caption><p>Summary of statistics of subjective evaluation.</p></caption><!--OASIS TABLE HERE--><table frame="hsides" rules="groups"><colgroup><col/><col/><col/><col/><col/><col/></colgroup><thead><tr><th rowspan="2" valign="top">Questions</th><th colspan="2" valign="top">ANOVA</th><th colspan="3" valign="top"><italic>Post hoc</italic><inline-formula><mml:math id="math8"><mml:mrow><mml:msub><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mi>STAT</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></th></tr><tr><th valign="top"><inline-formula><mml:math id="math9"><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>59</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula></th><th valign="top"><inline-formula><mml:math id="math10"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula></th><th valign="top">FOC</th><th valign="top">PAR</th><th valign="top">COMB</th></tr></thead><tbody><tr><td>Navigation</td><td>8.4487</td><td>0.0009</td><td>0.0019</td><td>0.4588</td><td>0.0035</td></tr><tr><td>Haptic task</td><td>6.6281</td><td>0.0034</td><td>0.0035</td><td>0.5390</td><td>0.0504</td></tr><tr><td>Immersion</td><td>1.3171</td><td>0.2799</td><td>&#x02014;</td><td>&#x02014;</td><td>&#x02014;</td></tr><tr><td>Realness</td><td>7.1601</td><td>0.0023</td><td>0.0001</td><td>0.0229</td><td>0.0014</td></tr><tr><td>Vividness</td><td>6.7238</td><td>0.0032</td><td>0.0036</td><td>0.2839</td><td>0.0034</td></tr><tr><td>Vividness depth</td><td>5.7316</td><td>0.0067</td><td>0.0001</td><td>0.0277</td><td>0.0008</td></tr><tr><td>Accurateness depth</td><td>6.7804</td><td>0.0030</td><td>0.0081</td><td>0.8141</td><td>0.0252</td></tr></tbody></table></table-wrap><p>In those questions with significantly different ratings, each of the conditions was tested against zero. Conditions FOC and COMB show significantly improved ratings in almost all questions, <inline-formula><mml:math id="math11"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula> values are summarized in <xref rid="t001" ref-type="table">Table&#x000a0;1</xref>. Thus subjects indicate that they found it easier to navigate in the FOC (<inline-formula><mml:math id="math12"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>FOC</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.0019</mml:mn></mml:mrow></mml:math></inline-formula>) and COMB (<inline-formula><mml:math id="math13"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>COMB</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.0035</mml:mn></mml:mrow></mml:math></inline-formula>) conditions. They rated the haptic task better in the FOC condition (<inline-formula><mml:math id="math14"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>FOC</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.0035</mml:mn></mml:mrow></mml:math></inline-formula>). The COMB condition did not reach significance (<inline-formula><mml:math id="math15"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>COMB</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.0504</mml:mn></mml:mrow></mml:math></inline-formula>), well matching with the fact, that performance did not increase significantly. They furthermore rated &#x0201c;realness&#x0201d; of the presented object (<inline-formula><mml:math id="math16"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>FOC</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.0001</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="math17"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>COMB</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.0014</mml:mn></mml:mrow></mml:math></inline-formula>) and vividness of presentation (<inline-formula><mml:math id="math18"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>FOC</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.0036</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="math19"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>COMB</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.0034</mml:mn></mml:mrow></mml:math></inline-formula>) significantly better. When asked about their subjective rating of depth perception, their vividness of depth (<inline-formula><mml:math id="math20"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>FOC</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.0001</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="math21"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>COMB</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.0008</mml:mn></mml:mrow></mml:math></inline-formula>), as well as accurateness in depth (<inline-formula><mml:math id="math22"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>FOC</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.0081</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="math23"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>COMB</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.0252</mml:mn></mml:mrow></mml:math></inline-formula>) significantly increased, as soon as the autofocus of the system was activated. But interestingly, also condition PAR showed significantly improved ratings in two questions, even though on much lower significance levels. Realness of the observed object was rated more positively in the PAR condition (<inline-formula><mml:math id="math24"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>PAR</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.0229</mml:mn></mml:mrow></mml:math></inline-formula>), and vividness of depth was rated significantly higher in the PAR condition (<inline-formula><mml:math id="math25"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>PAR</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.0277</mml:mn></mml:mrow></mml:math></inline-formula>).</p></sec></sec><sec id="sec4"><label>4</label><title>Discussion</title><p>This study demonstrates the positive contribution of focus cues and eye-movement-induced parallax cues in the perception of 3D content in a surgical microscope. In the FOC condition, in which the subject changed focus freely in the scene by accommodation, subjects rated interaction, immersion, and depth perception more positively. Furthermore, eye-movement-induced parallax cues played a role in depth perception as well. In sum, in surgical microscopes with static stereo, missing accommodation and parallax cues are likely responsible for suboptimal interaction, immersion, and depth perception.</p><sec id="sec4.1"><label>4.1</label><title>Limitations of the Study and Application to Microsurgery</title><p>The generation of a stereo microscope with autofocus and parallax restauration in hardware represented a challenging task, which came with imminent limitations, namely an unevitable presentation delay in the system used. Specifically, in the conditions which restored ocular parallax, delays of up to 295&#x000a0;ms occurred. These might explain decreased performance and ratings in the COMB condition, compared to the FOC condition where solely the autofocus was in operation. Furthermore, task performance estimation is limited in its sensitivity to changes in stereo perception. This becomes specifically evident in the parallax restoration condition, where subjective ratings do show improvements, but performance is not significantly improved. One reason might be that for the haptic interaction the increased delay upon restoration of ocular parallax plays a role in the PAR condition as well as in the COMB condition. But furthermore, the changes in subjective perception are more subtle compared to improvements induced by an autofocus. Thus haptic performance might just not have been affected by it significantly. Haptic performance depends on manual feedback and repetition learning. Thus a moderate degradation of visual depth estimation might not necessarily strongly affect accuracy of reaching in depth. A task design in which no feedback on the task is given as it is here through the successful placement of a sphere might increase accuracy. In that case, manual depth accuracy would have to be tracked by a high-accuracy tracking device. A further study limitation concerns the study cohort of subjects in a task. Thus task as well as haptic strategies significantly differ from the surgical situation. But this study does not intend to estimate the extent of degradation in accuracy in a surgical situation, it instead proves the existence of a degradation of stereo perception. This degradation is universal to any human system and can thus be assumed to be well transferable to the surgery situation. In a surgery situation, the accuracy of depth estimations needed might even be higher than in the described haptic task performed in this study, thus the impact of the observed mechanism is expected to be more prominent. An comparison of systems in a long-term evaluation in long-term use would thus reveal great insights into the actual burden imposed onto the surgeon.</p></sec><sec id="sec4.2"><label>4.2</label><title>Digitalization in Microscopes</title><p>Specifically in the surgical context, accurate perception is critical. But technical solutions to circumvent each of the digital stereo-induced inaccuracies are challenging and expensive.<xref rid="r29" ref-type="bibr"><sup>29</sup></xref> Furthermore, they seldomly restore more than one of the necessary depth cues. It is therefore useful, in the context of a stereo microscope, to classify alterations in depth cues according to their occurrence within the microscope and to enable an efficient search for technical solutions. In the following, we will therefore distinguish between image acquisition related depth cues and image presentation related depth cues.</p></sec><sec id="sec4.3"><label>4.3</label><title>Image Presentation-Related Depth Cues: Accommodation-Vergence Conflict</title><p>In most digital stereo systems, image presentation is bound to one specific surface at a fixed distance. Although stereo images encourage vergence changes within the image, based on relative parallax between image details at different depths, accommodation is fixed to a plane. The two properties are naturally coupled; thus a constant mismatch is known to induce discomfort, termed accommodation-vergence conflict.<xref rid="r15" ref-type="bibr"><sup>15</sup></xref><sup>,</sup><xref rid="r17" ref-type="bibr"><sup>17</sup></xref><sup>,</sup><xref rid="r30" ref-type="bibr"><sup>30</sup></xref><named-content content-type="online"><xref rid="r31" ref-type="bibr"/></named-content><named-content content-type="print"><sup>&#x02013;</sup></named-content><xref rid="r32" ref-type="bibr"><sup>32</sup></xref> Various technological concepts have been developed to allow digital content presentation, in which accommodation changes together with vergence.<xref rid="r29" ref-type="bibr"><sup>29</sup></xref> The approaches can roughly be classified into two groups &#x02013; concepts, in which information is presented in different depths in parallel, and concepts, in which the presentation plane is changed gaze-contingently. Both represent cost extensive expansions to the standard two plane arrangement and suffer from high technological complexity. At a close look, the accommodation-vergence conflict consists of two types of components: motor and visual components. On the motor side, a certain status of contraction of the ciliary muscles is connected to a certain degree of vergence. But furthermore, both lead to a specific footprint in the image, namely a specific image projection and a specific blur distribution. This study shows that already the gaze-correct visual presentation provides considerable benefit, a solution in which image projection and blur distribution match vergence, although accommodation is still bound to the monitor plane.</p></sec><sec id="sec4.4"><label>4.4</label><title>Image Acquisition-Related Depth Cues: Focus Cues and Temporal Relative Parallax</title><p>Thus important depth cue alterations through digitalization originate in the image acquisition. Digitalization differs from optical microscopes in the fact that the light experiences a digitalization in an interim image plane. The light field thus collapses to the image plane, and the achieved image is presented on a presentation device. In an optical surgical microscope, this image plane undergoes changes through the surgeon&#x02019;s eye, which is part of the optical system. Thus the main additional impact on depth cues through digitalization is the removal of relative changes of depth cues through eye movements and accommodation. Depth cues of absolute disparity, spatial frequency content, familiar size, and occlusions are already affected by the optical transformation, they thus impact the surgeon&#x02019;s 3D perception already in purely optical systems. But relative changes of focus cues through accommodation, as well as parallax cues through eye and small head movements are conserved in an optical stereo microscope, but abolished in a static digital stereo microscope. Thus 3D perception is impacted further through digitalization.</p></sec><sec id="sec4.5"><label>4.5</label><title>Focus Cues</title><p>The contribution of focus cues to 3D perception has been evaluated in detail.<xref rid="r5" ref-type="bibr"><sup>5</sup></xref><sup>,</sup><xref rid="r6" ref-type="bibr"><sup>6</sup></xref><sup>,</sup><xref rid="r9" ref-type="bibr"><sup>9</sup></xref><sup>,</sup><xref rid="r17" ref-type="bibr"><sup>17</sup></xref><sup>,</sup><xref rid="r33" ref-type="bibr"><sup>33</sup></xref><sup>,</sup><xref rid="r34" ref-type="bibr"><sup>34</sup></xref> Although suffering from ambiguity, a significant contribution of blur as depth cue has been shown.<xref rid="r5" ref-type="bibr"><sup>5</sup></xref><sup>,</sup><xref rid="r7" ref-type="bibr"><sup>7</sup></xref> In this study, specifically the fixation position induced change in focus cues has been shown to significantly contribute to realism of depth. Its dominant role in the surgical context might originate in two aspects. As mentioned above, the magnification disrupts a variety of depth cues, thus leading to a more fragile situation, in which the visual system must rely on remaining cues. Furthermore, in the current situation, correct focus cues equal the situation of an autofocus, meaning, the focus is always set to the fixated plane, allowing clear vision. The comparison situation of a fixed depth plane, providing blurred vision upon fixations in depth planes different from the set focus plane, is rather artificial and disturbing. But in the context of a digital stereo microscope, extending a stereo microscope with a static digital stereo would potentially become the new standard. Thus this study can be treated as a motivation to consider autofocus systems in digital microscopes.</p></sec><sec id="sec4.6"><label>4.6</label><title>Parallax Cues</title><p>Motion parallax is the only depth cue inherently defined in time. It describes the relative position change of objects at different distances upon global change of the environment, e.g., through movement of the observer. It has been demonstrated to be a rather strong depth cue.<xref rid="r35" ref-type="bibr"><sup>35</sup></xref> Parallax through eye movements is a rather subtle image change, and the system is not able to perceive the relative motion of the object&#x02019;s details at different depths directly through saccadic suppression. Nonetheless, it has been shown to function as depth cue.<xref rid="r23" ref-type="bibr"><sup>23</sup></xref> Specifically for near distances, a benefit of dynamic projection changes has been demonstrated in fusion times.<xref rid="r36" ref-type="bibr"><sup>36</sup></xref> The present subjective ratings of depth realism confirm that parallax as a depth cue provides a subjectively perceivable benefit.</p></sec><sec id="sec4.7"><label>4.7</label><title>Fatigue</title><p>Inaccuracies of stereo content as demonstrated in this study play a major role in 3D induced fatigue and 3D-induced discomfort.<xref rid="r37" ref-type="bibr"><sup>37</sup></xref> It can thus be assumed that decreased realism of 3D perception in static stereo images shows the potential to increase fatigue in digital surgical systems. Artificial stereo-induced fatigue has been measured extensively.<xref rid="r13" ref-type="bibr"><sup>13</sup></xref><sup>,</sup><xref rid="r17" ref-type="bibr"><sup>17</sup></xref><sup>,</sup><xref rid="r19" ref-type="bibr"><sup>19</sup></xref><sup>,</sup><xref rid="r22" ref-type="bibr"><sup>22</sup></xref><sup>,</sup><xref rid="r30" ref-type="bibr"><sup>30</sup></xref><sup>,</sup><xref rid="r31" ref-type="bibr"><sup>31</sup></xref><sup>,</sup><xref rid="r34" ref-type="bibr"><sup>34</sup></xref><sup>,</sup><xref rid="r37" ref-type="bibr"><sup>37</sup></xref><named-content content-type="online"><xref rid="r38" ref-type="bibr"/><xref rid="r39" ref-type="bibr"/><xref rid="r40" ref-type="bibr"/><xref rid="r41" ref-type="bibr"/></named-content><named-content content-type="print"><sup>&#x02013;</sup></named-content><xref rid="r42" ref-type="bibr"><sup>42</sup></xref> Target applications evaluated were mainly the commodity use of stereo content, in TV, cinema, or gaming VR.<xref rid="r20" ref-type="bibr"><sup>20</sup></xref><sup>,</sup><xref rid="r21" ref-type="bibr"><sup>21</sup></xref><sup>,</sup><xref rid="r43" ref-type="bibr"><sup>43</sup></xref><named-content content-type="online"><xref rid="r44" ref-type="bibr"/><xref rid="r45" ref-type="bibr"/></named-content><named-content content-type="print"><sup>&#x02013;</sup></named-content><xref rid="r46" ref-type="bibr"><sup>46</sup></xref> Fatigue after viewing of 3D content is a well-reproduced phenomenon; its occurrence is temporary and limited to the duration of use and some hours thereafter. Long-term detrimental effects of viewing artificial 3D stereo content have not been found.<xref rid="r47" ref-type="bibr"><sup>47</sup></xref> But in professional use, exposure to 3D stereo content differs from occasional commodity use of 3D: surgeons will use digital stereo microscopes for a vastly extended duration and more frequently. Furthermore, their role differs distinctly from that of a passive viewer. A surgeon interacts with the viewed content and needs to be able to perform high-acuity tasks. Thus the current results give insights into potential visual drawbacks of future digital surgical systems.</p></sec></sec><sec sec-type="supplementary-material"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="s01"><media xlink:href="JBO_024_106501_SD001.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec></body><back><ack><title>Acknowledgments</title><p>Access to the 3D printer as well as usage of according consumables to print the 3D object was kindly provided by Thomas Euler. Review with respect to clinical relevance was kindly provided by Joachim Steffen and Christoph Hauger. Support in maintaining the dynamic stereo microscope was kindly provided by Enkelejda Kasneci and her group, as well as Alois Herkommer and his group. This work was supported by the German Federal Ministry of Education and Research (BMBF) through the Bernstein Center for Computational Neuroscience T&#x000fc;bingen (No.&#x000a0;FKZ 01GQ1002). We furthermore acknowledge support by Deutsche Forschungsgemeinschaft and the Open Access Publishing Fund of University of T&#x000fc;bingen.</p></ack><notes notes-type="conflict-of-interest"><title>Disclosures</title><p>The authors Siegfried Wahl and Katharina Rifai are employees of Carl Zeiss Vision International GmbH.</p></notes><ref-list><title>References</title><ref id="r1"><label>1.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cumming</surname><given-names>B. G.</given-names></name><name><surname>Johnston</surname><given-names>E. B.</given-names></name><name><surname>Hurlbert</surname><given-names>A. C.</given-names></name></person-group>, &#x0201c;<article-title>Multiple cues for three-dimensional shape</article-title>,&#x0201d; in <source>The Cognitive Neurosciences</source>, <person-group person-group-type="editor"><name><surname>Gazzaniga</surname><given-names>M.</given-names></name></person-group>, Ed., pp.&#x000a0;<fpage>351</fpage>&#x02013;<lpage>364</lpage>, <publisher-name>The MIT Press</publisher-name>, <publisher-loc>Cambridge, Massachusetts</publisher-loc> (<year>1995</year>).</mixed-citation></ref><ref id="r2"><label>2.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cutting</surname><given-names>J. E.</given-names></name><name><surname>Vishton</surname><given-names>P. M.</given-names></name></person-group>, &#x0201c;<article-title>Perceiving layout and knowing distances: the integration, relative potency, and contextual use of different information about depth</article-title>,&#x0201d; in <source>Perception of Space and Motion</source>, <person-group person-group-type="editor"><name><surname>Epstein</surname><given-names>W.</given-names></name><name><surname>Rogers</surname><given-names>S.</given-names></name></person-group>, Eds., pp.&#x000a0;<fpage>69</fpage>&#x02013;<lpage>117</lpage>, <publisher-name>Elsevier</publisher-name>, <publisher-loc>San Diego, California</publisher-loc> (<year>1995</year>).</mixed-citation></ref><ref id="r3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parker</surname><given-names>A. J.</given-names></name></person-group>, &#x0201c;<article-title>Vision in our three-dimensional world</article-title>,&#x0201d; <source>Philos. Trans. R. Soc. B: Biol. Sci.</source>
<volume>371</volume>(<issue>1697</issue>) (<year>2016</year>).<pub-id pub-id-type="coden">PTRBAE</pub-id><issn>0962-8436</issn><pub-id pub-id-type="doi">10.1098/rstb.2015.0251</pub-id></mixed-citation></ref><ref id="r4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Landy</surname><given-names>M. S.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Measurement and modeling of depth cue combination: in defense of weak fusion</article-title>,&#x0201d; <source>Vision Res.</source>
<volume>35</volume>(<issue>3</issue>), <fpage>389</fpage>&#x02013;<lpage>412</lpage> (<year>1995</year>).<pub-id pub-id-type="coden">VISRAM</pub-id><issn>0042-6989</issn><pub-id pub-id-type="doi">10.1016/0042-6989(94)00176-M</pub-id><pub-id pub-id-type="pmid">7892735</pub-id></mixed-citation></ref><ref id="r5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langer</surname><given-names>M. S.</given-names></name><name><surname>Siciliano</surname><given-names>R. A.</given-names></name></person-group>, &#x0201c;<article-title>Are blur and disparity complementary cues to depth?</article-title>,&#x0201d; <source>Vision Res.</source>
<volume>107</volume>, <fpage>15</fpage>&#x02013;<lpage>21</lpage> (<year>2015</year>).<pub-id pub-id-type="coden">VISRAM</pub-id><issn>0042-6989</issn><pub-id pub-id-type="doi">10.1016/j.visres.2014.10.036</pub-id><pub-id pub-id-type="pmid">25482222</pub-id></mixed-citation></ref><ref id="r6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mather</surname><given-names>G.</given-names></name></person-group>, &#x0201c;<article-title>The use of image blur as a depth cue</article-title>,&#x0201d; <source>Perception</source>
<volume>26</volume>, <fpage>1147</fpage>&#x02013;<lpage>1158</lpage> (<year>1997</year>).<pub-id pub-id-type="coden">PCTNBA</pub-id><issn>0301-0066</issn><pub-id pub-id-type="doi">10.1068/p261147</pub-id><pub-id pub-id-type="pmid">9509149</pub-id></mixed-citation></ref><ref id="r7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watt</surname><given-names>S. J.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Focus cues affect perceived depth</article-title>,&#x0201d; <source>J. Vision</source>
<volume>5</volume>(<issue>10</issue>), <fpage>7</fpage> (<year>2005</year>).<pub-id pub-id-type="doi">10.1167/5.10.7</pub-id></mixed-citation></ref><ref id="r8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O&#x02019;shea</surname><given-names>R. P.</given-names></name><name><surname>Blackburn</surname><given-names>S. G.</given-names></name><name><surname>Ono</surname><given-names>H.</given-names></name></person-group>, &#x0201c;<article-title>Contrast as a depth cue</article-title>,&#x0201d; <source>Vision Res.</source>
<volume>34</volume>, <fpage>1595</fpage>&#x02013;<lpage>1604</lpage> (<year>1994</year>).<pub-id pub-id-type="coden">VISRAM</pub-id><issn>0042-6989</issn><pub-id pub-id-type="doi">10.1016/0042-6989(94)90116-3</pub-id><pub-id pub-id-type="pmid">7941367</pub-id></mixed-citation></ref><ref id="r9"><label>9.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Vinnikov</surname><given-names>M.</given-names></name><name><surname>Allison</surname><given-names>R. S.</given-names></name></person-group>, &#x0201c;<article-title>Gaze-contingent depth of field in realistic scenes</article-title>,&#x0201d; in <conf-name>Proc. Symp. Eye Tracking Res. and Appl. (ETRA &#x02019;14)</conf-name>, pp.&#x000a0;<fpage>119</fpage>&#x02013;<lpage>126</lpage> (<year>2014</year>).</mixed-citation></ref><ref id="r10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zimmer</surname><given-names>K.-P.</given-names></name></person-group>, &#x0201c;<article-title>Optical designs for stereomicroscopes</article-title>,&#x0201d; <source>Proc. SPIE</source>
<volume>3482</volume>, <fpage>690</fpage>&#x02013;<lpage>697</lpage> (<year>1998</year>).<pub-id pub-id-type="coden">PSISDG</pub-id><issn>0277-786X</issn><pub-id pub-id-type="doi">10.1117/12.321966</pub-id></mixed-citation></ref><ref id="r11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>S&#x000f6;derberg</surname><given-names>I.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Investigation of visual strain experienced by microscope operators at an electronics plant</article-title>,&#x0201d; <source>Appl. Ergon.</source>
<volume>14</volume>(<issue>4</issue>), <fpage>297</fpage>&#x02013;<lpage>305</lpage> (<year>1983</year>).<pub-id pub-id-type="coden">AERGBW</pub-id><issn>0003-6870</issn><pub-id pub-id-type="doi">10.1016/0003-6870(83)90010-8</pub-id><pub-id pub-id-type="pmid">15676492</pub-id></mixed-citation></ref><ref id="r12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeh</surname><given-names>Y. Y.</given-names></name><name><surname>Silverstein</surname><given-names>L. D.</given-names></name></person-group>, &#x0201c;<article-title>Limits of fusion and depth judgment in stereoscopic color displays</article-title>,&#x0201d; <source>Hum. Factors</source>
<volume>32</volume>, <fpage>45</fpage>&#x02013;<lpage>60</lpage> (<year>1990</year>).<pub-id pub-id-type="coden">HUFAA6</pub-id><issn>0018-7208</issn><pub-id pub-id-type="doi">10.1177/001872089003200104</pub-id><pub-id pub-id-type="pmid">2376407</pub-id></mixed-citation></ref><ref id="r13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bando</surname><given-names>T.</given-names></name><name><surname>Iijima</surname><given-names>A.</given-names></name><name><surname>Yano</surname><given-names>S.</given-names></name></person-group>, &#x0201c;<article-title>Visual fatigue caused by stereoscopic images and the search for the requirement to prevent them: a review</article-title>,&#x0201d; <source>Displays</source>
<volume>33</volume>(<issue>2</issue>), <fpage>76</fpage>&#x02013;<lpage>83</lpage> (<year>2012</year>).<pub-id pub-id-type="coden">DISPDP</pub-id><issn>0141-9382</issn><pub-id pub-id-type="doi">10.1016/j.displa.2011.09.001</pub-id></mixed-citation></ref><ref id="r14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoffman</surname><given-names>D. M.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Vergence-accommodation conflicts hinder visual performance and cause visual fatigue</article-title>,&#x0201d; <source>J. Vision</source>
<volume>8</volume>(<issue>3</issue>), <fpage>33</fpage>&#x02013;<lpage>33</lpage> (<year>2008</year>).<pub-id pub-id-type="doi">10.1167/8.3.33</pub-id></mixed-citation></ref><ref id="r15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>J.</given-names></name><name><surname>Kane</surname><given-names>D.</given-names></name><name><surname>Banks</surname><given-names>M. S.</given-names></name></person-group>, &#x0201c;<article-title>The rate of change of vergence-accommodation conflict affects visual discomfort</article-title>,&#x0201d; <source>Vision Res.</source>
<volume>105</volume>, <fpage>159</fpage>&#x02013;<lpage>165</lpage> (<year>2014</year>).<pub-id pub-id-type="coden">VISRAM</pub-id><issn>0042-6989</issn><pub-id pub-id-type="doi">10.1016/j.visres.2014.10.021</pub-id><pub-id pub-id-type="pmid">25448713</pub-id></mixed-citation></ref><ref id="r16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kooi</surname><given-names>F. L.</given-names></name><name><surname>Toet</surname><given-names>A.</given-names></name></person-group>, &#x0201c;<article-title>Visual comfort of binocular and 3D displays</article-title>,&#x0201d; <source>Displays</source>
<volume>25</volume>(<issue>2&#x02013;3</issue>), <fpage>99</fpage>&#x02013;<lpage>108</lpage> (<year>2004</year>).<pub-id pub-id-type="coden">DISPDP</pub-id><issn>0141-9382</issn><pub-id pub-id-type="doi">10.1016/j.displa.2004.07.004</pub-id></mixed-citation></ref><ref id="r17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lambooij</surname><given-names>M.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Visual discomfort and visual fatigue of stereoscopic displays: a review</article-title>,&#x0201d; <source>J. Imaging Sci. Technol.</source>
<volume>53</volume>(<issue>3</issue>), <fpage>30201</fpage> (<year>2009</year>).<pub-id pub-id-type="coden">JIMTE6</pub-id><issn>1062-3701</issn><pub-id pub-id-type="doi">10.2352/J.ImagingSci.Technol.2009.53.3.030201</pub-id></mixed-citation></ref><ref id="r18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>C. J.</given-names></name><name><surname>Woldegiorgis</surname><given-names>B. H.</given-names></name></person-group>, &#x0201c;<article-title>Interaction and visual performance in stereoscopic displays: a review</article-title>,&#x0201d; <source>J. Soc. Inf. Disp.</source>
<volume>23</volume>(<issue>7</issue>), <fpage>319</fpage>&#x02013;<lpage>332</lpage> (<year>2015</year>).<pub-id pub-id-type="coden">JSIDE8</pub-id><issn>0734-1768</issn><pub-id pub-id-type="doi">10.1002/jsid.378</pub-id></mixed-citation></ref><ref id="r19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Read</surname><given-names>J. C.</given-names></name><name><surname>Bohr</surname><given-names>I.</given-names></name></person-group>, &#x0201c;<article-title>User experience while viewing stereoscopic 3D television</article-title>,&#x0201d; <source>Ergonomics</source>
<volume>57</volume>(<issue>8</issue>), <fpage>1140</fpage>&#x02013;<lpage>1153</lpage> (<year>2014</year>).<pub-id pub-id-type="coden">ERGOAX</pub-id><issn>0014-0139</issn><pub-id pub-id-type="doi">10.1080/00140139.2014.914581</pub-id><pub-id pub-id-type="pmid">24874550</pub-id></mixed-citation></ref><ref id="r20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tam</surname><given-names>W. J.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Stereoscopic 3D-TV: visual comfort</article-title>,&#x0201d; <source>IEEE Trans. Broadcast.</source>
<volume>57</volume>(<issue>2 PART 2</issue>), <fpage>335</fpage>&#x02013;<lpage>346</lpage> (<year>2011</year>).</mixed-citation></ref><ref id="r21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>S. N.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Stereoscopic viewing and reported perceived immersion and symptoms</article-title>,&#x0201d; <source>Optometry Vision Sci.</source>
<volume>89</volume>(<issue>7</issue>), <fpage>1068</fpage>&#x02013;<lpage>1080</lpage> (<year>2012</year>).<pub-id pub-id-type="doi">10.1097/OPX.0b013e31825da430</pub-id></mixed-citation></ref><ref id="r22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>L.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Visual fatigue and discomfort after stereoscopic display viewing</article-title>,&#x0201d; <source>Acta Ophthalmol.</source>
<volume>91</volume>(<issue>2</issue>), <fpage>e149</fpage>&#x02013;<lpage>e153</lpage> (<year>2013</year>).<pub-id pub-id-type="doi">10.1111/aos.2013.91.issue-2</pub-id><pub-id pub-id-type="pmid">23164154</pub-id></mixed-citation></ref><ref id="r23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bingham</surname><given-names>G. P.</given-names></name></person-group>, &#x0201c;<article-title>Optical flow from eye movement with head immobilized: &#x0201c;Ocular occlusion&#x0201d; beyond the nose</article-title>,&#x0201d; <source>Vision Res.</source>
<volume>33</volume>(<issue>5&#x02013;6</issue>), <fpage>777</fpage>&#x02013;<lpage>789</lpage> (<year>1993</year>).<pub-id pub-id-type="coden">VISRAM</pub-id><issn>0042-6989</issn><pub-id pub-id-type="doi">10.1016/0042-6989(93)90197-5</pub-id><pub-id pub-id-type="pmid">8351849</pub-id></mixed-citation></ref><ref id="r24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wexler</surname><given-names>M.</given-names></name><name><surname>Van Boxtel</surname><given-names>J. J.</given-names></name></person-group>, &#x0201c;<article-title>Depth perception by the active observer</article-title>,&#x0201d; <source>Trends Cognit. Sci.</source>
<volume>9</volume>(<issue>9</issue>), <fpage>431</fpage>&#x02013;<lpage>438</lpage> (<year>2005</year>).<pub-id pub-id-type="doi">10.1016/j.tics.2005.06.018</pub-id><pub-id pub-id-type="pmid">16099197</pub-id></mixed-citation></ref><ref id="r25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fuhl</surname><given-names>W.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Non-intrusive practitioner pupil detection for unmodified microscope oculars</article-title>,&#x0201d; <source>Comput. Biol. Med.</source>
<volume>79</volume>, <fpage>36</fpage>&#x02013;<lpage>44</lpage> (<year>2016</year>).<pub-id pub-id-type="coden">CBMDAW</pub-id><issn>0010-4825</issn><pub-id pub-id-type="doi">10.1016/j.compbiomed.2016.10.005</pub-id><pub-id pub-id-type="pmid">27744179</pub-id></mixed-citation></ref><ref id="r26"><label>26.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Duchowski</surname><given-names>A. T.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Measuring gaze depth with an eye tracker during stereoscopic display</article-title>,&#x0201d; in <conf-name>Proc. ACM SIGGRAPH Symp. Appl. Percept. in Graphics and Vis. (APGV &#x02019;11)</conf-name> (<year>2011</year>).</mixed-citation></ref><ref id="r27"><label>27.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Santini</surname><given-names>T.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>EyeRecToo: open-source software for real-time pervasive head-mounted eye tracking</article-title>,&#x0201d; in <conf-name>Proc. 12th Int. Joint Conf. Comput. Vision, Imaging and Comput. Graphics Theory and Appl.</conf-name> (<year>2017</year>).</mixed-citation></ref><ref id="r28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garrido-Jurado</surname><given-names>S.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Automatic generation and detection of highly reliable fiducial markers under occlusion</article-title>,&#x0201d; <source>Pattern Recognit.</source>
<volume>47</volume>(<issue>6</issue>), <fpage>2280</fpage>&#x02013;<lpage>2292</lpage> (<year>2014</year>).<pub-id pub-id-type="doi">10.1016/j.patcog.2014.01.005</pub-id></mixed-citation></ref><ref id="r29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kramida</surname><given-names>G.</given-names></name></person-group>, &#x0201c;<article-title>Resolving the vergence-accommodation conflict in head-mounted displays</article-title>,&#x0201d; <source>IEEE Trans. Vis. Comput. Graphics</source>
<volume>22</volume>, <fpage>1912</fpage>&#x02013;<lpage>1931</lpage> (<year>2016</year>).<issn>1077-2626</issn><pub-id pub-id-type="doi">10.1109/TVCG.2015.2473855</pub-id></mixed-citation></ref><ref id="r30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>D.</given-names></name><name><surname>Sohn</surname><given-names>K.</given-names></name></person-group>, &#x0201c;<article-title>Visual fatigue prediction for stereoscopic image</article-title>,&#x0201d; <source>IEEE Trans. Circuits Syst. Video Technol.</source>
<volume>21</volume>(<issue>2</issue>), <fpage>231</fpage>&#x02013;<lpage>236</lpage> (<year>2011</year>).<pub-id pub-id-type="doi">10.1109/TCSVT.2011.2106275</pub-id></mixed-citation></ref><ref id="r31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lambooij</surname><given-names>M.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Measuring visual fatigue and visual discomfort associated with 3-D displays</article-title>,&#x0201d; <source>J. Soc. Inf. Disp.</source>
<volume>18</volume>(<issue>11</issue>), <fpage>931</fpage> (<year>2010</year>).<pub-id pub-id-type="coden">JSIDE8</pub-id><issn>0734-1768</issn><pub-id pub-id-type="doi">10.1889/JSID18.11.931</pub-id></mixed-citation></ref><ref id="r32"><label>32.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Mylonas</surname><given-names>G. P.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Assessment of perceptual quality for gaze-contingent motion stabilization in robotic assisted minimally invasive surgery</article-title>,&#x0201d; in <conf-name>Int. Conf. Med. Image Comput. and Comput.-Assist. Interv.</conf-name>, <publisher-name>Springer</publisher-name>, pp.&#x000a0;<fpage>660</fpage>&#x02013;<lpage>667</lpage> (<year>2007</year>).</mixed-citation></ref><ref id="r33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maiello</surname><given-names>G.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Simulated disparity and peripheral blur interact during binocular fusion</article-title>,&#x0201d; <source>J. Vision</source>
<volume>14</volume>(<issue>8</issue>), <fpage>13</fpage> (<year>2014</year>).<pub-id pub-id-type="doi">10.1167/14.8.13</pub-id></mixed-citation></ref><ref id="r34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vinnikov</surname><given-names>M.</given-names></name><name><surname>Allison</surname><given-names>R. S.</given-names></name><name><surname>Fernandes</surname><given-names>S.</given-names></name></person-group>, &#x0201c;<article-title>Impact of depth of field simulation on visual fatigue: who are impacted? and how?</article-title>&#x0201d; <source>Int. J. Hum.-Comput. Stud.</source>
<volume>91</volume>, <fpage>37</fpage>&#x02013;<lpage>51</lpage> (<year>2016</year>).<pub-id pub-id-type="coden">IHSTEI</pub-id><issn>1071-5819</issn><pub-id pub-id-type="doi">10.1016/j.ijhcs.2016.03.001</pub-id></mixed-citation></ref><ref id="r35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bradshaw</surname><given-names>M. F.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>The task-dependent use of binocular disparity and motion parallax under natural viewing conditions</article-title>,&#x0201d; <source>Perception</source>
<volume>26</volume>(<issue>suppl</issue>), <fpage>48</fpage> (<year>1997</year>).<pub-id pub-id-type="coden">PCTNBA</pub-id><issn>0301-0066</issn><pub-id pub-id-type="doi">10.1068/v970186</pub-id></mixed-citation></ref><ref id="r36"><label>36.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bernhard</surname><given-names>M.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>The effects of fast disparity adjustment in gaze-controlled stereoscopic applications</article-title>,&#x0201d; in <conf-name>Proc. Symp. Eye Tracking Res. and Appl.</conf-name> (<year>2014</year>).</mixed-citation></ref><ref id="r37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>J.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>3D visual discomfort predictor: analysis of horizontal disparity and neural activity statistics</article-title>,&#x0201d; <source>IEEE Trans. Image Process.</source>
<volume>24</volume>(<issue>3</issue>), <fpage>1101</fpage>&#x02013;<lpage>1114</lpage> (<year>2015</year>).<pub-id pub-id-type="coden">IIPRE4</pub-id><issn>1057-7149</issn><pub-id pub-id-type="doi">10.1109/TIP.2014.2383327</pub-id><pub-id pub-id-type="pmid">25532185</pub-id></mixed-citation></ref><ref id="r38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kane</surname><given-names>D.</given-names></name><name><surname>Held</surname><given-names>R. T.</given-names></name><name><surname>Banks</surname><given-names>M. S.</given-names></name></person-group>, &#x0201c;<article-title>Visual discomfort with stereo 3D displays when the head is not upright</article-title>,&#x0201d; <source>Proc. SPIE</source>
<volume>8288</volume>, <fpage>828814</fpage> (<year>2012</year>).<pub-id pub-id-type="coden">PSISDG</pub-id><issn>0277-786X</issn><pub-id pub-id-type="doi">10.1117/12.912204</pub-id></mixed-citation></ref><ref id="r39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nojiri</surname><given-names>Y.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Visual comfort/discomfort and visual fatigue caused by stereoscopic HDTV viewing</article-title>,&#x0201d; <source>Proc. SPIE</source>
<volume>5291</volume>, <fpage>303</fpage>&#x02013;<lpage>313</lpage> (<year>2004</year>).<pub-id pub-id-type="coden">PSISDG</pub-id><issn>0277-786X</issn><pub-id pub-id-type="doi">10.1117/12.522018</pub-id></mixed-citation></ref><ref id="r40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shibata</surname><given-names>T.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>The zone of comfort: predicting visual discomfort with stereo displays</article-title>,&#x0201d; <source>J. Vision</source>
<volume>11</volume>(<issue>8</issue>), <fpage>11</fpage> (<year>2011</year>).<pub-id pub-id-type="doi">10.1167/11.8.11</pub-id></mixed-citation></ref><ref id="r41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tosha</surname><given-names>C.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Accommodation response and visual discomfort</article-title>,&#x0201d; <source>Ophthal. Physiol. Opt.</source>
<volume>29</volume>(<issue>6</issue>), <fpage>625</fpage>&#x02013;<lpage>633</lpage> (<year>2009</year>).<pub-id pub-id-type="coden">OPOPD5</pub-id><issn>0275-5408</issn><pub-id pub-id-type="doi">10.1111/opo.2009.29.issue-6</pub-id></mixed-citation></ref><ref id="r42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yano</surname><given-names>S.</given-names></name><name><surname>Emoto</surname><given-names>M.</given-names></name><name><surname>Mitsuhashi</surname><given-names>T.</given-names></name></person-group>, &#x0201c;<article-title>Two factors in visual fatigue caused by stereoscopic HDTV images</article-title>,&#x0201d; <source>Displays</source>
<volume>25</volume>(<issue>4</issue>), <fpage>141</fpage>&#x02013;<lpage>150</lpage> (<year>2004</year>).<pub-id pub-id-type="coden">DISPDP</pub-id><issn>0141-9382</issn><pub-id pub-id-type="doi">10.1016/j.displa.2004.09.002</pub-id></mixed-citation></ref><ref id="r43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howarth</surname><given-names>P. A.</given-names></name></person-group>, &#x0201c;<article-title>Potential hazards of viewing 3-D stereoscopic television, cinema and computer games: a review</article-title>,&#x0201d; <source>Ophthal. Physiol. Opt.</source>
<volume>31</volume>(<issue>2</issue>), <fpage>111</fpage>&#x02013;<lpage>122</lpage> (<year>2011</year>).<pub-id pub-id-type="coden">OPOPD5</pub-id><issn>0275-5408</issn><pub-id pub-id-type="doi">10.1111/opo.2011.31.issue-2</pub-id></mixed-citation></ref><ref id="r44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lambooij</surname><given-names>M.</given-names></name><name><surname>Ijsselsteijn</surname><given-names>W. A.</given-names></name><name><surname>Heynderickx</surname><given-names>I.</given-names></name></person-group>, &#x0201c;<article-title>Visual discomfort of 3D TV: assessment methods and modeling</article-title>,&#x0201d; <source>Displays</source>
<volume>32</volume>(<issue>4</issue>), <fpage>209</fpage>&#x02013;<lpage>218</lpage> (<year>2011</year>).<pub-id pub-id-type="coden">DISPDP</pub-id><issn>0141-9382</issn><pub-id pub-id-type="doi">10.1016/j.displa.2011.05.012</pub-id></mixed-citation></ref><ref id="r45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname><given-names>A.</given-names></name><name><surname>Berry</surname><given-names>J.</given-names></name><name><surname>Holliman</surname><given-names>N.</given-names></name></person-group>, &#x0201c;<article-title>Can the perception of depth in stereoscopic images be influenced by 3D sound?</article-title>&#x0201d; <source>Proc. SPIE</source>
<volume>7863</volume>, <fpage>786307</fpage> (<year>2011</year>).<pub-id pub-id-type="coden">PSISDG</pub-id><issn>0277-786X</issn><pub-id pub-id-type="doi">10.1117/12.871960</pub-id></mixed-citation></ref><ref id="r46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Urvoy</surname><given-names>M.</given-names></name><name><surname>Barkowsky</surname><given-names>M.</given-names></name><name><surname>Le Callet</surname><given-names>P.</given-names></name></person-group>, &#x0201c;<article-title>How visual fatigue and discomfort impact 3D-TV quality of experience: a comprehensive review of technological, psychophysical, and psychological factors</article-title>,&#x0201d; <source>Ann. Telecommun.</source>
<volume>68</volume>(<issue>11&#x02013;12</issue>), <fpage>641</fpage>&#x02013;<lpage>655</lpage> (<year>2013</year>).<pub-id pub-id-type="coden">ANTEAU</pub-id><issn>0003-4347</issn><pub-id pub-id-type="doi">10.1007/s12243-013-0394-3</pub-id></mixed-citation></ref><ref id="r47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Read</surname><given-names>J. C.</given-names></name><etal>et al.</etal></person-group>, &#x0201c;<article-title>Viewing 3D TV over two months produces no discernible effects on balance, coordination or eyesight</article-title>,&#x0201d; <source>Ergonomics</source>
<volume>59</volume>, <fpage>1073</fpage>&#x02013;<lpage>1088</lpage> (<year>2016</year>).<pub-id pub-id-type="coden">ERGOAX</pub-id><issn>0014-0139</issn><pub-id pub-id-type="doi">10.1080/00140139.2015.1114682</pub-id><pub-id pub-id-type="pmid">26758965</pub-id></mixed-citation></ref></ref-list><bio id="d37e2964"><p>Biographies of the authors are not available.</p></bio></back></article>
