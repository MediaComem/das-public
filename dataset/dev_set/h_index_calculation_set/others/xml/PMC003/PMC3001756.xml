<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Neurosci</journal-id><journal-id journal-id-type="publisher-id">Front. Neurosci.</journal-id><journal-title-group><journal-title>Frontiers in Neuroscience</journal-title></journal-title-group><issn pub-type="ppub">1662-4548</issn><issn pub-type="epub">1662-453X</issn><publisher><publisher-name>Frontiers Research Foundation</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">21160550</article-id><article-id pub-id-type="pmc">PMC3001756</article-id><article-id pub-id-type="doi">10.3389/fnins.2010.00179</article-id><article-categories><subj-group subj-group-type="heading"><subject>Neuroscience</subject><subj-group><subject>Methods Article</subject></subj-group></subj-group></article-categories><title-group><article-title>Pyff &#x02013; A Pythonic Framework for Feedback Applications and Stimulus Presentation in Neuroscience</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Venthur</surname><given-names>Bastian</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="author-notes" rid="fn001">*</xref></contrib><contrib contrib-type="author"><name><surname>Scholler</surname><given-names>Simon</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author"><name><surname>Williamson</surname><given-names>John</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib><contrib contrib-type="author"><name><surname>D&#x000e4;hne</surname><given-names>Sven</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author"><name><surname>Treder</surname><given-names>Matthias S.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author"><name><surname>Kramarek</surname><given-names>Maria T.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author"><name><surname>M&#x000fc;ller</surname><given-names>Klaus-Robert</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff4"><sup>4</sup></xref></contrib><contrib contrib-type="author"><name><surname>Blankertz</surname><given-names>Benjamin</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff5"><sup>5</sup></xref></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>Machine Learning Laboratory, Berlin Institute of Technology</institution><country>Berlin, Germany</country></aff><aff id="aff2"><sup>2</sup><institution>Bernstein Center for Computational Neuroscience</institution><country>Berlin, Germany</country></aff><aff id="aff3"><sup>3</sup><institution>Department of Computing Science, University of Glasgow</institution><country>Glasgow, Scotland</country></aff><aff id="aff4"><sup>4</sup><institution>Bernstein Focus: Neurotechnology</institution><country>Berlin, Germany</country></aff><aff id="aff5"><sup>5</sup><institution>Fraunhofer FIRST (IDA)</institution><country>Berlin, Germany</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: David N. Kennedy, University of Massachusetts Medical School, USA</p></fn><fn fn-type="edited-by"><p>Reviewed by: Jonathan Peirce, Notthingham University, UK; K. Jarrod Millman, University of California at Berkeley, USA; Satrajit S. Ghosh, Massachusetts Institute of Technology, USA</p></fn><corresp id="fn001">*Correspondence: Bastian Venthur, Machine Learning Laboratory, Berlin Institute of Technology, Franklinstra&#x000df;e 28/29 10587 Berlin, Germany. e-mail: <email>bastian.venthur@tu-berlin.de</email></corresp><fn fn-type="other" id="fn002"><p>This article was submitted to Frontiers in Neuroscience Methods, a specialty of Frontiers in Neuroscience.</p></fn></author-notes><pub-date pub-type="epub"><day>02</day><month>12</month><year>2010</year></pub-date><pub-date pub-type="collection"><year>2010</year></pub-date><volume>4</volume><elocation-id>179</elocation-id><history><date date-type="received"><day>14</day><month>6</month><year>2010</year></date><date date-type="accepted"><day>02</day><month>10</month><year>2010</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2010 Venthur, Scholler, Williamson, D&#x000e4;hne, Treder, Kramarek, M&#x000fc;ller and Blankertz.</copyright-statement><copyright-year>2010</copyright-year><license license-type="open-access" xlink:href="http://www.frontiersin.org/licenseagreement"><license-p>This is an open-access article subject to an exclusive license agreement between the authors and the Frontiers Research Foundation, which permits unrestricted use, distribution, and reproduction in any medium, provided the original authors and source are credited.</license-p></license></permissions><abstract><p>This paper introduces Pyff, the Pythonic feedback framework for feedback applications and stimulus presentation. Pyff provides a platform-independent framework that allows users to develop and run neuroscientific experiments in the programming language Python. Existing solutions have mostly been implemented in C++, which makes for a rather tedious programming task for non-computer-scientists, or in Matlab, which is not well suited for more advanced visual or auditory applications. Pyff was designed to make experimental paradigms (i.e., feedback and stimulus applications) easily programmable. It includes base classes for various types of common feedbacks and stimuli as well as useful libraries for external hardware such as eyetrackers. Pyff is also equipped with a steadily growing set of ready-to-use feedbacks and stimuli. It can be used as a standalone application, for instance providing stimulus presentation in psychophysics experiments, or within a closed loop such as in biofeedback or brain&#x02013;computer interfacing experiments. Pyff communicates with other systems via a standardized communication protocol and is therefore suitable to be used with any system that may be adapted to send its data in the specified format. Having such a general, open-source framework will help foster a fruitful exchange of experimental paradigms between research groups. In particular, it will decrease the need of reprogramming standard paradigms, ease the reproducibility of published results, and naturally entail some standardization of stimulus presentation.</p></abstract><kwd-group><kwd>neuroscience</kwd><kwd>BCI</kwd><kwd>Python</kwd><kwd>framework</kwd><kwd>feedback</kwd><kwd>stimulus presentation</kwd></kwd-group><counts><fig-count count="11"/><table-count count="2"/><equation-count count="0"/><ref-count count="39"/><page-count count="0"/><word-count count="12981"/></counts></article-meta></front><body><sec sec-type=""><label>1</label><title>Introduction</title><p>During the past years, the neuroscience community has been moving toward increasingly complex stimulation paradigms (Pfurtscheller et al., <xref ref-type="bibr" rid="B26">2006</xref>; Brouwer and van Erp, <xref ref-type="bibr" rid="B7">2010</xref>; Schreuder et al., <xref ref-type="bibr" rid="B32">2010</xref>) that aim to investigate human function in a more natural setting. A technical bottleneck in this process is programming these complex stimulations. In particular, the rapidly growing field of brain&#x02013;computer interfacing (BCI, Dornhege et al., <xref ref-type="bibr" rid="B10">2007</xref>) requires stimulus presentation programs that can be used within a closed loop, i.e., feedback applications that are driven by a control signal that is derived from ongoing brain activity.</p><p>The present paper suggests a Python-based framework for experimental paradigms that combines the ease in programming and the inclusion of all necessary functionality for flawless stimulus presentation. This is well in line with the growing interest toward using Python in the neuroscience community (Spacek et al., <xref ref-type="bibr" rid="B33">2008</xref>; Br&#x000fc;derle et al., <xref ref-type="bibr" rid="B8">2009</xref>; Drewes et al., <xref ref-type="bibr" rid="B11">2009</xref>; Jurica and VanLeeuwen, <xref ref-type="bibr" rid="B17">2009</xref>; Ince et al., <xref ref-type="bibr" rid="B16">2009</xref>; Pecevski et al., <xref ref-type="bibr" rid="B24">2009</xref>; Strangman et al., <xref ref-type="bibr" rid="B34">2009</xref>). Pyff provides a powerful yet simple and highly accessible framework for the development of complex experimental paradigms containing multi-media. To this end, it accommodates a standardized interface for implementing experimental paradigms, support for special hardware such as eye trackers and EEG equipment as well as large library of ready-to-go experiments. Class experience shows that non-expert programmers typically learn the use of our framework within 2&#x02009;days. Note that a C++ implementation can easily take one order of magnitude more time to learn than the corresponding Python implementation and even for an experienced programmer a factor of two still remains (Prechelt, <xref ref-type="bibr" rid="B27">2000</xref>). The primary aim of Pyff was to provide a convenient basis for programming paradigms in the context of brain&#x02013;computer interfacing. To that end, Pyff can also easily be linked to BCI systems like BCI2000 (Schalk et al., <xref ref-type="bibr" rid="B29">2004</xref>), and the Berlin BCI via a standard communication protocol, see Section <xref ref-type="sec" rid="s1">3</xref>. Furthermore, Pyff can be used for general stimulus presentation. This allows a seamless transition from experiments in the fields of cognitive psychology, neuroscience, or psychophysics to BCI studies.</p><p>Since Pyff is open source, it makes an ideal basis for a vivid exchange of experimental paradigms between research groups and it releases the user from needing to reprogram standard paradigms. Furthermore, providing paradigm implementations as supplementary material within the Pyff framework will ease the reproducibility of published results. The following sections of this paper introduce the software concept, detail a number of typical paradigms and conclude. Several appendices expand on the software engineering side and the code of a sample paradigm is discussed in detail.</p><p>Throughout this paper, we use the term <italic>stimulus and feedback applications</italic> synonymous with <italic>experimental paradigm</italic>. The former term is common in the BCI field, whereas the latter is better known in neuroscience. The difference between a feedback application and a stimulus application is defined by the setup of the experiment. If the experimental setup forms a closed loop, such as in a neurofeedback paradigm, we call the application a <italic>feedback application</italic>. If the loop is not closed, we call it <italic>stimulus presentation</italic>. When referring to the actual software implementation of a paradigm, we use the term <italic>Feedback</italic> (with a capital f), synonymous for stimulus and feedback applications.</p></sec><sec><label>2</label><title>Related Work</title><p>Pyff is a general, high-level framework for the development of experimental paradigms within the programming language Python. In particular, Pyff can receive control signals of a BCI system to drive a feedback application within a closed-loop mode. Other software related to Pyff can be grouped in the following three categories:</p><list list-type="simple"><list-item><label>(1)</label><p>General Python module for visual or auditory presentation</p></list-item><list-item><label>(2)</label><p>Packages for stimulus presentation and experimental control</p></list-item><list-item><label>(3)</label><p>Feedback applications for the use with BCI systems</p></list-item></list><p>Software of categories (1) and (2) can be used within Pyff, while (3) is an alternative to Pyff. In the following, we will shortly discuss prominent examples of the three groups.</p><list list-type="simple"><list-item><label>(1)</label><p>These modules are usually used to write games and other software applications and can be used within Pyff to control stimulus presentation. Pygame<xref ref-type="fn" rid="fn1">1</xref> is a generic platform for gaming applications. It can be readily used to implement visual and auditory stimulus presentation. Similar to Pygame, pyglet<xref ref-type="fn" rid="fn2">2</xref> is a framework for developing games and visually rich applications and therefore suited for visual stimulus applications. PyOpenGL<xref ref-type="fn" rid="fn3">3</xref> provides bindings to OpenGL and related APIs, but requires the programmer to be familiar with OpenGL. In Pyff, a Pygame base class (see Section <xref ref-type="sec" rid="s9">13</xref>) exists, that facilitates the development of experimental paradigms based on this module.</p></list-item><list-item><label>(2)</label><p>There are some comprehensive Python libraries that provide means for creating und running experimental paradigms. Their advanced functionality for stimulus presentation can be used within Pyff. Vision Egg (Straw, <xref ref-type="bibr" rid="B35">2008</xref>) is a high-level interface to OpenGL. It was specifically designed to produce stimuli for vision research experiments. PsychoPy (Peirce, <xref ref-type="bibr" rid="B25">2007</xref>) is a platform-independent experimental control system written in Python. It provides means for stimulus presentation and response collection as well as simple data analysis. PyEPL (Geller et al., <xref ref-type="bibr" rid="B12">2007</xref>) is a another Python library for object-oriented coding of psychology experiments which supports the presentation of visual and auditory stimuli as well as manual and sound input as responses.</p><p>Pyff provides a <monospace>VisionEggFeedback</monospace> base class which allows for easily writing paradigms using Vision Egg for stimulus presentation. The other two modules have up-to-date not been used within Pyff.</p><p>The Psychophysics Toolbox (Brainard, <xref ref-type="bibr" rid="B4">1997</xref>) is a free set of Matlab and GNU/Octave functions for vision research. Being available since the 1990s, it is now a mature research tool and particularly popular among psychologists and neuroscientists. Currently, there is no principle framework to couple the Psychophysics Toolbox to a BCI system.</p><p>In addition to this, there are also commercial solutions such as E-Prime (Psychology Software Tools, Inc) and Presentation (Neurobehavioral Systems) which are software for experiment design, data collection, and analysis.</p></list-item><list-item><label>(3)</label><p>BCI2000 (Schalk et al., <xref ref-type="bibr" rid="B29">2004</xref>) is a general-purpose system for BCI research that is free for academic and educational purposes. It is written in C&#x02009;+ + and runs under Microsoft Windows. BCPy2000 (Schreiner, <xref ref-type="bibr" rid="B31">2008</xref>) is an extension that allows developers to implement BCI2000 modules in Python, which is less complex and less error prone than C++. BCPy200 is firmly coupled to BCI2000 resulting in a more constraint usage compared to Pyff.</p></list-item></list></sec><sec id="s1"><label>3</label><title>Overview of Pyff</title><p>This section gives an overview of our framework. A more complete and technical description is available in the Appendix.</p><p>The Pythonic feedback framework (Pyff) is a framework to develop experimental paradigms. The foremost design goal was to make the development of such applications fast and easy, even for users with little programming experience. For this reason we decided to the Python programming language as it is easier to learn than low level languages like C++. The code is shorter and clearer and thus leads to faster and less error prone results. Python is slower than a low level language like C++, but usually fast enough for multi media applications. In rare cases where Python is too slow for complex calculations, it is easy to port the computationally intensive parts to C and then call them within Python.</p><p>The framework consists of four parts: the Feedback Controller, a graphical user interface (GUI), a set of Feedbacks and a collection of Feedback base classes (see Figure <xref ref-type="fig" rid="F1">1</xref>).</p><fig id="F1" position="float"><label>Figure 1</label><caption><p><bold>Overview of the Pyff framework</bold>. The framework consists of the Feedback Controller, the GUI, a collection of Feedbacks and Feedback base classes.</p></caption><graphic xlink:href="fnins-04-00179-g001"/></fig><p>The <italic>Feedback Controller</italic> controls the execution of the stimulus and feedback applications and forward incoming signals from an arbitrary data source such as a BCI system to the applications. To enable as many existing systems as possible to communicate with Pyff, we developed a simple communications protocol based on the user datagram protocol (UDP). The protocol allows for transportation of data over a network using extensible markup language (XML) to encode the signal. This protocol enables virtually any software that is able to output it's data in some form to send it to Pyff with only minor modifications.</p><p>The <italic>GUI</italic> controls the Feedback Controller remotely. Within the GUI the experimenter can select and start Feedbacks as well as inspect and manipulate their variables (e.g., number of trials, position, and color of visual objects). The ability to inspect the Feedback application's internal variables in real time while the application is running makes the GUI an invaluable debugging tool. Being able to modify these variables on the fly also provides a great way to explore different settings in a pilot experiment. The GUI also uses the aforementioned communication protocol and thus does not need to run on the same machine as the Feedback Controller. This can be convenient for experiments where the subject is in a different room than the experimenter. Note, that the use of the GUI is optional as everything can also be controlled remotely via UDP/XML, see Section <xref ref-type="sec" rid="s8">11</xref>.</p><p>Pyff not only provides a platform to develop <italic>Feedback applications</italic> and <italic>stimuli</italic> easily, but it is also equipped with a variety of paradigms (see Section <xref ref-type="sec" rid="s2">4</xref> for examples). The BBCI group will continue to publish feedback and stimulus applications under a Free Software License, making them available to other research groups and we hope that others will also join our effort.</p><p>The collection of <italic>Feedback base classes</italic> provides a convenient set of standard methods for paradigms which can be used in derived Feedback classes to speed up the development of new Feedback applications. This standard functionality reduces the overheads of developing a new Feedback as well as minimizing code duplication. To give an example, Pygame is frequently used to provide the graphical output of the Feedback. Since some things need to be done in every Feedback using Pygame (i.e., initializing the graphics or regularly polling Pygame's event queue), we created the PygameFeedback base class. It contains methods required by all Pygame-based Feedbacks and some convenient helper methods we find useful. Using this base class for in a Pygame-based Feedback can drastically reduce the amount of new code required. It helps to concentrate on the code needed for the actual paradigm instead of dealing with the quirks of the library used. Pyff already provides some useful base classes like <monospace>VisionEggFeedback</monospace>, <monospace>EventDrivenFeedback</monospace>, and <monospace>VisualP300</monospace>. Our long term goal is to provide a rich set of base classes for standard experimental paradigms to ease the effort of programming new Feedbacks even more.</p></sec><sec id="s2"><label>4</label><title>Selected Feedbacks</title><p>Pyff allows for the rapid implementation of one's own paradigms, but it also comes equipped with a variety of ready-to-use paradigms. In the following sections, we will present a few examples.</p><sec id="s3"><label>4.1</label><title>Hex-o-spell for continuous input signals</title><p>The Hex-o-Spell is a text-entry device that is operated via timing-based changes of a continuous control signal (M&#x000fc;ller and Blankertz, <xref ref-type="bibr" rid="B23">2006</xref>; Williamson et al., <xref ref-type="bibr" rid="B39">2009</xref>). These properties render it a suitable paradigm for BCI experiments in which brain-state discriminating strategies are employed that also have a fine temporal resolution.</p><p>The structure of the Hex-o-Spell Feedback with all its visual components is shown in Figure <xref ref-type="fig" rid="F2">2</xref>A. The main visual elements are an arrow that is surrounded by an array of six hexagons in the center of the screen, a large text board that displays the spelled text, and a bar of varying height that indicates the current value of the control signal. The hexagons surrounding the central arrow contain the symbols that are used for spelling.</p><fig id="F2" position="float"><label>Figure 2</label><caption><p><bold>The Hex-o-Spell Feedback</bold>. <bold>(A)</bold> All components of the Hex-o-Spell are visible. The Feedback is in selection stage one and the currently spelled text consists of the letter &#x0201c;B&#x0201d; only. <bold>(B)</bold> The symbol layout after spelling of &#x0201c;BERL.&#x0201d; <bold>(C)</bold> The second hexagon (clockwise, from the top) has been selected in stage one and the Feedback is now in stage two.</p></caption><graphic xlink:href="fnins-04-00179-g002"/></fig><p>The actual selection of a symbol is a two stage process and involves the subject controlling the orientation and length of the arrow. How exactly the parameters of the arrow are manipulated by the subject is explained in the next paragraph, while the remainder of this paragraph outlines the symbol selection process. In the first stage each hexagon contains five symbols. The subject has to select the hexagon that contains the desired symbol by making the arrow point to it and then confirming the choice by making the arrow grow until it reaches the hexagon. After this confirmation, the symbol content of the selected hexagon is distributed over the entire hexagon array, such that each hexagon now contains maximally one symbol only. There is always one hexagon that contains no symbol, which, in conjunction with the delete symbol &#x0201c;&#x0003c;,&#x0201d; represents a practically unlimited <italic>undo</italic> option. The location of individual symbols in stage two reflects the positions they had in the single hexagon in stage one (compare Figures <xref ref-type="fig" rid="F2">2</xref>B,C). Now the subject has to position the arrow such that it points to the hexagon that contains the desired symbol and, again, confirm the selection by making the arrow grow until the respective hexagon is reached. After the symbol has been selected, the spelled text in the text board is updated accordingly. The Hex-o-Spell Feedback now returns to stage one, i.e., the hexagons again show their original symbol content and the symbol selection process can begin anew. All major events, including for example on- and offsets of transition animations between stages, symbol selection, and GUI interaction (play, pause, stop) are accompanied by sending an event-specific integer code to the parallel port of the machine that runs the Hex-o-Spell Feedback. These codes can be incorporated in the marker structure of the EEG recording software and used for later analysis.</p><p>In order to operate the Hex-o-Spell symbol selection mechanism, the subject has to control the behavior of the arrow. The arrow is always in one of three distinct states: (1) clockwise rotation, (2)&#x02009;no rotation, and (3) no rotation and growth, i.e., increase in length until a certain maximum length. Upon returning from state (3) to state (2), the arrow shrinks back to default length. The states of the arrow are directly linked to the control signal from the Feedback Controller, which is required to be in the range between &#x02212;1 and 1. Two thresholds, <italic>t</italic><sub>1</sub> and <italic>t</italic><sub>2</sub> with &#x02212;1 &#x0003c;&#x02009;<italic>t</italic><sub>1</sub> &#x0003c;&#x02009;<italic>t</italic><sub>2</sub> &#x0003c;&#x02009;1, partition the control signal range in three disjunct regions and thereby allow switching of arrow states by altering the control signal strength. The two thresholds are visualized as part of the control signal bar and therefore provide the subject with feedback as to how much more they have to in-/decrease signal strength in order to achieve a certain arrow state. The thresholds, time constants that determine rotation and scaling speed as well as other parameters of the Feedback can be adjusted during the experiment to allow for further accommodation to the subject.</p><p>So far all ingredients that are essential for operating the Hex-o-Spell Feedback have been explained. Additionally, our implementation includes mechanisms that speed up the spelling of words considerably by exploiting certain statistical properties of natural language. We achieve this by making those symbols easier accessible that are more likely to be selected next. In stage one the arrow starts always pointing to the hexagon containing the most probable next symbol. Additionally the positions of letters within each hexagon in stage one are arranged so that the arrow always starts pointing to the most probable symbol in stage two, followed by the second most probable, etc. The selection probability distribution model is updated after each new symbol. With these text-entry aids, the Hex-o-Spell Feedback allows for faster spelling rates of up to 7.6 symbols/min (Blankertz et al., <xref ref-type="bibr" rid="B3">2007</xref>).</p><p>This Feedback requires the following packages to be installed: NumPy<xref ref-type="fn" rid="fn4">4</xref> and Panda3D<xref ref-type="fn" rid="fn5">5</xref>. Numpy is essential for the underlying geometrical computations (angles, position, etc.) and the handling of the language model data. Panda3D provides the necessary subroutines for rendering the visual feedback. Both packages are freely available for all major platforms from their respective websites.</p></sec><sec id="s4"><label>4.2</label><title>ERP-based Hex-o-Spell</title><p>The ERP-based Hex-o-Spell (Treder and Blankertz, <xref ref-type="bibr" rid="B37">2010</xref>) is an adaptation of the standard Hex-o-Spell (see Section <xref ref-type="sec" rid="s3">4.1</xref>) utilizing event related potentials (ERPs) to select the symbols. The ERP is the brain response following an external or internal event. It involves early positive and negative components usually associated with sensory processing and later components reflecting cognitive processes. The main course of selecting symbols works in a two stage process as described in Section <xref ref-type="sec" rid="s3">4.1</xref>, but in the ERP-based variant the symbols are not selected by a rotating arrow, but by the ERPs caused by intensification of elements on the screen: Discs containing the symbols are intensified in random order and if the attended disc is intensified (Figure <xref ref-type="fig" rid="F3">3</xref>), a characteristic ERP is elicited. After 10 rounds of intensifications the BCI system has enough confidence to decide which of the presented discs the subject wanted to select.</p><fig id="F3" position="float"><label>Figure 3</label><caption><p><bold>Stage one and two of the ERP-based Hex-o-Spell</bold>. Instead of an arrow, intensified discs are used to present the current selection.</p></caption><graphic xlink:href="fnins-04-00179-g003"/></fig><p>Intensification is realized by up-sizing the disc including the symbol(s) by 62.5%. Each intensification is accompanied by a trigger which is sent to the EEG using the <monospace>send_parallel()</monospace> method of the Feedback base class. The triggers mark the exact points in time of the intensifications and are essential for the BCI system to do ERP-based BCI.</p><p>The ERP Hex-o-Spell Feedback class is derived from the <monospace>VisualP300</monospace> base class, provided by Pyff. This base class provides many useful methods to quickly write Visual ERP-based feedbacks or stimuli. For the actual drawing on the screen, Pygame<xref ref-type="fn" rid="fn1">1</xref> is used.</p></sec><sec><label>4.3</label><title>SSVEP-based Hex-o-Spell</title><p>The brain responds to flickering visual stimuli by generating <italic>steady state visual evoked potentials</italic> (SSVEP, Herrmann, <xref ref-type="bibr" rid="B14">2001</xref>) of corresponding frequency and its harmonics. Various studies have used this phenomena to characterize the spatial attention of a subject within a set of stimuli with differing frequencies. Cheng et al. (<xref ref-type="bibr" rid="B9">2002</xref>) showed a multi-class SSVEP-based BCI, where the subject had to select 1 of 10 numbers and two control buttons with mean information transfer rate of 27.15 bits/min. M&#x000fc;ller-Putz and Pfurtscheller (<xref ref-type="bibr" rid="B22">2008</xref>) demonstrated that subjects where able to control a hand prosthesis using SSVEP.</p><p>In our SSVEP-based variant of the Hex-o-Spell, the selection of symbols is again a two stage process as described in Section <xref ref-type="sec" rid="s3">4.1.</xref> In the SSVEP-based variant there is no rotating arrow but the hexagons are all blinking. The blinking frequencies are pairwise different and fixed for each hexagon. Two different approaches can be used to select the hexagon containing the given letter: <italic>overt-</italic> and <italic>covert attention</italic>. In the overt attention case the subject is required to look at the hexagon with the letter, whereas in the case of covert attention, the subject must look at the dot in the middle and only concentrate on the desired hexagon. In both cases it is tested if the subject looks at the required spot by means of an eye tracker. If the subject aims the gaze somewhere else, the trial is stopped and, after showing an error message accompanied by a sound, it is restarted.</p><p>Before the experiment one might be interested in the optimal blinking frequencies for the subject. The SSVEP-based Hex-o-Spell, supports a training mode where one centered hexagon is blinking in different frequencies. After testing different frequencies in random order, the experimenter can chose the six frequencies with the highest signal to noise ration in the power spectrum of the recorded SSVEP at the flicker frequency of the stimulus or one of its harmonics. Since harmonics of different flicker frequencies can overlap, care must be taken so that the chosen frequencies can be well discriminated (Krusienski and Allison, <xref ref-type="bibr" rid="B18">2008</xref>).</p><p>Since in SSVEP experiments appropriate frequencies of the flashing stimuli are very important, special attention was given to this matter. Thus, the Feedback was programmed using Vision Egg (Straw, <xref ref-type="bibr" rid="B35">2008</xref>) which allows very accurate presentation of time-critical stimuli. We verified that the specified frequencies where exactly produced on the monitor by measuring them with an oscilloscope (Handyscope HS3 by Bitzer Digitaltechnik).</p></sec><sec><label>4.4</label><title>ERP-based photo browser</title><p>The photobrowser Feedback enables users to select images from a collection of images, e.g., for selecting images from an album for future presentation, or simply for enjoyment. Images are selected by detecting a specific ERP response of the EEG upon attended target images compared to non-attended non-target images, similar to the ERP-based Hex-o-Spell, see Section <xref ref-type="sec" rid="s4">4.2.</xref> The photobrowser uses the images themselves as stimuli, tilting and flashing some subgroup of the displayed images at regular intervals (see Figure <xref ref-type="fig" rid="F4">4</xref>). Users simply focus attention on a particular image, and the system randomly stimulates the whole collection. After several stimulation cycles, there is sufficient evidence from the ERP signals to detect the image that the user is responding to.</p><fig id="F4" position="float"><label>Figure 4</label><caption><p><bold>(A)</bold> The ERP-based photobrowser in operation. <bold>(B)</bold> Close up of images in non-intensified state. <bold>(C)</bold> Same images during the intensification cycle, with two images intensified.</p></caption><graphic xlink:href="fnins-04-00179-g004"/></fig><p>Using the Pyff framework for communication with a BCI system, the photobrowser implementation displays a 5 &#x000d7;&#x02009;6 grid of photographs. The subject watches the whole grid, but attends to an image to be selected for further use. This can be done by direct gaze, but alternatively also covert attention mechanisms without direct gaze can be utilized by the subject. [For an investigation of the effect of target fixation see (Treder and Blankertz, <xref ref-type="bibr" rid="B37">2010</xref>)]. During this time, the implementation stimulates subsets of images in a series of discrete steps, simultaneously sending a synchronization trigger into the EEG system, so that the timing of the stimulation can be matched to the onset of the visual stimulation. The photobrowser goes through a series of stimulation cycles, where subsets of the photographs are highlighted. After a sufficient number of these cycles, enough evidence is built up to determine the photograph the user is interested in selecting.</p><p>Stimulated images flash white, with a subtle white grid superimposed, and are rotated and scaled at the same time. This gives the visual impression of the images suddenly glowing and popping. The specific set of the visual parameters used (e.g., scale factor, tilt factor, flash duration, flash color) can all be configured to maximize the ERP response. The timing of these stimulations is also completely adjustable via the Pyff interface. The browser uses an immediate onset followed by an exponential decay for all of the stimulation parameters (for example, at the instant of stimulation the image turns completely white and then returns to its normal luminosity according to an exponential schedule).</p><p>The feedback uses PyOpenGL<xref ref-type="fn" rid="fn3">3</xref> and Pygame<xref ref-type="fn" rid="fn1">1</xref> for rapid display updates and high-quality image transformation. OpenGL provides the hardware transformation and alpha blending required for acceptable performance. NumPy<xref ref-type="fn" rid="fn4">4</xref> is used in the optimization routines to optimize the group of images stimulated on each cycle. Pyff provides access to the parallel ports for hard synchronization between the visual stimuli and the EEG recordings. Even in this time-critical application, Python with these libraries has good enough timing to allow the use of the ERP paradigm.</p></sec><sec><label>4.5</label><title>Goalkeeper</title><p>The Goalkeeper Feedback is intended to be used for rapid-response BCIs (e.g., Ramsey et al., <xref ref-type="bibr" rid="B28">2009</xref>). In a rapid-response BCI, subjects are forced to alter their brain states in response to a cue as fast as possible in order to achieve a given task. The major aim of such experiments is to increase the bit-rates of BCI systems.</p><p>The main components of this Feedback are a ball and a keeper bar (see Figure <xref ref-type="fig" rid="F5">5</xref>). The ball starts at the top of the screen and then descends automatically with a predefined velocity while the keeper is controlled by the subject. The task of the subject is to alter the keeper position via the BCI such that the keeper catches the ball. The powerbar at the bottom of the screen visualizes the classifier output and thus gives immediate feedback to the subject, thereby helping them to perform the task more successfully.</p><fig id="F5" position="float"><label>Figure 5</label><caption><p><bold>The Goalkeeper Feedback: (A) Feedback during the trial start animation</bold>. <bold>(B)</bold> Feedback during the trial.</p></caption><graphic xlink:href="fnins-04-00179-g005"/></fig><p>Each trial starts with an animation (two hemispheres approaching each other) that is intended to make the beginning of the actual trial (i.e., the descent of the ball) predictable for the subject. After the animation, the ball will choose one of two directions at random (i.e., left or right) and start to descend. Since subjects normally need some time to adapt their brain states according to the direction of ball movement, the keeper is not controllable (i.e., the classifier output is not used) in the first period of the descent (e.g., the first 300&#x02009;ms). There are three main positions for the keeper: middle (initial position), left, and right. If the classifier threshold on either side is reached, the keeper will change its position to the respective side. This is typically realized as a non revertible jump according to the goalkeeper metaphor, but optionally a different behavior can be chosen. The velocity of the ball can be gradually increased during the experiment in order to force the subjects to speed up their response.</p><p>In addition to a number of general variables which can be used to define the time course (e.g., the time of a trial or the start animation) or the layout (e.g., the size of the visual components of the Feedback), there are also several main settings governing the work flow of the Feedback: (1) The powerbar can either show the direct classifier output or integrate the classifier output over time in order to smooth rapid fluctuations. (2) The keeper can either be set to perform exactly one move, or the Feedback can allow for a return of the keeper back to other positions. (3) The position change of the keeper can either be realized as a rapid jump or a smooth movement with a fixed duration. (4) Two additional control signals can optionally be visualized in the start animation by color changes between green and red of the two hemispheres. This option is inspired by the observation that a high pre-stimulus amplitude of sensorimotor rhythms (SMR) promotes better feedback performance in the subsequent trial (Maeder et al., <xref ref-type="bibr" rid="B21">2010</xref>). (5) The trial can be prolonged if the keeper is still in the initial position (i.e., the subject did not yet reach the threshold for either side).</p><p>The implementation of the Feedback is done in a subclass of the <monospace>MainLoop_Feedback</monospace> base class provided by Pyff and using Pygame<xref ref-type="fn" rid="fn1">1</xref> and the Python image library (PIL)<xref ref-type="fn" rid="fn6">6</xref> for presentation.</p></sec><sec><label>4.6</label><title>Other feedbacks and stimuli</title><p>While the previous selection of paradigms had a focus on brain&#x02013;computer interface research, Pyff also ships with a growing number that are widely used in neuroscience, neuroergonomics, and psychophysics. The following list gives a few examples.</p><def-list><def-item><term>d2 test</term><def><p>A computerized version of the d2 test (Brickenkamp, <xref ref-type="bibr" rid="B5">1972</xref>), a Psychological pen and paper test to assess concentration- and performance ability of a subject. The complete listing of the application is in Section <xref ref-type="sec" rid="s9">13</xref>.</p></def></def-item><def-item><term><italic>n</italic>-back</term><def><p>A parametric paradigm to induce workload. Symbols are presented in a chronological sequence and upon each presentation participants are required to match the current symbol with the <italic>n</italic>th preceding symbol (Gevins and Smith, <xref ref-type="bibr" rid="B13">2000</xref>).</p></def></def-item><def-item><term>boring clock</term><def><p>A computerized version of the Mackworth clock test, a task to investigate long-term vigilance and sustained attention. Participants monitor a virtual clock whose pointer makes rare jumps of two steps and they press a button upon detecting such an event (Mackworth, <xref ref-type="bibr" rid="B20">1948</xref>).</p></def></def-item><def-item><term>oddball</term><def><p>A versatile implementation of the oddball paradigm using visual, auditory, or tactile stimuli.</p></def></def-item></def-list></sec><sec><label>4.7</label><title>Support for special hardware</title><p>Since Python can utilize existing libraries (e.g., C-libraries, dlls). It is easy to use special hardware within Pyff. Pyff already provides a Python module for the IntelliGaze eye tracker by Alea Technologies and the g.STIMbox by g.tec. Other modules will follow.</p></sec></sec><sec><label>5</label><title>Using Pyff</title><p>So far, we stressed the point that implementing an experiment using Pyff is fast and easy, but what exactly needs to be done? Facing the task of implementing a paradigm one has three options: First, check if a similar solution already exists in Pyff that can be used for the given experiment with minor modifications. If it is a standard experiment in BCI or psychology, chances are high that it is already included in Pyff. Second, if there is no application available matching the given requirements, a new Feedback has to be written. In this case one has to check if one of the given base classes match the task or functionality of the paradigm (e.g., is it a P300 task?, is it written using Pygame?, etc.). If so, the base class can be used to develop the feedback or stimulus application which will drastically reduce the code and thus the time required to create the Feedback. Third, if everything has to be written from scratch and no base class seems to fit, one should write the code in a way that is well structured and reusable. A base class may be distilled from the application to reduce the amount of code to be written the next time a similar task appears. In the second and third case, the authors would ideally send their code to us in order to include it in the Pyff framework. Thus the collection of stimuli and base classes would grow, making the first case more probable over time.</p><sec><label>5.1</label><title>Ease of use</title><p>We have successfully used Pyff for teaching and experiments in our group since 2008. Our experience shows that researchers and students from various backgrounds quickly learn how to utilize Pyff to get their experiments done. We discuss three cases exemplary: In the early stages of Pyff, we asked a student to re-implement a given Matlab-Feedback in Pyff, to test if Pyff is feasible for our needs. We used Matlab for our experiments back then and we wanted to test if our new framework is capable of substituting the Matlab solutions. The paradigm was a Cursor-Arrow task, a standard BCI experiment. Given only the framework and documentation, the student completed the task within a few days without any questions. The resulting Feedback looked identical to the Matlab version but ran much smoother. Shortly later we wanted to compare the effort needed to implement a Feedback in Matlab and with our framework. We asked a student who was proficient in Matlab and Python to implement a Feedback in Matlab and our framework. The paradigm was quite simple: a ball is falling from the top of the screen, and the users tasks was to &#x0201c;catch&#x0201d; the ball with a bar on the bottom of the screen, which can only be moved to the left or right. The student implemented both Feedbacks within days without any questions regarding Pyff or Matlab. After the programming he told us that he had no problems with any of the two implementations. Since the given task was relatively simple he could use Matlab's plotting primitives to draw the Feedback on screen which was easier than with Pygame, which he used to draw the primitives on the screen, where he had to read the documentation first. He reported however that a more complex paradigm would also have required much more effort for the Matlab solution and only a little more for the Pyff version. In a third test we wanted a rather complex paradigm and see how well our framework copes with the requirements. The idea was to simulate a liquid floating on a plane which can tilt in any direction. The plane has three or more corners and the user's task is to tilt the plane in a way that the liquid floats to a designated corner. The simulation included a realistic physical model of liquid motion. A Ph.D. student implemented the Feedback in Pyff, the physical model was developed in C and then SWIG (Beazley, <xref ref-type="bibr" rid="B2">1996</xref>) was used to wrap the C-code in Python. He implemented the Feedback without any questions regarding the framework. The result is an impressive simulation, which runs very smoothly within our framework.</p><p>These three examples indicate that students and researchers without experience with Pyff have no difficulties implementing paradigms in a short time. The tasks varied from simple Matlab to Python comparisons to significantly more complex applications. This is consistent with our more than 2&#x02009;years of experience using Pyff in our lab, an environment where co-workers and students use it regularly for teaching and experiments.</p></sec></sec><sec><label>6</label><title>Conclusion</title><p>The Pythonic Feedback Framework provides a platform for writing high-quality stimulus and feedback applications with minimal effort, even for non-computer scientists.</p><p>Pyff's concept of Feedback base classes allows for rapid feedback and stimulus application development, e.g., oddball paradigms, ERP-based typewriters, Pygame-based applications, etc. Moreover, Pyff already includes a variety of stimulus presentations and feedback applications which are ready to be used instantly or with minimal modifications. This list is ever growing as we constantly develop new ones and other groups will hopefully join the effort.</p><p>By providing an interface utilizing well known standard protocols and formats, this framework should be adaptable to most existing neuro systems. Such a unified framework creates the unique opportunity of exchanging neuro feedback applications and stimuli between different groups, even if individual systems are used for signal acquisition, processing, and classification.</p><p>At the time of writing Pyff has been used in four labs and several publications (Ramsey et al., <xref ref-type="bibr" rid="B28">2009</xref>; Acqualagna et al., <xref ref-type="bibr" rid="B1">2010</xref>; H&#x000f6;hne et al., <xref ref-type="bibr" rid="B15">2010</xref>; Maeder et al., <xref ref-type="bibr" rid="B21">2010</xref>; Schmidt et al., <xref ref-type="bibr" rid="B30">2010</xref>; Treder and Blankertz, <xref ref-type="bibr" rid="B37">2010</xref>; Venthur et al., <xref ref-type="bibr" rid="B38">2010</xref>).</p><p>We consider Pyff as stable software which is actively maintained. To date, new versions are released every few months that include bug fixes and new features. We plan to continue development of Pyff as our group uses it to conduct many experiments and other groups are starting to adopt it. As such, we realize that backward compatibility of the API is very important and we work hard to avoid breakage of existing experiments when making changes.</p><p>Pyff has a homepage<xref ref-type="fn" rid="fn7">7</xref>, where users can download current as well as older versions of Pyff. The homepage also provides online documentation for each Pyff version and a link to Pyff's mailing list for developers and users and to Pyff's repository.</p><p>Pyff is free software and available under the terms of the GNU general public license (GPL)<xref ref-type="fn" rid="fn8">8</xref>. Pyff currently requires Python 2.6<xref ref-type="fn" rid="fn9">9</xref> and PyQT version 4<xref ref-type="fn" rid="fn10">10</xref> or later. Some Feedbacks may require additional Python modules. Pyff runs under all major operating systems including Linux, Mac, and Windows.</p></sec><sec><title>Conflict of Interest Statement</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec></body><back><ack><p>This work was partly supported by grants of the Bundesministerium f&#x000fc;r Bildung und Forschung (BMBF) (FKZ 01IB001A, 01GQ0850) and by the FP7-ICT Programme of the European Community, under the PASCAL2 Network of Excellence, ICT-216886. This publication only reflects the authors&#x02019; views. Funding agencies are not liable for any use that may be made of the information contained herein. We would also like to thank the reviewers, who helped to substantially improve the manuscript.</p></ack><fn-group><fn id="fn1"><p><sup>1</sup>Pygame homepage. <uri xlink:type="simple" xlink:href="http://pygame.org/">http://pygame.org/</uri></p></fn><fn id="fn2"><p><sup>2</sup>pyglet homepage. URL <uri xlink:type="simple" xlink:href="http://pyglet.org/">http://pyglet.org/</uri></p></fn><fn id="fn3"><p><sup>3</sup>Pyopengl homepage. URL <uri xlink:type="simple" xlink:href="http://pyopengl.sourceforge.net/">http://pyopengl.sourceforge.net/</uri></p></fn><fn id="fn4"><p><sup>4</sup>Numpy homepage. URL <uri xlink:type="simple" xlink:href="http://numpy.scipy.org/">http://numpy.scipy.org/</uri></p></fn><fn id="fn5"><p><sup>5</sup>Panda3d homepage. URL <uri xlink:type="simple" xlink:href="http://panda3d.org/">http://panda3d.org/</uri></p></fn><fn id="fn6"><p><sup>6</sup>Pil homepage. URL <uri xlink:type="simple" xlink:href="http://www.pythonware.com/products/pil/">http://www.pythonware.com/products/pil/</uri></p></fn><fn id="fn7"><p><sup>7</sup>Pyff homepage. URL <uri xlink:type="simple" xlink:href="http://bbci.de/pyff/">http://bbci.de/pyff/</uri></p></fn><fn id="fn8"><p><sup>8</sup>GNU general public license. URL <uri xlink:type="simple" xlink:href="http://www.gnu.org/copyleft/gpl.html">http://www.gnu.org/copyleft/gpl.html</uri></p></fn><fn id="fn9"><p><sup>9</sup>Python homepage. URL <uri xlink:type="simple" xlink:href="http://python.org/">http://python.org/</uri></p></fn><fn id="fn10"><p><sup>10</sup>Pyqt homepage. URL <uri xlink:type="simple" xlink:href="http://www.riverbankcomputing.co.uk/software/pyqt/">http://www.riverbankcomputing.co.uk/software/pyqt/</uri></p></fn><fn id="fn11"><p><sup>11</sup>Thread state and the global interpreter lock. URL <uri xlink:type="simple" xlink:href="http://docs.python.org/c-api/init">http://docs.python.org/c-api/init</uri>.html#thread-state-and-the-global-interpreter-lock</p></fn><fn id="fn12"><p><sup>12</sup>Extensible markup language 1.0, design and goals. URL <uri xlink:type="simple" xlink:href="http://www.w3.org/TR/REC-xml/#sec-origin-goals">http://www.w3.org/TR/REC-xml/#sec-origin-goals</uri></p></fn></fn-group><app-group><app id="A1"><title>Appendix</title><p>Throughout this appendix we use the term <italic>feedback</italic> synonymous for feedback and stimulus application as both concepts make technically little difference in the context of the framework: feedbacks are usually stimuli with some sort of closed loop from the subject to the application.</p><sec><label>7</label><title>Components of the Framework</title><p>Pyff consists of four major parts: the <italic>Feedback Controller</italic>, the g<italic>raphical user interface</italic> (GUI), a set of <italic>Feedback baseclasses</italic> and a set of <italic>Feedbacks</italic>. Figure <xref ref-type="fig" rid="F6">6</xref> shows an overview of Pyff.</p><p>Pyff communicates with the rest of the world via a standardized communication protocol using UDP and XML (Section <xref ref-type="sec" rid="s8">11</xref>).</p><fig id="F6" position="anchor"><label>Figure 6</label><caption><p><bold>Schematic overview of an experiment using Pyff</bold>. The arbitrary data source, which could be a BCI system, feeds data to the Feedback Controller using our protocol. The Feedback Controller translates those signals and forward them to the currently running Feedback which in turn produces output to be consumed by the subject. The experimenter can remotely control the whole experiment via the GUI, which is connected to the Feedback Controller. The Feedback uses modules of the Feedback base classes.</p></caption><graphic xlink:href="fnins-04-00179-g006"/></fig><sec id="s5"><label>7.1</label><title>The feedback controller</title><p>The Feedback Controller manages the communication between the Feedback and the world outside Pyff. It is responsible for spawning new Feedback processes, starting, pausing and stopping them as well as inspecting and manipulating their internal variables.</p><p>Once started, the Feedback Controller acts like a server, waiting for incoming signals from the network. Incoming signals are encoded in XML (Section <xref ref-type="sec" rid="s8">11</xref>), the Feedback Controller converts them to message objects and &#x02013; depending on the kind of signal (Section <xref ref-type="sec" rid="s6">8</xref>) &#x02013; either processes them directly or passes them to the currently running Feedback.</p><p>The Feedback Controller starts new Feedbacks by spawning new Feedback processes. This has the advantage that a crashing or otherwise misbehaving Feedback application does not directly affect the Feedback Controller as it would do if we had used Threads. Since this framework also aims to be a workbench for easy Feedback development, misbehaving Feedbacks can be quite common, especially in the beginning of the Feedback development. The communication between two different processes however, is a bit more complicated than between two threads since two processes do not share the same address space. In our framework we solved this issue with an Inter-Process Communication mechanism using sockets to pass message objects back and forth between the Feedback Controller and a running Feedback (Section <xref ref-type="sec" rid="s7">9</xref>).</p></sec><sec><label>7.2</label><title>The graphical user interface</title><p>The framework provides a graphical user interface (GUI) which enables easy access to the Feedback Controller's main functions and the Feedback's variables. The GUI communicates with the Feedback Controller like the system connected to Pyff does: via XML over UDP. Therefore, the GUI does not have to run on the same machine as the Feedback Controller does, which is particularly useful for experiments where the experimenter and subject are in different rooms.</p><p>Figure <xref ref-type="fig" rid="F7">7</xref> shows a screen shot of the GUI. The drop down menu presents a list of available Feedbacks. On the right of this list are various buttons for Init, Play, Pause, etc. The main part of the GUI is occupied by a table which presents the name, value, and type of variables belonging to the currently running Feedback. The table is editable so that the user can modify any Feedback variable as desired and send it back to the Feedback where the change is directly applied. The possibility to inspect and manipulate the running Feedback's object variables gives a great deal of flexibility for experimenters to explore and try new settings.</p><fig id="F7" position="anchor"><label>Figure 7</label><caption><p><bold>The graphical user interface of the framework</bold>. Within the GUI the experimenter can select a Feedback, start, pause, and stop it and he can inspect and manipulate the Feedback's internal variables.</p></caption><graphic xlink:href="fnins-04-00179-g007"/></fig></sec><sec><label>7.3</label><title>The feedback base classes</title><p>The Feedback base class is the base class of all Feedbacks and the interface to the Feedback Controller's plugin system.</p><p>As mentioned in Section <xref ref-type="sec" rid="s5">7.1</xref>, the Feedback Controller is able to load, control, and unload Feedbacks dynamically. Feedbacks can be very different in complexity, functionality, and purpose. To work with different Feedbacks properly, the Feedback Controller relies on a small set of methods that every Feedback has to provide. The Feedback base class declares these methods and thus guarantees that from the Feedback Controller's point of view, a well defined set of operations is supported by every Feedback.</p><p>The methods are: <monospace>on_init, on_play, on_pause, on_stop, on_quit, on_interaction_event</monospace>, and <monospace>on_control_event</monospace>. The Feedback Controller calls them whenever it received a respective signal. For example: when the Feedback Controller receives a control signal, it calls the <monospace>on_control_event</monospace> method of the Feedback. For a complete overview which events cause which method calls in the Feedback, see Section <xref ref-type="sec" rid="s6">8</xref>.</p><p>Most of the above methods are parameterless, they are just called to let the Feedback know that a certain event just happened. Exceptions are <monospace>on_control_event</monospace> and <monospace>on_interaction_event</monospace>. These events carry an argument <italic>data</italic>, which is a dictionary containing all variables which where sent to the Feedback Controller and should be set in the Feedback. For convenience, a Feedback does not have to implement these two methods just to get the data, the Feedback Controller takes care that the data is already set in the Feedback before the respective method is called in the Feedback.</p><p>The Feedback base class also has a method <monospace>send_parallel(data)</monospace> which the can be used to send data to the parallel port of its host machine and this is a typical way to set markers into the acquired EEG.</p><p>For convenience the Feedback base class also provides a logger attribute. The logger attribute is a logger object of Python's standard logging facility. A logger is basically like the print statement with a severity (the loglevel) attached. A global severity threshold can be set which stops all log-messages below that level from appearing on the console or in the logfile. The global loglevel and the output format is configured by the Feedback Controller: The loglevel has a default value but it can be modified via the Feedback Controller's command line option. A Feedback programmer can use this logger directly without any extra initialization of the logger whatsoever.</p><p>By subclassing the Feedback base class, the derived class inherits all methods from the base class and thus becomes a valid and ready-to-use Feedback for the Feedback Controller. The Feedback programmer's task is to implement the methods as needed in a derived class or just leave the unneeded methods alone.</p><p>This object-oriented approach drastically simplifies the development of Feedbacks, since common code of similar Feedbacks can be moved out of the actual Feedbacks into a common base class. Figure <xref ref-type="fig" rid="F8">8</xref> shows an example: on the left side, two similar Feedbacks share a fair amount of code. Both Feedbacks work with a main loop. They have a set of main loop related methods and also more Feedback specific related methods. Implementing a third Feedback with a main loop means the programmer will likely copy the main loop from one of the existing Feedbacks into his new Feedback. This a bad approach for several reasons, the most important one being that a bug in the main loop related logic will probably appear in all of the Feedbacks. Due to code duplication, it will then have to be fixed every feedback using the logic. A solution is to extract the main loop related logic into a base class and implement it there (Figure <xref ref-type="fig" rid="F8">8</xref>, right). The Feedbacks can derive from this class, inherit the logic, and only need to implement the Feedback specific part. The code of the Feedbacks is much shorter, less error prone, and main loop related bugs can be fixed in a single file.</p><fig id="F8" position="anchor"><label>Figure 8</label><caption><p><bold>Left: Two Feedbacks sharing common code</bold>. Right: The common code was moved into a single base class.</p></caption><graphic xlink:href="fnins-04-00179-g008"/></fig></sec><sec><label>7.4</label><title>Feedbacks</title><p>Besides providing a platform for easy development of feedback applications, Pyff also provides a set of useful and ready-to-use Feedbacks. The list of those Feedbacks will grow as we and hopefully other groups will develop more of them.</p></sec></sec><sec id="s6"><label>8</label><title>Control- and Interaction Signals</title><p>Pyff receives two different kinds of signals: <italic>Control Signals</italic> and <italic>Interaction Signals</italic>. The difference between the two is that Interaction Signals can contain variables and commands and Control Signals only variables. The commands in the Interaction Signal are for the Feedback Controller to control the behavior of the Feedback, the variables contained in both the Control- and Interaction Signals are to be set in the currently running Feedback. Technically both variants are equivalent but practically they are used for different things: The variables in the Interaction Signal are used to modify the Feedback's object variables and therefore influence the Feedback's behavior. The variables in the Control Signal are the actual data coming from a data source like EEG. The distinction is useful for example when a Feedback needs to do something on every arriving block of data. It can then take advantage of the fact that incoming Control- and Interaction Signals trigger <monospace>on_control_event</monospace> respective <monospace>on_interaction_event</monospace> methods in the Feedback. The programmer of the Feedback can focus on the handling of the data by implementing <monospace>on_control_event</monospace> and does not have to worry if the variables sent by the signal are actual data or just object variables of the Feedback.</p><p>Table <xref ref-type="table" rid="T1">1</xref> shows the complete list of available commands in Control signals and Figure <xref ref-type="fig" rid="F9">9</xref> shows a sequence diagram illustrating the interaction between the GUI, the Feedback Controller, and the Feedback. All signals coming from the GUI are Interaction Signals, the signals coming from the BCI system are Control signals. Once the GUI is started, it tries to automatically connect to a running Feedback Controller on the machine where the GUI is running. If that fails (e.g., since the Feedback Controller is running on a different machine on the network) the experimenter can connect it manually by providing the host name or IP address of the target machine. Upon a successful connection with the Feedback Controller, the Feedback Controller replies with a list of available Feedbacks which is then shown in a drop down menu in the GUI. The experimenter can now select a Feedback and click the Init Button which sends the appropriate signal to the Feedback Controller telling it to load the desired Feedback. The Feedback Controller loads the Feedback and requests the Feedback's object variables which it sends back to the GUI. The GUI then shows them in a table where the experimenter can inspect and manipulate them. Within the GUI, the experimenter can also Start, Pause, Stop, and Quit the Feedback.</p><table-wrap id="T1" position="anchor"><label>Table 1</label><caption><p><bold>Available commands</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Command</th><th align="left" rowspan="1" colspan="1">Meaning</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">getfeedbacks</td><td align="left" rowspan="1" colspan="1">Return a list of available Feedbacks</td></tr><tr><td align="left" rowspan="1" colspan="1">getvariables</td><td align="left" rowspan="1" colspan="1">Return a dictionary of the Feedback's variables</td></tr><tr><td align="left" rowspan="1" colspan="1">sendinit</td><td align="left" rowspan="1" colspan="1">Load a Feedback</td></tr><tr><td align="left" rowspan="1" colspan="1">play</td><td align="left" rowspan="1" colspan="1">Start the Feedback</td></tr><tr><td align="left" rowspan="1" colspan="1">pause</td><td align="left" rowspan="1" colspan="1">Pause the Feedback</td></tr><tr><td align="left" rowspan="1" colspan="1">stop</td><td align="left" rowspan="1" colspan="1">Stop the Feedback</td></tr><tr><td align="left" rowspan="1" colspan="1">quit</td><td align="left" rowspan="1" colspan="1">Unload the Feedback</td></tr></tbody></table></table-wrap><fig id="F9" position="anchor"><label>Figure 9</label><caption><p><bold>The different kinds of signals and the events they cause in the Feedback</bold>. 1, The GUI connects to the Feedback Controller; 2, The user selects a Feedback; 3, The user starts the Feedback; 4, The BCI System provides EEG data; 5&#x02013;7, The user pauses, stops, and quits the Feedback</p></caption><graphic xlink:href="fnins-04-00179-g009"/></fig></sec><sec id="s7"><label>9</label><title>Inter-Process Communication</title><p>When writing applications, there are usually two possible options to achieve concurrency: The first one is to use threads, the second is to use processes. Processes are heavyweight compared to threads. Processes have their own address space and some form of inter-process communication (IPC) is needed to allow processes to interact with each other. Threads on the other side are lightweight, there can be one ore more thread in the same process and threads of the same process share their address space. The implication of the shared address space is, that a modification of a variable within one thread is immediately visible to all other threads of this process. A complete description of processes, threads, and inter-process communication can be found in (Tanenbaum, <xref ref-type="bibr" rid="B36">2001</xref>). Since there is no need for a IPC when using threads, threads are often more desirable than processes and a sufficient solution for many common concurrent applications. In Python however things are a bit different. First, and most importantly: in Python two threads of a process do not run truly concurrently, but sequentially in a time-sliced manner. The reason is Python's global interpreter lock (GIL) which ensures that only one Python thread can execute in the interpreter at once (Lutz, <xref ref-type="bibr" rid="B19">2006</xref>)<xref ref-type="fn" rid="fn11">11</xref>. The consequence is that Python programs cannot make use of multi processors using Python's threads. In order to use real concurrency one has to use processes, effectively sidestepping the GIL. The second important point is that many important graphical Python libraries like pygame or PyQT need to run in the main (first) thread of the Python process. In a threaded version of Pyff this would make things a lot more complicated, since the Feedback Controller as a server runs during the whole lifetime of the experiment, while Feedbacks (where those libraries are used) usually get loaded, unloaded, started and stopped many times during an experiment. The natural way to implement this in a threaded way would be to run the Feedback Controller in the main thread and let it spawn Feedback threads as needed. Doing it the other way round by reserving the main thread for the Feedback and letting the Feedback Controller insert Feedbacks into the first thread on demand would be a lot more complicated and error prone.</p><p>For that reasons we decided to use processes instead of threads. The feedback applications have much more resources running in their own process (even on their own processor) while keeping the programming of Pyff's logic simple and easy to maintain.</p><sec><label>9.1</label><title>Inter-process communication in pyff</title><p>We decided to use a socket based IPC since it works on all major platforms. The basic idea is that two processes establish a TCP connection to communicate. If a peer wants to send a message to the other one, it uses his end of the connection-called socket-to send the message. The message is then sent to the other end of the connection, where the second peer can read it from his socket (Lutz, <xref ref-type="bibr" rid="B19">2006</xref>).</p><p>The messages itself are message objects which are serialized before being sent and unserialized on the receiving end. Serialization and Unserialization is done using Python's pickle library.</p></sec></sec><sec><label>10</label><title>Remotely Controlling Pyff</title><p>To make the framework as portable as possible to other existing systems, it was critical to design an interface generic enough to support a wide range of programming languages and operating systems. This loose coupling is achieved through our choice to use the UDP for the transport of the data through the network and XML for the encoding of the signal.</p><p>User datagram protocol was chosen because it is a well known and established standard network protocol and because virtually every programming language supports it. UDP-clients and -servers are fairly easy to implement and it is possible to send arbitrary data over UDP. For similar reasons, XML was chosen for the encoding of the signals: XML is an established standard for exchanging data and libraries for parsing and writing XML are available for most common programming languages.</p><p>Once the FeedbackController starts, it listens on UDP port 12345 for incoming control- and interaction signals. Clients can remotely control the FeedbackController as they would using the GUI by sending signals to that address. The signals have to be wrapped in XML. Documentation for Pyff's XML scheme is given in Section <xref ref-type="sec" rid="s8">11</xref>.</p></sec><sec id="s8"><label>11</label><title>Pyff's XML Scheme</title><p>To wrap the content of the control- and interaction-signals, we chose XML. XML is a well known, accepted standard and was specifically designed for tasks like this, where arbitrary data needs to be exchanged between different systems<xref ref-type="fn" rid="fn12">12</xref>.</p><p>The Feedback Controller accepts a defined set of <italic>commands</italic> and arbitrary <italic>variables</italic> which should be set in the Feedback. A command is a simple string and has no parameters, they are not arbitrary Python statements but a well defined set of commands that the Feedback Controller executes, like loading a Feedback or starting it. A variable is defined as a triple (<italic>type</italic>,<italic>name</italic>,<italic>value</italic>), where <italic>type</italic> is the data type, <italic>name</italic> the name of the variable and <italic>value</italic> the actual value of the variable.</p><p>For this purpose, we created an XML scheme capable of containing variables and commands. The following describes the version 1.0 of the scheme.</p><p>The root element of this XML scheme is the <italic>bci-signal</italic>-node which contains an attribute <italic>version</italic>, defining the bci-signal version and one child node which can be either of the type interaction- or control signal.</p><preformat position="float" xml:space="preserve">
<monospace>
&#x0003c;?xml version=&#x0201c;1.0&#x0201d;?&#x0003e;
&#x0003c;bci-signal version=&#x0201c;1.0&#x0201d;&#x0003e;
   &#x0003c;interaction-signal&#x0003e;
   &#x0003c;command value=&#x0201c;start&#x0201d;/&#x0003e;
   &#x0003c;s name=&#x0201d;string&#x0201c; value=&#x0201d;foo&#x0201d;/&#x0003e;
   &#x0003c;f name=&#x0201d;float&#x0201c; value=&#x0201d;0.69&#x0201d;/&#x0003e;
   &#x0003c;list name=&#x0201c;list&#x0201d;&#x0003e;
      &#x0003c;/i value=&#x0201c;1&#x0201d;/&#x0003e;
      &#x0003c;/i value=&#x0201c;2&#x0201d;/&#x0003e;
      &#x0003c;/i value=&#x0201c;3&#x0201d;/&#x0003e;
&#x0003c;/list&#x0003e;
 &#x0003c;/interaction-signal&#x0003e;
&#x0003c;/bci-signal&#x0003e;
</monospace>
</preformat><sec><title/><sec><title>Commands</title><p>Commands are only allowed in interaction signals and have the following form:</p><p><monospace>&#x0003c;command value=&#x0201d;commandname&#x0201d;/&#x0003e;</monospace></p><p>where <italic>commandname</italic> is a member of the set of supported commands. Supported commands are: <italic>getfeedbacks</italic>, <italic>play</italic>, <italic>pause</italic>, <italic>stop</italic>, <italic>quit</italic>, <italic>sendinit</italic>, <italic>getvariables</italic>. Only one command is allowed per interaction signal. The meaning of the commands is explained in Section <xref ref-type="sec" rid="s6">8</xref>. An interaction signal can contain a command and several variables.</p></sec><sec><title>Variables</title><p>Variables are allowed in interaction- and control signals. A variable is represented by the triple (<italic>type</italic>,<italic>name</italic>,<italic>value</italic>) and has the following form in XML:</p><p><monospace>&#x0003c;type name=&#x0201d;varname&#x0201d; value=&#x0201d;varvalue&#x0201d;&#x0003e;</monospace></p><p>for example: an Integer with the variable name &#x0201c;foo&#x0201d; and the value 42 would be represented as:</p><p><monospace>&#x0003c;integer name=&#x0201d;foo&#x0201d; value=&#x0201d;42&#x0201d;&#x0003e;</monospace></p><p>The XML scheme supports all variable types supported by Python, Table <xref ref-type="table" rid="T2">2</xref> shows a complete listing. Sometimes there is more than one way to express the type of a variable. For example, a boolean can be expressed in this XML scheme via <monospace>&#x0003c;boolean&#x02026; /&#x0003e;, &#x0003c;bool&#x02026; /&#x0003e;</monospace> and <monospace>&#x0003c;b&#x02026; /&#x0003e;</monospace>. All alternatives are equivalent and exist merely for convenience.</p><table-wrap id="T2" position="anchor"><label>Table 2</label><caption><p><bold>Data types supported by our XML scheme</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Type</th><th align="left" rowspan="1" colspan="1">Type in XML</th><th align="left" rowspan="1" colspan="1">Example values</th><th align="left" rowspan="1" colspan="1">Nestable</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Boolean</td><td align="left" rowspan="1" colspan="1">boolean, bool, b</td><td align="left" rowspan="1" colspan="1">&#x0201c;True,&#x0201d; &#x0201c;true,&#x0201d; &#x0201c;1&#x0201d;</td><td align="left" rowspan="1" colspan="1">No</td></tr><tr><td align="left" rowspan="1" colspan="1">Integer</td><td align="left" rowspan="1" colspan="1">integer, int, i</td><td align="left" rowspan="1" colspan="1">&#x0201c;1&#x0201d;</td><td align="left" rowspan="1" colspan="1">No</td></tr><tr><td align="left" rowspan="1" colspan="1">Float</td><td align="left" rowspan="1" colspan="1">float, f</td><td align="left" rowspan="1" colspan="1">&#x0201c;1.0&#x0201d;</td><td align="left" rowspan="1" colspan="1">No</td></tr><tr><td align="left" rowspan="1" colspan="1">Long</td><td align="left" rowspan="1" colspan="1">long, l</td><td align="left" rowspan="1" colspan="1">&#x0201c;1&#x0201d;</td><td align="left" rowspan="1" colspan="1">No</td></tr><tr><td align="left" rowspan="1" colspan="1">Complex</td><td align="left" rowspan="1" colspan="1">complex, cmplx, c</td><td align="left" rowspan="1" colspan="1">&#x0201c;(1&#x02009;+&#x02009;0j),&#x0201d; &#x0201c;(1&#x02009;+&#x02009;0i)&#x0201d;</td><td align="left" rowspan="1" colspan="1">No</td></tr><tr><td align="left" rowspan="1" colspan="1">String</td><td align="left" rowspan="1" colspan="1">string, str, s</td><td align="left" rowspan="1" colspan="1">&#x0201c;foo&#x0201d;</td><td align="left" rowspan="1" colspan="1">No</td></tr><tr><td align="left" rowspan="1" colspan="1">List</td><td align="left" rowspan="1" colspan="1">list</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Yes</td></tr><tr><td align="left" rowspan="1" colspan="1">Tuple</td><td align="left" rowspan="1" colspan="1">tuple</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Yes</td></tr><tr><td align="left" rowspan="1" colspan="1">Set</td><td align="left" rowspan="1" colspan="1">set</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Yes</td></tr><tr><td align="left" rowspan="1" colspan="1">Frozenset</td><td align="left" rowspan="1" colspan="1">frozenset</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Yes</td></tr><tr><td align="left" rowspan="1" colspan="1">Dictionary</td><td align="left" rowspan="1" colspan="1">dict</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Yes</td></tr><tr><td align="left" rowspan="1" colspan="1">None</td><td align="left" rowspan="1" colspan="1">None</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">No</td></tr></tbody></table></table-wrap></sec><sec><title>Nested variables</title><p>Some variables can contain other variables, like Lists (Vector, Array) or Dictionaries (Hashes, Hash Tables). The following example shows the XML representation of a List called <italic>mylist</italic> containing three Integers:</p><preformat position="float" xml:space="preserve">
<monospace>
&#x0003c;list name=&#x0201c;mylist&#x0201d;&#x0003e;
 &#x0003c;i value=&#x0201c;1&#x0201d;/&#x0003e;
 &#x0003c;i value=&#x0201c;2&#x0201d;/&#x0003e;
 &#x0003c;i value=&#x0201c;3&#x0201d;/&#x0003e;
&#x0003c;/list&#x0003e;
</monospace>
</preformat><p>Nested variables can also contain other nested variables. The following example shows a list containing two integers and a list which also contains two integers:</p><preformat position="float" xml:space="preserve">
<monospace>
&#x0003c;list name=&#x0201c;mylist2&#x0201d;&#x0003e;
 &#x0003c;i value=&#x0201c;1&#x0201d;/&#x0003e;
 &#x0003c;i value=&#x0201c;2&#x0201d;/&#x0003e;
 &#x0003c;list&#x0003e;
  &#x0003c;i value=&#x0201c;3&#x0201d;/&#x0003e;
  &#x0003c;i value=&#x0201c;4&#x0201d;/&#x0003e;
 &#x0003c;/list&#x0003e;
&#x0003c;/list&#x0003e;
</monospace>
</preformat><p>No restrictions are put upon the depth of the nested variables.</p><p>Special care has to be taken using Dictionaries. Dictionaries are not simply collections of variables but collections of mappings from Strings to a Variables. The mappings are expressed through Tuples (String, Variable) and consequently, a Dictionary is expressed as a collection of Tuples in our XML scheme. The following example shows a dictionary <italic>mydict</italic> containing three mappings (foo, 1), (bar, 2) and (baz, 3):</p><preformat position="float" xml:space="preserve">
<monospace>
&#x0003c;dict name=&#x0201c;mydict&#x0201d;&#x0003e;
  &#x0003c;tuple&#x0003e;
   &#x0003c;s value=&#x0201c;foo&#x0201d;/&#x0003e;
   &#x0003c;i value=&#x0201c;1&#x0201d;/&#x0003e;
  &#x0003c;/tuple&#x0003e;
  &#x0003c;tuple&#x0003e;
   &#x0003c;s value=&#x0201c;bar&#x0201d;/&#x0003e;
   &#x0003c;i value=&#x0201c;2&#x0201d;/&#x0003e;
  &#x0003c;/tuple&#x0003e;
  &#x0003c;tuple&#x0003e;
   &#x0003c;s value=&#x0201c;baz&#x0201d;/&#x0003e;
   &#x0003c;i value=&#x0201c;3&#x0201d;/&#x0003e;
  &#x0003c;/tuple&#x0003e;
&#x0003c;/dict&#x0003e;
</monospace>
</preformat><p>This XML scheme provides a well defined and clean interface for other systems to communicate with Pyff: we provide a set of commands to control the Feedback Controller and the Feedback and a way to read and write the Feedback's variables. The whole protocol is operating system and programming language independent. It is simple enough to grasp it without much effort but generic enough to send any kind of data to the Feedback.</p></sec></sec></sec><sec><label>12</label><title>Documentation and Examples</title><p>Part of the framework is a complete documentation of the system and its interfaces. All modules, classes, and methods are also extensively documented in form of Python docstrings. Those docstrings are used by integrated development environments (IDEs) and tools like Python's pydoc to generate help and documentation from the source files.</p><p>The framework also provides Tutorials explaining every major aspect of Feedback development with example Feedbacks.</p></sec><sec id="s9"><label>13</label><title>Example Feedback</title><p>This section contains a complete listing of the TestD2 Feedback, a Feedback implementing a computer version of the classic d2 test of attention, developed by Brickenkamp and Zillmer (<xref ref-type="bibr" rid="B6">1998</xref>) as the paper-and-pencil test. The listing is complete and functionally identical with the TestD2 Feedback delivered with Pyff, however many blank lines, comments, and the copyright statement in the beginning of the file were removed make it shorter. The following subsections correspond to one or two methods of the TestD2 class. The complete listing of the TestD2 module is given at the end of this section.</p><p>In the paper-and-pencil version, the test consists of the letters <italic>d</italic> and <italic>p</italic>, which are printed on a sheet of paper in 14 lines with 47 letters per line. Each letter has between one and four vertical lines above or below. The subject's task is to cross out all occurrences of the letter <italic>d</italic> with two lines (target) on the current line as fast and correct as possible. A <italic>d</italic> with more or less than two lines or a <italic>p</italic> (non-targets) must not be crossed out. The subject usually has 20&#x02009;s to process a line. After those 20&#x02009;s the experimenter gives a signal and the subject has to process the next line. The number of mistakes (erroneously crossed out non d2s or not crossed out d2s) can be used afterward to quantify the attention of the subject.</p><p>Our computerized version of TestD2 differs from the paper-and-pencil variant in the following ways: The stimuli are not presented in rows which have to be processed in a given short amount of time, but are presented one by one on the screen. For each stimulus the subject has to decide whether it is a target or a non-target by pressing the according key on the keyboard. Figure <xref ref-type="fig" rid="F10">10</xref> shows screenshots of the running Feedback. In the paper-and-pencil version, the subject has to process 14 rows of 47 symbols and has 20&#x02009;s per row. In the standard configuration of our Feedback we present 14&#x02009;&#x000b7;&#x02009;20 symbols and give the user a maximum of 14&#x02009;&#x000b7;&#x02009;(20/47) s. Of course this makes the results of our version not directly comparable with the paper-and-pencil variant, we think however that it still makes a good demonstration on how to implement a sophisticated feedback application with our framework.</p><fig id="F10" position="anchor"><label>Figure 10</label><caption><p><bold>Screenshots of the stimuli presented by the Feedback</bold>. A stimulus is presented until the subject presses one of two available buttons to decide if he sees a d2 or not. After the key is pressed the next stimulus is presented immediately.</p></caption><graphic xlink:href="fnins-04-00179-g010"/></fig><sec><label>13.1</label><title>Initialization of the feedback</title><p>The TestD2 (Listing 1) class is derived from the PygameFeedback Class. PygameFeedback takes care of proper initialization and shutdown of Pygame before and after the Feedback runs. It also provides helpful methods and members which make dealing with Pygame much easier.</p><p>The <monospace>init</monospace> method of TestD2 sets various variables controlling the behavior of the Feedback. In the first line of <monospace>init</monospace> the <monospace>init</monospace> method of the parent class is called. This is necessary since the parent's <monospace>init</monospace> declares variables needed to make PygameFeedback's methods work. <monospace>caption</monospace> sets the caption text of the Pygame window, <monospace>random_seed</monospace> sets the seed for the random number generator. This is important to make experiments reproducible when using random numbers. <monospace>number_of_symbols, seconds_per_symbol</monospace>, and <monospace>targets_persent</monospace> set how many symbols are presented at most, how much time the subject has to process one symbol and the percentage of target symbols of all symbols presented. The number of symbols and seconds per symbol are used to calculate the duration of the experiment. The three numbers are taken from the standard Test D2 condition where a subject has 14 lines with 47 symbols per line and 20&#x02009;s time per line. <monospace>color, backgroundColor, and fontheight</monospace> control the look of the Feedback, <monospace>color</monospace> is for the color of the symbols, <monospace>backgroundColor</monospace> for the color of the background and <monospace>fontheight</monospace> the height of the font in pixels. <monospace>key_target</monospace> and <monospace>key_nontarget</monospace> are the keys on the keyboard the user has to click if s/he wants to mark a target or a non-target.</p><p>All variables declared in this method are immediately visible in the GUI after the Feedback is loaded. The experimenter can modify them as he likes and set them in the Feedback.</p></sec><sec><label>13.2</label><title>Pre- and postmainloop</title><p>Pre- and postmainloop (Listings 2 and 3) are invoked respectively, immediately before after the mainloop of the Feedback. Since the mainloop of a Feedback can be invoked several times during it's lifetime, variables which need to be initialized before each run should be set in <monospace>pre_mainloop</monospace> and evaluations of the run should be done in <monospace>post_mainloop</monospace>.</p><p>In <monospace>pre_mainloop</monospace> we call the parent's <monospace>pre_mainloop</monospace> which initializes Pygame. Then we generate the sequence of stimuli (see Section <xref ref-type="sec" rid="s10">13.4</xref>), the graphics for the stimuli (see Section <xref ref-type="sec" rid="s11">13.5</xref>). The variables <monospace>current_index</monospace> represents the current position in the list of stimuli, <monospace>e1</monospace> and <monospace>e2</monospace> are the number of errors of omission and errors of commission. The <monospace>QUIT</monospace> event is then scheduled. This event will appear in Pygame's event queue after the given time and marks the regular end of the Feedback. The current time is measured and the first stimulus presented and the mainloop starts.</p><p>In <monospace>post_mainloop</monospace> we measure the time needed to run the experiment by calling <monospace>clock.tick()</monospace> a second time which returns the time in milliseconds since the last call. Then we call the parent's <monospace>post_mainloop</monospace> which cleanly shuts down Pygame. After that, various results of the experiment are calculated and printed out.</p></sec><sec><label>13.3</label><title>Tick</title><p>The <monospace>tick</monospace> method (Listing 4) is called repeatedly during the mainloop by the MainloopFeedback which is a parent Feedback of the PygameFeedback. Usually the <monospace>tick</monospace> method of PygameFeedback limits the Framerate by calling <monospace>self.clock.tick(self.FPS)</monospace> and processes Pygame's event queue. This is the desired behavior in most Pygame Feedbacks containing a mainloop. In this case, however the Feedback is not paced by small timesteps, but by the keypresses of the subject, so we overwrite the parent's <monospace>tick</monospace> to omit the call of <monospace>self.clock.tick</monospace> and call <monospace>self.wait_for_pygame_event</monospace> instead of <monospace>self.process_pygame_events</monospace>. This means tick waits at this call until the user pressed a key (or another Pygame event appears in the event queue). Then the key is checked if it is one of the two available keys and if the correct key was pressed. If not, the corresponding error accumulator is increased by one. Then the position in the list of stimuli is increased by one. If the index reached the end of the list, the Feedback is stopped, otherwise the next stimulus is presented (Listing 7).</p></sec><sec id="s10"><label>13.4</label><title>Generating the d2 list</title><p>The method <monospace>generate_d2list</monospace> (Listing 5) sets the seed for the random number generator to make the results reproducible and generates the list of targets and non-targets. It uses the <monospace>targets_percent</monospace> attribute to obey the correct ratio of targets and non-targets in the list. The method also ensures that a symbol never appears twice or more in a row.</p></sec><sec id="s11"><label>13.5</label><title>Generating the symbols</title><p>The method <monospace>generate_symbols</monospace> (Listing 6) generates the images, or surfaces in Pygame lingo, for the various symbols and stores them in an object attribute so the Feedback can later use them directly when painting them on the screen. The Symbols are generated as a combination of a letter and one of two possible lines above and below the letter. The method first generates the images for the letters then the images for the lines and glues them together in the last loop.</p></sec></sec><sec><label>14</label><title>Using Pyff</title><p>After downloading and extracting Pyff from<xref ref-type="fn" rid="fn7">7</xref> there are two directories: src and tools. To start Pyff go into src and run FeedbackController.py. This will start the Feedback Controller and the GUI. The control elements of the GUI are explained in Figure <xref ref-type="fig" rid="F11">11</xref>.</p><fig id="F11" position="anchor"><label>Figure 11</label><caption><p><bold>The control elements of the GUI</bold>. 1, Filter; 2, Clear Button; 3, Connect Button; 4, Feedback Selector; 5, Init Button; 6, Send Button; 7, Refresh Button; 8, Play-, Pause, and Stop Buttons; 9, Quit Button.</p></caption><graphic xlink:href="fnins-04-00179-g011"/></fig><p>When the GUI has started, it will automatically ask the Feedback Controller for available Feedbacks and will populate the Feedback Selector. The experimenter selects the desired Feedback from there and loads it by pushing the Init button. The Feedback Controller will now load the Feedback and send the Feedback's object variables back to the GUI where they are presented in the Table. The experimenter can now modify variables as needed and apply the changes in the Feedback by pushing the Send button.</p><p>Up to this point, the Feedback has just been loaded and initialized but not yet started. To start, pause and stop the Feedback the experimenter pushes the Play-, Pause- and Stop buttons. A Feedback can be started, stopped and paused unlimited times during the Feedback's lifetime.</p><p>To quit the Feedback the experimenter uses the quit button. Quitting the Feedback will stop and unload the Feedback from the Feedback Controller. Selecting and loading a Feedback while another one is already running, will stop and quit the running Feedback and load the new one afterwards.</p><sec><label>14.1</label><title>Feedback controller's options</title><p>The Feedback Controller supports various options. To get a complete listing start the Feedback Controller with the <monospace>&#x02206;help</monospace> parameter.</p><sec><title>Feedback controller's loglevel</title><p>The <monospace>--loglevel=LEVEL</monospace> parameter sets the loglevel of the Feedback Controller and it's components. Accepted levels in increasing order are: notset, debug, info, warning, error and critical. Setting the loglevel will cause the Feedback Controller to output certain log messages of the given level and higher.</p></sec><sec><title>Feedback's loglevel</title><p>When developing Feedback applications it is important to have a logger dedicated for Feedbacks. Pyff provides a logger for Feedbacks which is configurable separately from the Feedback Controller's loglevel. The <monospace>--fb-loglevel=LEVEL</monospace> parameter controls the loglevel of the Feedback's logger. It accepts the same values as the <monospace>--loglevel</monospace> parameter.</p></sec><sec><title>Additional feedback directory</title><p>When developing Feedback applications which are not to be included in the Pyff framework, it might be desirable to locate them in a different directory than Pyff's default directory for Feedbacks. The <monospace>--additional-feedback-path=PATH</monospace> parameter supports this, by causing the Feedback Controller to additionally search for Feedbacks under the given path.</p></sec><sec><title>Starting without GUI</title><p>When running the Feedback Controller in an automated experiment setup where the interaction signals are emitted from a different source than the GUI, the experimenter can start the Feedback Controller without the GUI via the <monospace>--nogui</monospace> parameter.</p></sec><sec><title>Configuring the parallel port</title><p>On different computers the parallel port sometimes has a different address than the default. The <monospace>--port=PORTNUM</monospace> parameter configures the parallel port address (in hexadecimal) the Feedback Controller tries to use.</p><preformat position="float" xml:space="preserve">
<monospace>
Listing 1: First par of the file
1  <bold>import</bold> random
2  <bold>import</bold> pygame
3  <bold>from</bold> FeedbackBase.PygameFeedback
   <bold>import</bold> PygameFeedback
4
5  TARGETS=[&#x02018;<bold>d11</bold>&#x02019;, &#x02018;<bold>d20</bold>&#x02019;, &#x02018;<bold>d02</bold>&#x02019;]
6  NON_TARGETS=[&#x02018;<bold>d10</bold>&#x02019;, &#x02018;<bold>d01</bold>&#x02019;, &#x02018;<bold>d21</bold>&#x02019;, &#x02018;<bold>d12</bold>&#x02019;, &#x02018;<bold>d22</bold>&#x02019;,
7  &#x02018;<bold>p10</bold>&#x02019;, &#x02018;<bold>p01</bold>&#x02019;, &#x02018;<bold>p11</bold>&#x02019;, &#x02018;<bold>p20</bold>&#x02019;, &#x02018;<bold>p02</bold>&#x02019;, &#x02018;<bold>p21</bold>&#x02019;, &#x02018;<bold>p12</bold>&#x02019;, &#x02018;<bold>p22</bold>&#x02019;]
8
9  <bold>class</bold> TestD2(PygameFeedback):
10
11  <bold>def</bold> init(self):
12     PygameFeedback.init(self)
13     self.caption=&#x0201d;Test D2&#x0201d;
14     self.random_seed=1234
15     self.number_of_symbols=47 * 14
16     self.seconds_per_symbol=20 / 47.
17     self.targets_percent=45.45
18     self.color=[0, 0, 0]
19     self.backgroundColor=[127, 127, 127]
20     self.fontheight=200
21     self.key_target=&#x0201d;f&#x0201d;
22     self.key_nontarget=&#x0201d;j&#x0201d;
Listing 2: pre_mainloop
1 <bold>def</bold> pre_mainloop(self):
2     PygameFeedback.pre_mainloop(self)
3     self.generate_d2list()
4     self.generate_symbols()
5     self.current_index+0
6     self.e1+0
7     self.e2+0
8     pygame.time.set_timer(pygame.QUIT, self. number_of_symbols * self. seconds_per_symbol * 1000)
9     self.clock.tick()
10    self.present_stimulus()
Listing 3: post_mainloop
1 <bold>def</bold> post_mainloop(self):
2     elapsed_seconds+self.clock.tick() / 1000.
3     PygameFeedback.post_mainloop(self)
4     tn+self.current_index&#x02009;+&#x02009;1
5     error=self.e1&#x02009;+ self.e2
6     error_rate=100. * error / tn
7     correctly_processed = tn - error
8     cp=correctly_processed - self.e2
9     rt_avg=elapsed_seconds / tn
10    <bold>print</bold> &#x0201d;<bold>&#x0003e;Results</bold>:&#x0201d;
11    <bold>print</bold> &#x0201c;&#x02009;========&#x0201d;
12     <bold>print</bold>
13     <bold>print</bold> &#x0201d;<bold>&#x0003e;Processed symbols: %i of %i</bold>&#x0201d; % (tn, self.number_of_symbols)
14     <bold>print</bold> &#x0201d;<bold>&#x0003e;Elapsed time: %f sec</bold>&#x0201d; % elapsed_seconds
15     <bold>print</bold> &#x0201d;<bold>&#x0003e;Correctly processed symbols: %i</bold>&#x0201d; % (correctly_processed)
16     <bold>print</bold> &#x0201d;<bold>&#x0003e;Percentage of Errors: %f</bold>&#x0201d; % (error_rate)
17     <bold>print</bold> &#x0201d;<bold>&#x0003e;Errors: %i</bold>&#x0201d; % error
18     <bold>print</bold> &#x0201c;<bold>&#x02026; errors of omission: %i</bold>&#x0201d; % self.e1
19     <bold>print</bold> &#x0201c;<bold>&#x02026; errors of commission: %i</bold>&#x0201d; % self.e2
20     <bold>print</bold> &#x0201d;<bold>&#x0003e;Concentration Performance: %i</bold>&#x0201d; % cp
21     <bold>print</bold> &#x0201d;<bold>&#x0003e;Average reaction time: %f sec</bold>&#x0201d; % rt_avg
Listing 4: tick
 1 <bold>def</bold> tick(self):
 2     self.wait_for_pygame_event()
 3     <bold>if</bold> self.keypressed:
 4       key=self.lastkey_unicode
 5       self.keypressed=False
 6       <bold>if</bold> key <bold>not in</bold> (self.key_target, self. key_nontarget):
 7          <bold>return</bold>
 8        <bold>else</bold>:
 9            <bold>if</bold> key == self.key_nontarget
10            <bold>and</bold> self.d2list[self. current_index] <bold>in</bold> TARGETS:
11            self.e1+= 1
12            <bold>elif</bold> key == self.key_target
13            <bold>and</bold> self.d2list[self. current_index] <bold>in</bold> NON_TARGETS:
14                self.e2 += 1
15            <bold>else</bold>:
16               <bold>pass</bold>
17            self.current_index&#x02009;+= 1
18            <bold>if</bold> self.current_index &#x0003e; self.number_of_symbols - 1:
19 self.on_stop()
20 <bold>else</bold>:
21 self.present_stimulus()
Listing 5: generate_d2list
 1 <bold>def</bold> generate_d2list(self):
 2     random.seed(self.random_seed)
 3     targets=int(round(self.number_of_symbols * self.targets_percent / 100))
 4     non_targets=int(self.number_of_symbols - targets)
 5     l=[random.choice(TARGETS) <bold>for</bold> i <bold>in</bold> range(targets)] +&#x02009;\
 6     [random.choice(NON_TARGETS) <bold>for</bold> i <bold>in</bold> range(non_targets)]
 7     random.shuffle(l)
 8     <bold>for</bold> i <bold>in</bold> range(len(l) - 1):
 9         <bold>if</bold> l[i]== l[i +&#x02009;1]:
10             pool=TARGETS <bold>if</bold> l[i] <bold>in</bold> TARGETS <bold>else</bold> NON_TARGETS
11         new=random.choice(pool)
12         while new == l[i +&#x02009;1]:
13         new=random.choice(pool)
14         l[i]=new
15     self.d2list=l
Listing: 6 generate_symbols
 1 <bold>def</bold> generate_symbols(self):
 2     linewidth=self.fontheight / 11
 3     font=pygame.font.Font(None, self.fontheight)
 4     surface_d=font.render(&#x0201d;<bold>d</bold>&#x0201d;, True, self.color)
 5     surface_p=font.render(&#x0201d;<bold>p</bold>&#x0201d;, True, self.color)
 6     width, height=surface_d.get_size()
 7     surface_l1=pygame.Surface((width, height), pygame.SRCALPHA)
 8     surface_l2=pygame.Surface((width, height), pygame.SRCALPHA)
 9     pygame.draw.line(surface_l1, self.color,
10      (width / 2, height / 10),
11      (width / 2, height - height / 10), linewidth)
12     pygame.draw.line(surface_l2, self.color,
13      (width / 3, height / 10),
14      (width / 3, height - height / 10), linewidth)
15     pygame.draw.line(surface_l2, self.color,
16      (2 * width / 3, height / 10),
17      (2 * width / 3, height - height / 10), linewidth)
18     self.symbol= {}
19     <bold>for</bold> symbol <bold>in</bold> TARGETS&#x02009;+ NON_TARGETS:
20      surface =&#x02009;pygame.Surface((width, height * 3), pygame.SRCALPHA)
21      letter= surface_d <bold>if</bold> symbol[0]= = &#x02018;<bold>d</bold>&#x02019; <bold>else</bold> surface_p
22      surface.blit(letter, (0, height))
23      <bold>if</bold> symbol[1]= = &#x02018;<bold>1</bold>&#x02019;:
24      surface.blit(surface_l1, (0, 0))
25      <bold>elif</bold> symbol[1]= = &#x02018;<bold>2</bold>&#x02019;:
26      surface.blit(surface_l2, (0, 0))
27      <bold>if</bold> symbol[2]= = &#x02018;<bold>1</bold>&#x02019;:
28      surface.blit(surface_l1, (0, 2 * height))
29      <bold>elif</bold> symbol[2]= = &#x02018;<bold>2</bold>&#x02019;:
30      surface.blit(surface_l2, (0, 2 * height))
31      self.symbol[symbol]=surface
Listing 7: present_stimulus
1 <bold>def</bold> present_stimulus(self):
2     self.screen.fill(self.backgroundColor)
3     symbol=self.d2list[self.current_index]
4     self.screen.blit(self.symbol[symbol],
5     self.symbol[symbol].get_rect(center=self. screen.get_rect().center))
6     pygame.display.flip()
Lisiting 8: End of file
1<bold>if</bold> __name__ == &#x0201c;<bold>__main__</bold>&#x0201d;:
2     fb=TestD2()
3     fb.on_init()
4     fb.on_play()
</monospace>
</preformat></sec></sec></sec></app></app-group><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Acqualagna</surname><given-names>L.</given-names></name><name><surname>Treder</surname><given-names>M. S.</given-names></name><name><surname>Schreuder</surname><given-names>M.</given-names></name><name><surname>Blankertz</surname><given-names>B.</given-names></name></person-group> (<year>2010</year>). <article-title>A novel brain&#x02013;computer interface based on the rapid serial visual presentation paradigm, Proc</article-title>. <source>32nd Ann. Int. IEEE EMBS Conf</source>. <fpage>2686</fpage>&#x02013;<lpage>2689</lpage></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Beazley</surname><given-names>D. M.</given-names></name></person-group> (<year>1996</year>). <article-title>Swig: an easy to use tool for integrating scripting languages with c and c++, TCLTK&#x02019;96</article-title>: <conf-name>Proceedings of the 4th conference on USENIX Tcl/Tk Workshop, 1996, USENIX Association</conf-name>, <conf-loc>Berkeley, CA, USA</conf-loc>, <year>1996</year>, pp. <fpage>15</fpage>&#x02013;<lpage>15</lpage> URL <uri xlink:type="simple" xlink:href="http://www.swig.org/">http://www.swig.org/</uri></mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blankertz</surname><given-names>B.</given-names></name><name><surname>Krauledat</surname><given-names>M.</given-names></name><name><surname>Dornhege</surname><given-names>G.</given-names></name><name><surname>Williamson</surname><given-names>J.</given-names></name><name><surname>Murray-Smith</surname><given-names>R.</given-names></name><name><surname>M&#x000fc;ller</surname><given-names>K.-R.</given-names></name></person-group> (<year>2007</year>). <article-title>A note on brain actuated spelling with the Berlin brain-computer interface</article-title>. <source>Lect. Notes Comput. Sci.</source><volume>4555</volume>, <fpage>759</fpage><pub-id pub-id-type="doi">10.1007/978-3-540-73281-5_83</pub-id></mixed-citation></ref><ref id="B4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>D.</given-names></name></person-group> (<year>1997</year>). <article-title>The psychophysics toolbox</article-title>. <source>Spat. Vis.</source><volume>10</volume>, <fpage>433</fpage>&#x02013;<lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id><pub-id pub-id-type="pmid">9176952</pub-id></mixed-citation></ref><ref id="B5"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Brickenkamp</surname><given-names>R.</given-names></name></person-group> (<year>1972</year>). <source>Test d2</source>. <publisher-loc>G&#x000f6;ttingen, Germany</publisher-loc>: <publisher-name>Hogrefe Verlag f&#x000fc;r Psychologie</publisher-name></mixed-citation></ref><ref id="B6"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Brickenkamp</surname><given-names>R.</given-names></name><name><surname>Zillmer</surname><given-names>E.</given-names></name></person-group> (<year>1998</year>). <source>D2 Test of Attention</source>. <publisher-loc>G&#x000f6;ttingen, Germany</publisher-loc>: <publisher-name>Hogrefe and Huber</publisher-name></mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brouwer</surname><given-names>A.-M.</given-names></name><name><surname>van Erp</surname><given-names>J. B. F.</given-names></name></person-group> (<year>2010</year>). <article-title>A tactile p300 brain-computer interface</article-title>. <source>Front. Neuroprosthetics</source><fpage>4</fpage>:<lpage>19</lpage> <pub-id pub-id-type="doi">10.3389/fnins.2010.00019</pub-id></mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Br&#x000fc;derle</surname><given-names>D.</given-names></name><name><surname>M&#x000fc;ller</surname><given-names>E.</given-names></name><name><surname>Davison</surname><given-names>A.</given-names></name><name><surname>Muller</surname><given-names>E.</given-names></name><name><surname>Schemmel</surname><given-names>J.</given-names></name><name><surname>Meier</surname><given-names>K.</given-names></name></person-group> (<year>2009</year>). <article-title>Establishing a novel modeling tool: a python-based interface for a neuromorphic hardware system</article-title>. <source>Front. Neuroinformatics</source><fpage>3</fpage>:<lpage>17</lpage> <pub-id pub-id-type="doi">10.3389/neuro.11.017.2009</pub-id><pub-id pub-id-type="pmid">19562085</pub-id></mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>M.</given-names></name><name><surname>Gao</surname><given-names>X.</given-names></name><name><surname>Gao</surname><given-names>S.</given-names></name><name><surname>Xu</surname><given-names>D.</given-names></name></person-group> (<year>2002</year>). <article-title>Design and implementation of a brain-computer interface with high transfer rates</article-title>. <source>IEEE Trans. Biomed. Eng.</source><volume>49</volume>, <fpage>1181</fpage>&#x02013;<lpage>1186</lpage><pub-id pub-id-type="doi">10.1109/TBME.2002.803536</pub-id><pub-id pub-id-type="pmid">12374343</pub-id></mixed-citation></ref><ref id="B10"><mixed-citation publication-type="book"><person-group person-group-type="editor"><name><surname>Dornhege</surname><given-names>G.</given-names></name><name><surname>del</surname><given-names>J.</given-names></name><name><surname>Mill&#x000e1;n</surname><given-names>R.</given-names></name><name><surname>Hinterberger</surname><given-names>T.</given-names></name><name><surname>McFarland</surname><given-names>D.</given-names></name><name><surname>M&#x000fc;ller</surname><given-names>K.-R.</given-names></name></person-group> (eds.). (<year>2007</year>). <source>Toward Brain-Computer Interfacing.</source><publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name></mixed-citation></ref><ref id="B11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drewes</surname><given-names>R.</given-names></name><name><surname>Zou</surname><given-names>Q.</given-names></name><name><surname>Goodman</surname><given-names>P.</given-names></name></person-group> (<year>2009</year>). <article-title>Brainlab: a Python toolkit to aid in the design, simulation, and analysis of spiking neural networks with the NeoCortical simulator</article-title>. <source>Front. Neuroinformatics</source><fpage>3</fpage>:<lpage>16</lpage> <pub-id pub-id-type="doi">10.3389/neuro.11.016.2009</pub-id><pub-id pub-id-type="pmid">19506707</pub-id></mixed-citation></ref><ref id="B12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geller</surname><given-names>A.</given-names></name><name><surname>Schleifer</surname><given-names>I.</given-names></name><name><surname>Sederberg</surname><given-names>P.</given-names></name><name><surname>Jacobs</surname><given-names>J.</given-names></name><name><surname>Kahana</surname><given-names>M.</given-names></name></person-group> (<year>2007</year>). <article-title>PyEPL: a cross-platform experiment-programming library</article-title>. <source>Behav. Res. Methods</source><volume>39</volume>, <fpage>950</fpage>&#x02013;<lpage>958</lpage><pub-id pub-id-type="pmid">18183912</pub-id></mixed-citation></ref><ref id="B13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gevins</surname><given-names>A.</given-names></name><name><surname>Smith</surname><given-names>M.</given-names></name></person-group> (<year>2000</year>). <article-title>Neurophysiological measures of working memory and individual differences in cognitive ability and cognitive style</article-title>. <source>Cereb. Cortex</source><volume>10</volume>, <fpage>829</fpage><pub-id pub-id-type="doi">10.1093/cercor/10.9.829</pub-id><pub-id pub-id-type="pmid">10982744</pub-id></mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herrmann</surname><given-names>C. S.</given-names></name></person-group> (<year>2001</year>). <article-title>Human EEG responses to 1-100 hz flicker: resonance phenomena in visual cortex and their potential correlation to cognitive phenomena</article-title>. <source>Exp. Brain Res</source>. <volume>137</volume>, <fpage>346</fpage>&#x02013;<lpage>353</lpage><pub-id pub-id-type="doi">10.1007/s002210100682</pub-id><pub-id pub-id-type="pmid">11355381</pub-id></mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>H&#x000f6;hne</surname><given-names>J.</given-names></name><name><surname>Schreuder</surname><given-names>M.</given-names></name><name><surname>Blankertz</surname><given-names>B.</given-names></name><name><surname>Tangermann</surname><given-names>M.</given-names></name></person-group> (<year>2010</year>). <article-title>Two-dimensional auditory P300 speller with predictive text system, Proc</article-title>. <source>32nd Ann. Int. IEEE EMBS Conf</source>. <fpage>4185</fpage>&#x02013;<lpage>4188</lpage></mixed-citation></ref><ref id="B16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ince</surname><given-names>R.</given-names></name><name><surname>Petersen</surname><given-names>R.</given-names></name><name><surname>Swan</surname><given-names>D.</given-names></name><name><surname>Panzeri</surname><given-names>S.</given-names></name></person-group> (<year>2009</year>). <article-title>Python for information theoretic analysis of neural data</article-title>. <source>Front. Neuroinformatics</source><fpage>3</fpage>:<lpage>4</lpage> <pub-id pub-id-type="doi">10.3389/neuro.11.004.2009</pub-id><pub-id pub-id-type="pmid">19242557</pub-id></mixed-citation></ref><ref id="B17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jurica</surname><given-names>P.</given-names></name><name><surname>Van Leeuwen</surname><given-names>C.</given-names></name></person-group> (<year>2009</year>). <article-title>OMPC: an open-source MATLAB&#x000ae;-to-Python compiler</article-title>. <source>Front. Neuroinformatics</source><fpage>3</fpage>:<lpage>5</lpage> <pub-id pub-id-type="doi">10.3389/neuro.11.005.2009</pub-id><pub-id pub-id-type="pmid">19225577</pub-id></mixed-citation></ref><ref id="B18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krusienski</surname><given-names>D. J.</given-names></name><name><surname>Allison</surname><given-names>B. Z.</given-names></name></person-group> (<year>2008</year>). <article-title>Harmonic coupling of steady-state visual evoked potentials</article-title>. <source>Conf. Proc. IEEE Eng. Med. Biol. Soc.</source><volume>2008</volume>, <fpage>5037</fpage>&#x02013;<lpage>5040</lpage><pub-id pub-id-type="pmid">19163848</pub-id></mixed-citation></ref><ref id="B19"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lutz</surname><given-names>M.</given-names></name></person-group> (<year>2006</year>). <source>Programming Python</source>. <publisher-loc>Sebastopol, CA, USA</publisher-loc>: <publisher-name>O'Reilly Media, Inc</publisher-name></mixed-citation></ref><ref id="B20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mackworth</surname><given-names>N.</given-names></name></person-group> (<year>1948</year>). <article-title>The breakdown of vigilance during prolonged visual search</article-title>. <source>Q. J. Exp. Psychol.</source><volume>1</volume>, <fpage>6</fpage>&#x02013;<lpage>21</lpage><pub-id pub-id-type="doi">10.1080/17470214808416738</pub-id></mixed-citation></ref><ref id="B21"><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Maeder</surname><given-names>C.</given-names></name><name><surname>Sannelli</surname><given-names>C.</given-names></name><name><surname>Haufe</surname><given-names>S.</given-names></name><name><surname>Lemm</surname><given-names>S.</given-names></name><name><surname>Blankertz</surname><given-names>B.</given-names></name></person-group> (<year>2010</year>). Effect of prestimulus SMR amplitude on BCI performance. Poster at the TOBI Workshop &#x02018;Integrating Brain-Computer Interfaces with Conventional Assistive Technology&#x02019; in Graz.</mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>M&#x000fc;ller-Putz</surname><given-names>G.</given-names></name><name><surname>Pfurtscheller</surname><given-names>G.</given-names></name></person-group> (<year>2008</year>). <article-title>Control of an electrical prosthesis with an SSVEP-based BCI</article-title>. <source>IEEE Trans. Biomed. Eng.</source><volume>55</volume>, <fpage>361</fpage>&#x02013;<lpage>364</lpage><pub-id pub-id-type="doi">10.1109/TBME.2007.897815</pub-id><pub-id pub-id-type="pmid">18232384</pub-id></mixed-citation></ref><ref id="B23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>M&#x000fc;ller</surname><given-names>K.-R.</given-names></name><name><surname>Blankertz</surname><given-names>B.</given-names></name></person-group> (<year>2006</year>). <article-title>Toward noninvasive brain-computer interfaces</article-title>. <source>IEEE Signal Process Mag.</source><volume>23</volume>, <fpage>125</fpage>&#x02013;<lpage>128</lpage></mixed-citation></ref><ref id="B24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pecevski</surname><given-names>D.</given-names></name><name><surname>Natschl&#x000e4;ger</surname><given-names>T.</given-names></name><name><surname>Schuch</surname><given-names>K.</given-names></name></person-group> (<year>2009</year>). <article-title>PCSIM: a parallel simulation environment for neural circuits fully integrated with python</article-title>. <source>Front. Neuroinformatics</source><fpage>3</fpage>:<lpage>11</lpage> <pub-id pub-id-type="doi">10.3389/neuro.11.011.2009</pub-id><pub-id pub-id-type="pmid">19543450</pub-id></mixed-citation></ref><ref id="B25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peirce</surname><given-names>J. W.</given-names></name></person-group> (<year>2007</year>). <article-title>Psychopy&#x02013;psychophysics software in python</article-title>. <source>J. Neurosci. Methods</source><volume>162</volume>, <fpage>8</fpage>&#x02013;<lpage>13</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2006.11.017</pub-id><pub-id pub-id-type="pmid">17254636</pub-id></mixed-citation></ref><ref id="B26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfurtscheller</surname><given-names>G.</given-names></name><name><surname>Leeb</surname><given-names>R.</given-names></name><name><surname>Keinrath</surname><given-names>C.</given-names></name><name><surname>Friedman</surname><given-names>D.</given-names></name><name><surname>Neuper</surname><given-names>C.</given-names></name><name><surname>Guger</surname><given-names>C.</given-names></name><name><surname>Slater</surname><given-names>M.</given-names></name></person-group> (<year>2006</year>). <article-title>Walking from thought</article-title>. <source>Brain Res.</source><volume>1071</volume>, <fpage>145</fpage>&#x02013;<lpage>152</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2005.11.083</pub-id><pub-id pub-id-type="pmid">16405926</pub-id></mixed-citation></ref><ref id="B27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prechelt</surname><given-names>L.</given-names></name></person-group> (<year>2000</year>). <article-title>An empirical comparison of C, C + +, Java, Perl, Python, Rexx and Tcl</article-title>. <source>IEEE Comput.</source><volume>33</volume>, <fpage>23</fpage>&#x02013;<lpage>29</lpage></mixed-citation></ref><ref id="B28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramsey</surname><given-names>L.</given-names></name><name><surname>Tangermann</surname><given-names>M.</given-names></name><name><surname>Haufe</surname><given-names>S.</given-names></name><name><surname>Blankertz</surname><given-names>B.</given-names></name></person-group> (<year>2009</year>). <article-title>Practicing fast-decision BCI using a &#x0201c;goalkeeper&#x0201d; paradigm</article-title>. <source>BMC Neurosci.</source><volume>10</volume> (<issue>Suppl. 1</issue>), <fpage>P69</fpage><pub-id pub-id-type="doi">10.1186/1471-2202-10-S1-P69</pub-id></mixed-citation></ref><ref id="B29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schalk</surname><given-names>G.</given-names></name><name><surname>McFarland</surname><given-names>D.</given-names></name><name><surname>Hinterberger</surname><given-names>T.</given-names></name><name><surname>Birbaumer</surname><given-names>N.</given-names></name><name><surname>Wolpaw</surname><given-names>J.</given-names></name></person-group> (<year>2004</year>). <article-title>Bci2000: a general-purpose brain-computer interface (BCI) system</article-title>. <source>IEEE Trans. Biomed. Eng.</source><volume>51</volume>, <fpage>1034</fpage>&#x02013;<lpage>1043</lpage><pub-id pub-id-type="doi">10.1109/TBME.2004.827072</pub-id><pub-id pub-id-type="pmid">15188875</pub-id></mixed-citation></ref><ref id="B30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidt</surname><given-names>N. M.</given-names></name><name><surname>Blankertz</surname><given-names>B.</given-names></name><name><surname>Treder</surname><given-names>M. S.</given-names></name></person-group> (<year>2010</year>). <article-title>Alpha-modulation induced by covert attention shifts as a new input modality for EEG-based BCIs</article-title>. <source>Proc. 2010 IEEE Conf. Syst. Man Cybernet.</source>, (in press).</mixed-citation></ref><ref id="B31"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schreiner</surname><given-names>T.</given-names></name></person-group> (<year>2008</year>). <source>Development and Application of a Python Scripting Framework for bci2000</source>. Master's thesis. <publisher-loc>Universit&#x000e4;t T&#x000fc;bingen</publisher-loc>, <publisher-name>T&#x000fc;bingen</publisher-name></mixed-citation></ref><ref id="B32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schreuder</surname><given-names>M.</given-names></name><name><surname>Blankertz</surname><given-names>B.</given-names></name><name><surname>Tangermann</surname><given-names>M.</given-names></name></person-group> (<year>2010</year>). <article-title>A new auditory multi-class brain-computer interface paradigm: spatial hearing as an informative cue</article-title>. <source>PLoS One</source><volume>5</volume>, <fpage>e9813</fpage> <pub-id pub-id-type="doi">10.1371/journal.pone.0009813</pub-id><pub-id pub-id-type="pmid">20368976</pub-id></mixed-citation></ref><ref id="B33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spacek</surname><given-names>M.</given-names></name><name><surname>Blanche</surname><given-names>T.</given-names></name><name><surname>Swindale</surname><given-names>N.</given-names></name></person-group> (<year>2008</year>). <article-title>Python for large-scale electrophysiology</article-title>. <source>Front. Neuroinformatics</source><fpage>2</fpage>:<lpage>9</lpage> <pub-id pub-id-type="doi">10.3389/neuro.11.009.2008</pub-id><pub-id pub-id-type="pmid">19198646</pub-id></mixed-citation></ref><ref id="B34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strangman</surname><given-names>G.</given-names></name><name><surname>Zhang</surname><given-names>Q.</given-names></name><name><surname>Zeffiro</surname><given-names>T.</given-names></name></person-group> (<year>2009</year>). <article-title>Near-infrared neuroimaging with NinPy</article-title>. <source>Front. Neuroinformatics</source><fpage>3</fpage>:<fpage>12</fpage> <pub-id pub-id-type="doi">10.3389/neuro.11.012.2009</pub-id><pub-id pub-id-type="pmid">19543449</pub-id></mixed-citation></ref><ref id="B35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Straw</surname><given-names>A. D.</given-names></name></person-group> (<year>2008</year>). <article-title>Vision Egg: an open-source library for realtime visual stimulus generation</article-title>. <source>Front. Neuroinformatics</source><fpage>2</fpage>:<lpage>4</lpage> <pub-id pub-id-type="doi">10.3389/neuro.11/004.2008</pub-id><pub-id pub-id-type="pmid">19050754</pub-id></mixed-citation></ref><ref id="B36"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tanenbaum</surname><given-names>A. S.</given-names></name></person-group> (<year>2001</year>). <source>Modern Operating Systems</source>. <publisher-loc>Upper Saddle River, NJ, USA</publisher-loc>: <publisher-name>Prentice Hall PTR</publisher-name></mixed-citation></ref><ref id="B37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treder</surname><given-names>M. S.</given-names></name><name><surname>Blankertz</surname><given-names>B.</given-names></name></person-group> (<year>2010</year>). <article-title>(C)overt attention and visual speller design in an ERP-based brain-computer interface</article-title>. <source>Behav. Brain Funct</source>. <volume>6</volume>, <fpage>28</fpage><pub-id pub-id-type="doi">10.1186/1744-9081-6-28</pub-id><pub-id pub-id-type="pmid">20509913</pub-id></mixed-citation></ref><ref id="B38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Venthur</surname><given-names>B.</given-names></name><name><surname>Blankertz</surname><given-names>B.</given-names></name><name><surname>Gugler</surname><given-names>M. F.</given-names></name><name><surname>Curio</surname><given-names>G.</given-names></name></person-group> (<year>2010</year>). <article-title>Novel applications of BCI technology: psychophysiological optimization of working conditions in industry, in: Proc</article-title>. <source>2010 IEEE Conf. Syst. Man Cybernet.</source> in press.</mixed-citation></ref><ref id="B39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williamson</surname><given-names>J.</given-names></name><name><surname>Murray-Smith</surname><given-names>R.</given-names></name><name><surname>Blankertz</surname><given-names>B.</given-names></name><name><surname>Krauledat</surname><given-names>M.</given-names></name><name><surname>M&#x000fc;ller</surname><given-names>K.-R.</given-names></name></person-group> (<year>2009</year>). <article-title>Designing for uncertain, asymmetric control: interaction design for brain-computer interfaces</article-title>. <source>Int. J. Hum. Comput. Stud.</source><volume>67</volume>, <fpage>827</fpage>&#x02013;<lpage>841</lpage><pub-id pub-id-type="doi">10.1016/j.ijhcs.2009.05.009</pub-id></mixed-citation></ref></ref-list></back></article>
