<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Comput Neurosci</journal-id><journal-id journal-id-type="publisher-id">Front. Comput. Neurosci.</journal-id><journal-title-group><journal-title>Frontiers in Computational Neuroscience</journal-title></journal-title-group><issn pub-type="epub">1662-5188</issn><publisher><publisher-name>Frontiers Research Foundation</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">21160559</article-id><article-id pub-id-type="pmc">PMC3001990</article-id><article-id pub-id-type="doi">10.3389/fncom.2010.00143</article-id><article-categories><subj-group subj-group-type="heading"><subject>Neuroscience</subject><subj-group><subject>Original Research</subject></subj-group></subj-group></article-categories><title-group><article-title>STDP in Adaptive Neurons Gives Close-To-Optimal Information Transmission</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Hennequin</surname><given-names>Guillaume</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="author-notes" rid="fn001">*</xref></contrib><contrib contrib-type="author"><name><surname>Gerstner</surname><given-names>Wulfram</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author"><name><surname>Pfister</surname><given-names>Jean-Pascal</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>School of Computer and Communication Sciences, Brain-Mind Institute, Ecole Polytechnique F&#x000e9;d&#x000e9;rale de Lausanne</institution><country>Lausanne, Switzerland</country></aff><aff id="aff2"><sup>2</sup><institution>Department of Engineering, University of Cambridge</institution><country>Cambridge, UK</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: Per Jesper Sj&#x000f6;str&#x000f6;m, University College London, UK</p></fn><fn fn-type="edited-by"><p>Reviewed by: Walter Senn, University of Bern, Switzerland; Rasmus S. Petersen, University of Manchester, UK; Abigail Morrison, Albert-Ludwig University Freiburg, Germany</p></fn><corresp id="fn001">*Correspondence: Guillaume Hennequin, Laboratory of Computational Neuroscience, Ecole Polytechnique F&#x000e9;d&#x000e9;rale de Lausanne, Station 15, CH-1015 Lausanne, Switzerland. e-mail: <email>guillaume.hennequin@epfl.ch</email></corresp></author-notes><pub-date pub-type="epub"><day>03</day><month>12</month><year>2010</year></pub-date><pub-date pub-type="collection"><year>2010</year></pub-date><volume>4</volume><elocation-id>143</elocation-id><history><date date-type="received"><day>19</day><month>2</month><year>2010</year></date><date date-type="accepted"><day>28</day><month>9</month><year>2010</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2010 Hennequin, Gerstner and Pfister.</copyright-statement><copyright-year>2010</copyright-year><license license-type="open-access" xlink:href="http://www.frontiersin.org/licenseagreement"><license-p>This is an open-access article subject to an exclusive license agreement between the authors and the Frontiers Research Foundation, which permits unrestricted use, distribution, and reproduction in any medium, provided the original authors and source are credited.</license-p></license></permissions><abstract><p>Spike-frequency adaptation is known to enhance the transmission of information in sensory spiking neurons by rescaling the dynamic range for input processing, matching it to the temporal statistics of the sensory stimulus. Achieving maximal information transmission has also been recently postulated as a role for spike-timing-dependent plasticity (STDP). However, the link between optimal plasticity and STDP in cortex remains loose, as does the relationship between STDP and adaptation processes. We investigate how STDP, as described by recent minimal models derived from experimental data, influences the quality of information transmission in an adapting neuron. We show that a phenomenological model based on triplets of spikes yields almost the same information rate as an optimal model specially designed to this end. In contrast, the standard pair-based model of STDP does not improve information transmission as much. This result holds not only for additive STDP with hard weight bounds, known to produce bimodal distributions of synaptic weights, but also for weight-dependent STDP in the context of unimodal but skewed weight distributions. We analyze the similarities between the triplet model and the optimal learning rule, and find that the triplet effect is an important feature of the optimal model when the neuron is adaptive. If STDP is optimized for information transmission, it must take into account the dynamical properties of the postsynaptic cell, which might explain the target-cell specificity of STDP. In particular, it accounts for the differences found <italic>in vitro</italic> between STDP at excitatory synapses onto principal cells and those onto fast-spiking interneurons.</p></abstract><kwd-group><kwd>STDP</kwd><kwd>plasticity</kwd><kwd>spike-frequency adaptation</kwd><kwd>information theory</kwd><kwd>optimality</kwd></kwd-group><counts><fig-count count="8"/><table-count count="1"/><equation-count count="31"/><ref-count count="52"/><page-count count="16"/><word-count count="13281"/></counts></article-meta></front><body><sec sec-type="introduction"><title>Introduction</title><p>The experimental discovery of spike-timing-dependent plasticity (STDP) in the mid-nineties (Bell et al., <xref ref-type="bibr" rid="B3">1997</xref>; Magee and Johnston, <xref ref-type="bibr" rid="B27">1997</xref>; Markram et al., <xref ref-type="bibr" rid="B29">1997</xref>; Bi and Poo, <xref ref-type="bibr" rid="B4">1998</xref>; Zhang et al., <xref ref-type="bibr" rid="B52">1998</xref>) led to two questions, in particular. The first is: what is the simplest way of describing this complex phenomenon? This question has been answered in a couple of minimal models (phenomenological approach) whereby long-term potentiation (LTP) and long-term depression (LTD) are reduced to the behavior of a small number of variables (Gerstner et al., <xref ref-type="bibr" rid="B15">1996</xref>; Kempter et al., <xref ref-type="bibr" rid="B20">1999</xref>; Song et al., <xref ref-type="bibr" rid="B43">2000</xref>; van Rossum et al., <xref ref-type="bibr" rid="B49">2000</xref>; Rubin et al., <xref ref-type="bibr" rid="B38">2001</xref>; Gerstner and Kistler, <xref ref-type="bibr" rid="B16">2002a</xref>; Froemke et al., <xref ref-type="bibr" rid="B14">2006</xref>; Pfister and Gerstner, <xref ref-type="bibr" rid="B35">2006</xref>; Clopath et al., <xref ref-type="bibr" rid="B10">2010</xref>; see Morrison et al., <xref ref-type="bibr" rid="B31">2008</xref> for a review). Because they are inspired by <italic>in vitro</italic> plasticity experiments, the state variables usually depend solely on what is experimentally controlled, i.e., on spike times and possibly on the postsynaptic membrane potential. They are computationally cheap enough to be used in large-scale simulations (Morrison et al., <xref ref-type="bibr" rid="B30">2007</xref>; Izhikevich and Edelman, <xref ref-type="bibr" rid="B19">2008</xref>). The second question has to do with the functional role of STDP: what is STDP good for? The minimal models mentioned above can address this question only indirectly, by solving the dynamical equation of synaptic plasticity for input with given stationary properties (Kempter et al., <xref ref-type="bibr" rid="B20">1999</xref>; van Rossum et al., <xref ref-type="bibr" rid="B49">2000</xref>; Rubin et al., <xref ref-type="bibr" rid="B38">2001</xref>). An alternative approach is to postulate a role for synaptic plasticity, and formulate it in the mathematical framework of optimization (&#x0201c;top-down approach&#x0201d;). Thus, in artificial neural networks, Hebbian-like learning rules were shown to arise from unsupervised learning paradigms such as principal components analysis (Oja, <xref ref-type="bibr" rid="B32">1982</xref>, <xref ref-type="bibr" rid="B33">1989</xref>), independent components analysis (ICA; Intrator and Cooper, <xref ref-type="bibr" rid="B18">1992</xref>; Bell and Sejnowski, <xref ref-type="bibr" rid="B2">1995</xref>; Clopath et al., <xref ref-type="bibr" rid="B11">2008</xref>), maximization of mutual information (MI; Linsker, <xref ref-type="bibr" rid="B24">1989</xref>), sparse coding (Olshausen and Field, <xref ref-type="bibr" rid="B34">1996</xref>; Smith and Lewicki, <xref ref-type="bibr" rid="B42">2006</xref>), and predictive coding (Rao and Ballard, <xref ref-type="bibr" rid="B37">1999</xref>). In spiking neurons, local STDP-like learning rules were obtained from optimization criteria such as maximization of information transmission (Chechik, <xref ref-type="bibr" rid="B9">2003</xref>; Toyoizumi et al., <xref ref-type="bibr" rid="B46">2005</xref>, <xref ref-type="bibr" rid="B47">2007</xref>), information bottleneck (Klampfl et al., <xref ref-type="bibr" rid="B21">2009</xref>), maximization of the neuron's sensitivity to the input (Bell and Parra, <xref ref-type="bibr" rid="B1">2005</xref>), reduction of the conditional entropy (Bohte and Mozer, <xref ref-type="bibr" rid="B8">2007</xref>), slow-feature analysis (Sprekeler et al., <xref ref-type="bibr" rid="B45">2007</xref>), and maximization of the expected reward (Xie and Seung, <xref ref-type="bibr" rid="B51">2004</xref>; Pfister et al., <xref ref-type="bibr" rid="B36">2006</xref>; Florian, <xref ref-type="bibr" rid="B13">2007</xref>; Sprekeler et al., <xref ref-type="bibr" rid="B44">2009</xref>).</p><p>The functional consequences of STDP have mainly been investigated in simple integrate-and-fire neurons, where the range of temporal dependencies in the postsynaptic spike train spans no more than the membrane time constant. Few studies have addressed the question of the synergy between STDP and more complex dynamical properties on different timescales. In Seung (<xref ref-type="bibr" rid="B40">2003</xref>), more complex dynamics were introduced not at the cell level, but through short-term plasticity of the synapses. The postsynaptic neuron was then able to become selective to temporal order in the input. Another elegant approach to this question was taken in Lengyel et al. (<xref ref-type="bibr" rid="B23">2005</xref>) in a model of hippocampal autoassociative memory. Memories were encoded in the phase of firing of a population of neurons relative to an ongoing theta oscillation. Under the assumption that memories are stored using a classical form of STDP, they derived the form of the postsynaptic dynamics that would optimally achieve their recall. This turned out to match what they recorded <italic>in vitro</italic>, suggesting that STDP might optimally interact with the dynamical properties of the postsynaptic cell in this memory storage task.</p><p>More generally, optimality models are ideally suited to study plasticity and dynamics together. Indeed, optimal learning rules contain an explicit reference to the dynamical properties of the postsynaptic cell, by means of the transfer function that maps input to output values. This function usually appears in the formulation of a gradient ascent on the objective function. In this article, we exploit this in order to relate STDP to spike-frequency adaptation (SFA), an important feature of the dynamics of a number of cell types found in cortex. Recent phenomenological models of STDP have emphasized the importance of the interaction between postsynaptic spikes in the LTP process (Senn et al., <xref ref-type="bibr" rid="B39">2001</xref>; Pfister and Gerstner, <xref ref-type="bibr" rid="B35">2006</xref>; Clopath et al., <xref ref-type="bibr" rid="B10">2010</xref>). In these models, the amount of LTP obtained from a pre-before-post spike pair increases with the number of postsynaptic spikes fired in the recent past, which we call the &#x0201c;triplet effect&#x0201d; (combination of one pre-spike and at least two post-spikes). The timescale of this post&#x02013;post interaction was fitted to <italic>in vitro</italic> STDP experiments, and found to be very close to that of adaptation (100&#x02013;150&#x02009;ms).</p><p>We reason that STDP may be ideally tuned to SFA of the postsynaptic cell. We specifically study this idea within the framework of optimal information transmission (infomax) between input and output spike trains. We compare the performance of a learning rule derived from the infomax principle in Toyoizumi et al. (<xref ref-type="bibr" rid="B46">2005</xref>), to that of the triplet model developed in Pfister and Gerstner (<xref ref-type="bibr" rid="B35">2006</xref>). We also compare them to the standard pair-based learning window used in most STDP papers. Performance is measured in terms of information theoretic quantities. We find that the triplet learning rule yields a better performance than pair-STDP on a spatio-temporal receptive field formation task, and that this advantage crucially depends on the presence of postsynaptic SFA. This reflects a synergy between the triplet effect and adaptation. The reasons for this optimality are further studied by showing that the optimal model features a similar triplet effect when the postsynaptic neuron adapts. We also show that both the optimal and triplet learning rules increase the variability of the postsynaptic spike trains, and enlarge the frequency band in which signals are transmitted, extending it toward lower frequencies (1&#x02013;5&#x02009;Hz). Finally, we exploit the optimal model to predict the form of the STDP mechanism for two different target cell types. The results qualitatively agree with the <italic>in vitro</italic> data reported for excitatory synapses onto principal cells and those onto fast-spiking (FS) inhibitory interneurons. In the model, the learning windows are different because the intrinsic dynamical properties of the two postsynaptic cell types are different. This might be the functional reason for the target-cell specificity of STDP.</p></sec><sec sec-type="materials|methods" id="s2"><title>Materials and Methods</title><sec><title>Neuron model</title><p>We simulate a single stochastic point neuron (Gerstner and Kistler, <xref ref-type="bibr" rid="B17">2002b</xref>) and a small portion of its incoming synapses (<italic>N</italic>&#x02009;=&#x02009;1 for the simulation of <italic>in vitro</italic> experiments, <italic>N</italic>&#x02009;=&#x02009;100 in the rest of the paper). Each postsynaptic potential (PSP) adds up linearly to form the total modeled synaptic drive</p><disp-formula id="E1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mo>&#x003b5;</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><p>with</p><disp-formula id="E2"><label>(2)</label><mml:math id="M2"><mml:mrow><mml:msub><mml:mo>&#x003b5;</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>&#x0222b;</mml:mo><mml:mn>0</mml:mn><mml:mi>t</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02032;</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02032;</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mo>&#x003c4;</mml:mo><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><p>where <inline-formula><mml:math id="M3"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mo>&#x003a3;</mml:mo><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mi>j</mml:mi><mml:mi>f</mml:mi></mml:msubsup></mml:mrow></mml:msub><mml:mo>&#x003b4;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mi>j</mml:mi><mml:mi>f</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> denotes the <italic>j</italic><sup>th</sup> input spike train, and <italic>w<sub>j</sub></italic> (mV) are the synaptic weights. The effect of thousands of other synapses is not modeled explicitly, but treated as background noise. The firing activity of the neuron is entirely described by an instantaneous firing density</p><disp-formula id="E3"><label>(3)</label><mml:math id="M4"><mml:mrow><mml:mo>&#x003c1;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><p>where</p><disp-formula id="E4"><label>(4)</label><mml:math id="M5"><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mtext>log</mml:mtext><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>&#x003b2;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><p>is the gain function, drawn in Figure <xref ref-type="fig" rid="F1">1</xref>A. Refractoriness and SFA both modulate the instantaneous firing rate via</p><disp-formula id="E5"><label>(5)</label><mml:math id="M6"><mml:mrow><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><p>The variables <italic>g<sub>R</sub></italic> and <italic>g<sub>A</sub></italic> evolve according to</p><disp-formula id="E6"><label>(6)</label><mml:math id="M7"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mo>&#x003c4;</mml:mo><mml:mi>R</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02003;</mml:mo><mml:mtext>and</mml:mtext><mml:mo>&#x02003;</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mo>&#x003c4;</mml:mo><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><p>where <inline-formula><mml:math id="M8"><mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mo>&#x003a3;</mml:mo><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow><mml:mi>f</mml:mi></mml:msubsup></mml:mrow></mml:msub><mml:mo>&#x003b4;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow><mml:mi>f</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the postsynaptic spike train and 0&#x02009;&#x0003c;&#x02009;&#x003c4;<italic><sub>R</sub></italic> &#x0226a;&#x02009;&#x003c4;<italic><sub>A</sub></italic> are the time constants of refractoriness and adaptation respectively. The firing rate thus becomes a compressive function of the average gain, as shown in Figure <xref ref-type="fig" rid="F1">1</xref>B. The response of the neuron to a step in input firing rate is depicted in Figure <xref ref-type="fig" rid="F1">1</xref>C.</p><p>For the simulation of <italic>in vitro</italic> STDP experiments, only one synapse is investigated. The potential <italic>u</italic> is thus given a baseline <italic>u<sub>b</sub></italic> (to which the PSP of the single synapse will add) such that <italic>g</italic>(<italic>u<sub>b</sub></italic>) yields a spontaneous firing rate of 7.5 Hz (Figure <xref ref-type="fig" rid="F1">1</xref>B).</p><fig id="F1" position="float"><label>Figure 1</label><caption><p><bold>Stochastic neuron model</bold>. <bold>(A)</bold> The gain function <italic>g</italic>(<italic>u</italic>) (Eq. <xref ref-type="disp-formula" rid="E4">4</xref>, solid line here) shows the momentary rate of a non-refractory neuron as a function of the membrane potential <italic>u</italic>. <bold>(B)</bold> The mean rate &#x03008;<italic>g</italic>[<italic>u</italic>(<italic>t</italic>)]<italic>M</italic>(<italic>t</italic>)&#x03009; of a neuron with refractoriness and adaptation is lower (solid red line). The baseline potential <italic>u<sub>b</sub></italic> used in the simulation is defined as the membrane potential that yields a spontaneous firing rate of 7.5 Hz (green arrow and dashed line). In some simulations, we need to switch off adaptation, but we want the same holding potential <italic>u<sub>b</sub></italic> to evoke the same 7.5 Hz output firing rate. The slope <italic>r</italic><sub>0</sub> of the gain function is therefore rescaled (<bold>A</bold>, dashed curve) so that the frequency curves in the adaptation and no-adaptation cases (<bold>B</bold>, solid and dashed red curves) cross at <italic>u</italic>&#x02009;=&#x02009;<italic>u<sub>b</sub></italic>. <bold>(C)</bold> Example response property of an adaptive neuron. A single neuron receives synaptic inputs from 100 Poisson spike trains with a time-varying rate. The experiment is repeated 1000 times independently. Bottom: the input rate jumps from 10 to 50 Hz, stays there for half a second and returns back to 10 Hz (bottom). Middle: Peri-stimulus time histogram (PSTH, 4&#x02009;ms bin). Top: example spike trains (first 100 trials).</p></caption><graphic xlink:href="fncom-04-00143-g001"/></fig><p>In some of our simulations, postsynaptic SFA is switched off (<italic>q<sub>A</sub></italic>&#x02009;=&#x02009;0). In order to preserve the same average firing rate given the same synaptic weights, <italic>r</italic><sub>0</sub> is rescaled accordingly (Figures <xref ref-type="fig" rid="F1">1</xref>A,B, dashed lines).</p><p>In the simulation of Figure <xref ref-type="fig" rid="F8">8</xref>, we add a third variable <italic>g<sub>B</sub></italic> in the after-spike kernel <italic>M</italic> in order to model a FS inhibitory interneuron. This variable jumps down (<italic>q<sub>B</sub></italic>&#x02009;&#x0003c;&#x02009;0) following every postsynaptic spike, and decays exponentially with time constant &#x003c4;<italic><sub>B</sub></italic> (with &#x003c4;<italic><sub>R</sub></italic>&#x02009;&#x0226a; &#x003c4;<italic><sub>B</sub></italic>&#x02009;&#x0003c;&#x02009;&#x003c4;<italic><sub>A</sub></italic>).</p><p>All simulations were written in Objective Caml and run on a standard desktop computer operated by Linux. We used simple Euler integration of all differential equations, with 1&#x02009;ms time resolution (0.1&#x02009;ms for the simulation of <italic>in vitro</italic> experiments). All parameters are listed in Table <xref ref-type="table" rid="T1">1</xref> together with their values.</p><table-wrap id="T1" position="float"><label>Table 1</label><caption><p><bold>Baseline values of all parameters defined in the text</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" colspan="2" rowspan="1">Neuron model</th><th align="center" colspan="2" rowspan="1">Optimal rule</th><th align="center" colspan="2" rowspan="1">Triplet rule</th><th align="center" colspan="2" rowspan="1">Pair rule</th><th align="center" colspan="2" rowspan="1">Weight bounds</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">&#x003c4;<italic><sub>m</sub></italic></td><td align="left" rowspan="1" colspan="1">20&#x02009;ms</td><td align="left" rowspan="1" colspan="1">&#x003b7;<italic><sub>o</sub></italic></td><td align="left" rowspan="1" colspan="1">0.04</td><td align="left" rowspan="1" colspan="1">&#x003b7;<sub>3</sub></td><td align="left" rowspan="1" colspan="1">1.0</td><td align="left" rowspan="1" colspan="1">&#x003b7;<sub>2</sub></td><td align="left" rowspan="1" colspan="1">1.0</td><td align="left" rowspan="1" colspan="1"><italic>w</italic><sub>min</sub></td><td align="left" rowspan="1" colspan="1">0&#x02009;mV</td></tr><tr><td align="left" rowspan="1" colspan="1"><italic>g</italic><sub>0</sub></td><td align="left" rowspan="1" colspan="1">1&#x02009;Hz <bold>(35)</bold></td><td align="left" rowspan="1" colspan="1">&#x003c4;<italic><sub>C</sub></italic></td><td align="left" rowspan="1" colspan="1">20&#x02009;ms</td><td align="left" rowspan="1" colspan="1">&#x003c4;<sub>+</sub></td><td align="left" rowspan="1" colspan="1">16.8&#x02009;ms</td><td align="left" rowspan="1" colspan="1">&#x003c4;<sub>+</sub></td><td align="left" rowspan="1" colspan="1">16.8&#x02009;ms</td><td align="left" rowspan="1" colspan="1"><italic>w</italic><sub>max</sub></td><td align="left" rowspan="1" colspan="1">4&#x02009;mV</td></tr><tr><td align="left" rowspan="1" colspan="1"><italic>r</italic><sub>0</sub></td><td align="left" rowspan="1" colspan="1">9.25&#x02009;Hz <italic>(3.25)</italic></td><td align="left" rowspan="1" colspan="1">&#x003c4;<italic><sub>g</sub></italic></td><td align="left" rowspan="1" colspan="1">10&#x02009;s</td><td align="left" rowspan="1" colspan="1">&#x003c4;<sub>&#x02212;</sub></td><td align="left" rowspan="1" colspan="1">33.7&#x02009;ms</td><td align="left" rowspan="1" colspan="1">&#x003c4;<sub>&#x02212;</sub></td><td align="left" rowspan="1" colspan="1">33.7&#x02009;ms</td><td align="left" rowspan="1" colspan="1"><italic>a</italic></td><td align="left" rowspan="1" colspan="1">9</td></tr><tr><td align="left" rowspan="1" colspan="1">&#x003b2;</td><td align="left" rowspan="1" colspan="1">0.5&#x02009;mV<sup>&#x02212;1</sup></td><td align="left" rowspan="1" colspan="1">&#x003b3;</td><td align="left" rowspan="1" colspan="1">1 <bold>(0)</bold></td><td align="left" rowspan="1" colspan="1">&#x003c4;<italic><sub>y</sub></italic></td><td align="left" rowspan="1" colspan="1">114&#x02009;ms</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1"><italic>u<sub>T</sub></italic></td><td align="left" rowspan="1" colspan="1">15&#x02009;mV</td><td align="left" rowspan="1" colspan="1"><italic>g</italic><sub>targ</sub></td><td align="left" rowspan="1" colspan="1"><italic>ad hoc</italic></td><td align="left" rowspan="1" colspan="1"><inline-formula><mml:math id="M9"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mn>2</mml:mn><mml:mo>&#x02212;</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula></td><td align="left" rowspan="1" colspan="1">2.8&#x000d7;10<sup>&#x02212;3</sup></td><td align="left" rowspan="1" colspan="1"><inline-formula><mml:math id="M10"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mn>2</mml:mn><mml:mo>&#x02212;</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula></td><td align="left" rowspan="1" colspan="1">2.8&#x000d7;10<sup>&#x02212;3</sup></td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">&#x003c4;<italic><sub>R</sub></italic></td><td align="left" rowspan="1" colspan="1">2&#x02009;ms</td><td align="left" rowspan="1" colspan="1">&#x003bb;</td><td align="left" rowspan="1" colspan="1">0.0094</td><td align="left" rowspan="1" colspan="1"><inline-formula><mml:math id="M11"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mn>3</mml:mn><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula></td><td align="left" rowspan="1" colspan="1">6.5&#x000d7;10<sup>&#x02212;3</sup></td><td align="left" rowspan="1" colspan="1"><inline-formula><mml:math id="M12"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mn>2</mml:mn><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula></td><td align="left" rowspan="1" colspan="1">5.6&#x000d7;10<sup>&#x02212;3</sup></td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">&#x003c4;<italic><sub>A</sub></italic></td><td align="left" rowspan="1" colspan="1">150&#x02009;ms</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#x003c1;<sub>targ</sub></td><td align="left" rowspan="1" colspan="1"><italic>ad hoc</italic></td><td align="left" rowspan="1" colspan="1">&#x003c1;<sub>targ</sub></td><td align="left" rowspan="1" colspan="1"><italic>ad hoc</italic></td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1"><italic>q<sub>R</sub></italic></td><td align="left" rowspan="1" colspan="1">100</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#x003c4;<sub>&#x003c1;</sub></td><td align="left" rowspan="1" colspan="1">10&#x02009;s</td><td align="left" rowspan="1" colspan="1">&#x003c4;<sub>&#x003c1;</sub></td><td align="left" rowspan="1" colspan="1">10&#x02009;s</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1"><italic>q<sub>A</sub></italic></td><td align="left" rowspan="1" colspan="1">1 <italic>(0)</italic></td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/></tr></tbody></table><table-wrap-foot><p><italic>Some parameters were set to different values when the neuron was non-adapting (italic numbers). Similarly, some parameters were different for the simulations of in vitro experiment (bold faces)</italic>.</p></table-wrap-foot></table-wrap></sec><sec><title>Presynaptic firing statistics</title><p>To analyze the evolution of information transmission under different plasticity learning rules, we consider <italic>N</italic>&#x02009;=&#x02009;100 periodic input spike of 5&#x02009;s duration generated once and for all (see below). This &#x0201c;frozen noise&#x0201d; is then replayed continuously, feeding the postsynaptic neuron for as long as is necessary (e.g., for learning, or for MI estimation).</p><p>To generate the time-varying rates of the <italic>N</italic> processes underlying this frozen noise, we first draw point events at a constant Poisson rate of 10 Hz, and then smooth them with a Gaussian kernel of width 150&#x02009;ms. Rates are further multiplicatively normalized so that each presynaptic neuron fires an average of 10 spikes per second. We emphasize that this process describes the statistics of the inputs <italic>across different learning experiments</italic>. When we mention &#x0201c;independent trials,&#x0201d; we mean a set of experiments which have their own independent realizations of those input spike trains. However, in one learning experiment, a single such set of <italic>N</italic> input spike trains is chosen and replayed continuously as input to the postsynaptic neuron. The input is therefore deterministic and periodic. When the periodic input is generated, some neurons can happen to fire at some point during those 5&#x02009;s within a few milliseconds of each other, and by virtue of the periodicity, these synchronous firing events will repeat in each period, giving rise to strong spatio-temporal correlations in the inputs. We are interested in seeing how different learning rules can exploit this correlational structure to improve the information carried by the postsynaptic activity about those presynaptic spike trains. We now describe what we mean by information transmission under this specific stimulation scenario.</p></sec><sec><title>Information theoretic measurements</title><p>The neuron can be seen as a noisy communication channel in which multidimensional signals are compressed and distorted before being transmitted to subsequent receivers. The goodness of a communication channel is traditionally measured by Shannon's MI between the input and output variables, where the input is chosen randomly from some &#x0201c;alphabet&#x0201d; or vocabulary of symbols.</p><p>Here, the input is deterministic and periodic (Figure <xref ref-type="fig" rid="F2">2</xref>A). We therefore define the quality of information transmission by the reduction of uncertainty about the phase of the current input if we observe a certain output spike train at an unknown time. In discrete time (with time bin &#x00394;&#x02009;=&#x02009;1&#x02009;ms), there are only <italic>N</italic><sub>&#x003c6;</sub>&#x02009;=&#x02009;5000 possible phases since the input has a period of 5&#x02009;s. Therefore, the maximum number of bits that the noisy postsynaptic neuron can transmit is log<sub>2</sub>(<italic>N</italic><sub>&#x003c6;</sub>)&#x02009;&#x02243;12.3&#x02009;bits. We further assume that an observer of the output neuron can only see &#x0201c;words&#x0201d; corresponding to spike trains of finite duration <italic>T</italic>&#x02009;=&#x02009;<italic>K</italic>&#x00394;. We assume <italic>T</italic>&#x02009;=&#x02009;1&#x02009;s for most of the paper, which corresponds to <italic>K</italic>&#x02009;=&#x02009;1000 time bins. This choice is justified below.</p><fig id="F2" position="float"><label>Figure 2</label><caption><p><bold>Information transmission through a noisy postsynaptic neuron</bold>. <bold>(A)</bold> Schematic representation of the feed-forward network. Five-second input spike trains repeat continuously in time (periodic input) and drive a noisy and possibly adapting output neuron via plastic synapses. It is assumed that an observer of the output spike train has access to portions <italic>Y<sup>K</sup></italic> of it, called &#x0201c;words,&#x0201d; of duration <italic>T</italic>&#x02009;=&#x02009;<italic>K</italic>&#x00394;. The observer does not have access to a clock, and therefore has a flat prior expectation over possible phases before observing a word. The goodness of the system, given a set of synaptic weights <bold>w</bold>, is measured by the reduction of uncertainty about the phase, gained from the observation of an output word <italic>Y<sup>K</sup></italic> (mutual information, see text). <bold>(B)</bold> For a random set of synaptic weights (20 weights at 4&#x02009;mV, the rest at 0), the mutual information (MI) is reported as a function of the output word size <italic>K</italic>&#x00394;. Asymptotically, the MI converges to the theoretical limit given by log<sub>2</sub>(<italic>N</italic><sub>&#x003c6;</sub>) ( 12.3 bits. In the rest of this study, 1-s output words are considered (square). <bold>(C)</bold> Mutual information (MI, top) and information per spike (MI&#x02019;, bottom) as a function of the average firing rate. Black: with SFA. Green: without SFA. Each dot is obtained by setting a fraction of randomly chosen synaptic efficacies to the upper bound (4&#x02009;mV) and the rest to 0. The higher the fraction of non-zero weights, the higher the firing rate. The information per spike is a relevant quantity because spike generation costs energy.</p></caption><graphic xlink:href="fncom-04-00143-g002"/></fig><p>The discretized output spike trains of size <italic>K</italic> (binary vectors), called <bold>Y<sup>K</sup></bold>, can be observed at random times and play the role of the output variable. The input random variable is the phase &#x003c6; of the input. The quality of information transmission is quantified by the MI, i.e., the difference between the total response entropy <inline-formula><mml:math id="M13"><mml:mrow><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>&#x02329;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mtext>log</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>&#x0232a;</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mi>K</mml:mi></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and the noise entropy <inline-formula><mml:math id="M14"><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mo>|</mml:mo><mml:mo>&#x003c6;</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>&#x02329;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>&#x02329;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mtext>log</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mo>|</mml:mo><mml:mo>&#x003c6;</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>&#x0232a;</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mo>|</mml:mo><mml:mo>&#x003c6;</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x0232a;</mml:mo></mml:mrow></mml:mrow><mml:mo>&#x003c6;</mml:mo></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> Here &#x03008;&#x000b7;&#x03009; denotes the ensemble average. In order to compute these entropies, we need to be able to estimate the probability of occurrence of any sample word <italic>Y<sup>K</sup></italic>, knowing and not knowing the phase. To do so, a large amount of data is first generated. The noisy neuron is fed continuously for a large number of periods <italic>N<sub>p</sub></italic>&#x02009;=&#x02009;100 with a single periodic set of input spike trains and a fixed set of synaptic weights. The output spikes are recorded with &#x00394;&#x02009;=&#x02009;1&#x02009;ms precision. From this very long output spike train, we randomly pick words of length <italic>K</italic> and gather them in a set &#x1d4ae;. We take |&#x1d4ae;|&#x02009;=&#x02009;1000. This is our sample data.</p><p>In general, estimating the probability of a random binary vector of size <italic>K</italic> is very difficult if <italic>K</italic> is large. Luckily, we have a statistical model for how spike trains are generated (Eq. <xref ref-type="disp-formula" rid="E3">3</xref>), which considerably reduces the amount of data needed to produce a good estimate. Specifically, if the refractory state of the neuron [<italic>g<sub>R</sub></italic>(<italic>t</italic>),<italic>g<sub>A</sub></italic>(<italic>t</italic>)] is known at time <italic>t</italic> (initial conditions), then the probability 1&#x02009;&#x02212; exp(&#x02212;&#x003c1;<italic><sub>k</sub></italic>&#x00394;)&#x02009;&#x02243;&#x003c1;<italic><sub>k</sub></italic>&#x00394; of the postsynaptic neuron spiking is also known for each of the <italic>K</italic> time bins following <italic>t</italic> (Eqs <xref ref-type="disp-formula" rid="E3">3</xref>&#x02013;<xref ref-type="disp-formula" rid="E5">5</xref>). The neuron model gives us the probability that a word <italic>Y<sup>K</sup></italic> occurred at time <italic>t</italic> &#x02013; not necessarily the time at which the word was actually picked &#x02013; (Toyoizumi et al., <xref ref-type="bibr" rid="B46">2005</xref>):</p><disp-formula id="E7"><label>(7)</label><mml:math id="M15"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mo>|</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>k</mml:mi><mml:mi>K</mml:mi></mml:msubsup><mml:mtext>log</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mo>&#x003c1;</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mo>&#x00394;</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>k</mml:mi><mml:mi>K</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext>log</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mo>&#x003c1;</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mo>&#x00394;</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>where &#x003c1;<italic><sub>k</sub></italic>&#x02009;=&#x02009;&#x003c1;(<italic>t</italic>&#x02009;+ <italic>k</italic>&#x00394;) and <inline-formula><mml:math id="M16"><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>k</mml:mi><mml:mi>K</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is one if there is a spike in the word at position <italic>k</italic>, and 0 otherwise. To compute the conditional probability of occurrence of a word <italic>Y<sup>K</sup></italic> knowing the phase &#x003c6;, we have to further average Eq. <xref ref-type="disp-formula" rid="E7">7</xref>:</p><disp-formula id="E8"><label>(8)</label><mml:math id="M17"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mo>|</mml:mo><mml:mo>&#x003c6;</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>&#x02329;</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mo>|</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>&#x0232a;</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02009;</mml:mo><mml:mtext>with</mml:mtext><mml:mo>&#x02009;</mml:mo><mml:mo>&#x003a6;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>&#x003c6;</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><p>where &#x003c6;(<italic>t</italic>)&#x02009;=&#x02009;1&#x02009;+ (<italic>t</italic> mod <italic>N</italic><sub>&#x003c6;</sub>) denotes the phase at time <italic>t</italic>. Averaging over multiple times with same phase also averages over the initial conditions [<italic>g<sub>R</sub></italic>(<italic>t</italic>),<italic>g<sub>A</sub></italic>(<italic>t</italic>)], so that they do not appear in Eq. <xref ref-type="disp-formula" rid="E8">8</xref>. The average in Eq. <xref ref-type="disp-formula" rid="E8">8</xref> is estimated using a set of 10 randomly chosen times <italic>t<sub>i</sub></italic> with &#x003c6;(<italic>t<sub>i</sub></italic>)&#x02009;=&#x02009;&#x003c6;.</p><p>The full probability of observing a word <italic>Y<sup>K</sup></italic> is given by <inline-formula><mml:math id="M18"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mo>&#x003c6;</mml:mo></mml:msub><mml:mo>&#x02009;</mml:mo><mml:msubsup><mml:mo>&#x003a3;</mml:mo><mml:mrow><mml:mo>&#x003c6;</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mo>&#x003c6;</mml:mo></mml:msub></mml:mrow></mml:msubsup><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mo>|</mml:mo><mml:mo>&#x003c6;</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> where <italic>P</italic>(<italic>Y<sup>K</sup></italic>&#x02009;| &#x003c6;) is computed as described above. Owing to the knowledge of the model that underlies spike generation, and to this huge averaging over all the possible phases, the obtained <italic>P</italic>(<italic>Y<sup>K</sup></italic>) is a very good estimate of the true density. We can then take a Monte-Carlo approach to estimate the entropies, using the set &#x1d4ae; of randomly picked words: <inline-formula><mml:math id="M19"><mml:mrow><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mo>&#x003a3;</mml:mo><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:mrow/></mml:msubsup><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mtext>log</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> can be estimated by</p><disp-formula id="E9"><label>(9)</label><mml:math id="M20"><mml:mrow><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo>|</mml:mo><mml:mo>&#x1d4ae;</mml:mo><mml:mo>|</mml:mo></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mo>&#x02208;</mml:mo><mml:mo>&#x1d4ae;</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mtext>log</mml:mtext></mml:mrow><mml:mtext>2</mml:mtext></mml:msub><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><p>and <inline-formula><mml:math id="M21"><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mo>|</mml:mo><mml:mo>&#x003c6;</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mstyle displaystyle="true"><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mo>&#x003c6;</mml:mo></mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>&#x003c6;</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle><mml:mstyle displaystyle="true"><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mi>K</mml:mi></mml:msup></mml:mrow></mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mi>K</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mo>|</mml:mo><mml:mo>&#x003c6;</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mi>K</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mtext>log</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mo>|</mml:mo><mml:mo>&#x003c6;</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> is estimated using</p><disp-formula id="E10"><label>(10)</label><mml:math id="M22"><mml:mrow><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mo>|</mml:mo><mml:mo>&#x003c6;</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mo>&#x003c6;</mml:mo></mml:msub></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mo>&#x003c6;</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mo>&#x003c6;</mml:mo></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mfrac><mml:mtext>1</mml:mtext><mml:mrow><mml:mtext>|</mml:mtext><mml:mo>&#x1d4ae;</mml:mo><mml:mtext>|</mml:mtext></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mo>&#x02208;</mml:mo><mml:mo>&#x1d4ae;</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mo>|</mml:mo><mml:mo>&#x003c6;</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mi>K</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle><mml:msub><mml:mrow><mml:mtext>log</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mo>|</mml:mo><mml:mo>&#x003c6;</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><p>The MI estimate is the difference of these two entropies, and is expressed in bits. In Figure <xref ref-type="fig" rid="F2">2</xref>C, we introduce the information per spike MI&#x02019; (bits/spike), obtained by dividing the MI by the expected number of spikes in a window of duration <italic>K</italic>&#x00394;. Figure <xref ref-type="fig" rid="F2">2</xref>B shows that the MI approaches its upper bound log<sub>2</sub>(<italic>N</italic><sub>&#x003c6;</sub>) as the word size increases. The word size considered here (1&#x02009;s) is large enough to capture the effects of SFA while being small enough not to saturate the bound.</p><p>Although we constrain the postsynaptic firing rate to lie around a fixed value &#x003c1;<sub>targ</sub> (see homeostasis in the next section), the rate will always jitter. Even a small jitter of less than 0.5 Hz (which we have in the present case) makes it impossible to directly compare entropies across learning rules. Indeed, while the MI depends only weakly on small deviations of the firing rate around &#x003c1;<sub>targ</sub>, the response and noise entropies have much larger (co-)variations. In order to compare the entropies across learning rules, we need to know what the entropy would have been if the rate was exactly &#x003c1;<sub>targ</sub> instead of &#x003c1;<sub>targ</sub>&#x02009;+&#x02009;&#x003b5;. We therefore compute the entropy [<italic>H</italic>(<bold>Y<sup>K</sup></bold>) or <italic>H</italic>(<bold>Y<sup>K</sup></bold>|&#x003c6;)] for different firing rates in the vicinity of &#x003c1;<sub>targ</sub>. These firing rates are achieved by slightly rescaling the synaptic weights, i.e., <italic>w<sub>ij</sub></italic>&#x02009;&#x02190;&#x02009;&#x003ba;<italic>w<sub>ij</sub></italic> where &#x003ba; takes several values around 1. We then fit a linear model <italic>H</italic>&#x02009;=&#x02009;<italic>a</italic>&#x003c1;&#x02009;+ <italic>b</italic>, and evaluate <italic>H</italic> at &#x003c1;<sub>targ</sub>.</p><p>The computation of the conditional probabilities <italic>P</italic>(<italic>Y<sup>K</sup></italic>|&#x003c6;) was accelerated on an ATI Radeon (HD 4850) graphics processing unit (GPU), which was 130 times faster than a decent CPU implementation.</p></sec><sec><title>Learning rules</title><sec><title>Optimal learning rule</title><p>The optimal learning rule aims at maximizing information transmission under some metabolic constraints (&#x0201c;infomax&#x0201d; principle). Toyoizumi et al. (<xref ref-type="bibr" rid="B46">2005</xref>, <xref ref-type="bibr" rid="B47">2007</xref>) showed that this can be achieved by means of a stochastic gradient ascent on the following objective function</p><disp-formula id="E11"><label>(11)</label><mml:math id="M23"><mml:mrow><mml:mi mathvariant="double-struck">L</mml:mi><mml:mo>&#x0200a;</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">I</mml:mi><mml:mo>&#x0200a;</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mo>&#x003b3;</mml:mo><mml:mi mathvariant="double-struck">D</mml:mi><mml:mo>&#x0200a;</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mo>&#x0200a;</mml:mo><mml:mo>&#x003bb;</mml:mo><mml:mo>&#x003a8;</mml:mo></mml:mrow></mml:math></disp-formula><p>whereby the mutual information &#x1d540; between input and output spike trains competes with a homeostatic constraint on the mean firing rate &#x1d53b; and a metabolic penalty &#x003a8; for strong weights that are often active. The first constraint is formulated as <inline-formula><mml:math id="M24"><mml:mrow><mml:mi mathvariant="double-struck">D</mml:mi><mml:mo>=</mml:mo><mml:mtext>KL</mml:mtext><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mi>K</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> where KL denotes the Kullback&#x02013;Leibler (KL) divergence. <italic>P</italic> denotes the true probability distribution of output spike trains produced by the stochastic neuron model, while <inline-formula><mml:math id="M25"><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover></mml:math></inline-formula> assumes a similar model in which the gain <italic>g</italic>(<italic>t</italic>) is kept constant at a target gain <italic>g</italic><sub>targ</sub>. Minimizing the divergence between <italic>P</italic> and <inline-formula><mml:math id="M26"><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover></mml:math></inline-formula> therefore means driving the average gain close to <italic>g</italic><sub>targ</sub>, thus implementing firing rate homeostasis. The second constraint reads <inline-formula><mml:math id="M27"><mml:mrow><mml:mo>&#x003a8;</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mo>&#x003a3;</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mo>&#x02329;</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&#x0232a;</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> whereby the cost for synapse <italic>j</italic> is proportional to its weight <italic>w<sub>j</sub></italic> and to the average number <italic>n<sub>j</sub></italic> of presynaptic spikes relayed during the <italic>K</italic> time bins under consideration. The Lagrange multipliers &#x003b3; and &#x003bb; set the relative importance of the three objectives.</p><p>Performing gradient ascent on &#x1d543; yields the following online learning rule (Toyoizumi et al., <xref ref-type="bibr" rid="B46">2005</xref>, <xref ref-type="bibr" rid="B47">2007</xref>):</p><disp-formula id="E12"><label>(12)</label><mml:math id="M28"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mo>&#x003b7;</mml:mo><mml:mi>o</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mo>&#x003bb;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><p>where</p><disp-formula id="E13"><label>(13)</label><mml:math id="M29"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>&#x0222b;</mml:mo><mml:mn>0</mml:mn><mml:mi>t</mml:mi></mml:msubsup><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>&#x02032;</mml:mo><mml:mtext>exp</mml:mtext></mml:mrow></mml:mrow></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02032;</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mo>&#x003c4;</mml:mo><mml:mi>C</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x0200a;</mml:mo><mml:mo>&#x02009;</mml:mo><mml:msub><mml:mo>&#x003b5;</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02032;</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mrow><mml:mi>g</mml:mi><mml:mo>&#x02032;</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02032;</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02032;</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02032;</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02032;</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02032;</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><p>and</p><disp-formula id="E14"><label>(14)</label><mml:math id="M30"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>log</mml:mi><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mover accent="true"><mml:mi>g</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mtext>targ</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mover accent="true"><mml:mi>g</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>&#x003b3;</mml:mo></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mi>g</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mo>&#x003b3;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mtext>targ</mml:mtext></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mi>g</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><p>&#x003b7;<italic><sub>o</sub></italic> is a small learning rate. The first term <italic>C<sub>j</sub></italic> is Hebbian in the sense that it reflects the correlations between the input and output spike trains. <italic>B</italic><sub>post</sub> is purely postsynaptic: it compares the instantaneous gain <italic>g</italic> to its average <inline-formula><mml:math id="M31"><mml:mover accent="true"><mml:mi>g</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover></mml:math></inline-formula> (information term), as well as the average gain to its target value <italic>g</italic><sub>targ</sub> (homeostasis). The average <inline-formula><mml:math id="M32"><mml:mover accent="true"><mml:mi>g</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover></mml:math></inline-formula> is estimated online by a low pass filter of <italic>g</italic> with time constant &#x003c4;<italic><sub>g</sub></italic>. The time course of these quantities is shown in Figure <xref ref-type="fig" rid="F3">3</xref>A for example spike trains of 1&#x02009;s duration, for &#x003b3;&#x02009;=&#x02009;0.</p><fig id="F3" position="float"><label>Figure 3</label><caption><p><bold>Description of the three learning rules</bold>. <bold>(A)</bold> Time course of the variables involved in the optimal model. <inline-formula><mml:math id="M33"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo stretchy="true">&#x002dc;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> denotes the cumulative weight change. <bold>(B)</bold> Schematic representation of the phenomenological models of STDP used in this paper. Each presynaptic spike yields LTD proportionally to <italic>o</italic><sub>1</sub> (blue trace) in both models (pair and triplet). In the pair model, postsynaptic spikes evoke LTP proportionally to <italic>r<sub>j</sub></italic> (green trace), while in the triplet model <italic>r<sub>j</sub></italic> is combined with an additional postsynaptic trace <italic>o</italic><sub>2</sub> (red).</p></caption><graphic xlink:href="fncom-04-00143-g003"/></fig><p>Because of the competition between the three objectives in Eq. <xref ref-type="disp-formula" rid="E11">11</xref>, the homeostatic constraint does not yield the exact desired gain <italic>g</italic><sub>targ</sub>. In practice, we set the value of <italic>g</italic><sub>targ</sub> empirically, such that the actual mean firing rate approaches the desired value.</p><p>Finally, we use &#x003c4;<italic><sub>C</sub></italic>, &#x003b7;<italic><sub>o</sub></italic>, and &#x003bb; as three free parameters to fit the results of <italic>in vitro</italic> STDP pairing experiments (Figure <xref ref-type="fig" rid="F8">8</xref>). &#x003c4;<italic><sub>C</sub></italic> is set empirically equal to the membrane time constant &#x003c4;<italic><sub>m</sub></italic>&#x02009;=&#x02009;20&#x02009;ms, while &#x003b7;<italic><sub>o</sub></italic> and &#x003bb; are determined through a least-squares fit of the experimental data. The learning rate &#x003b7;<italic><sub>o</sub></italic> can be rescaled arbitrarily. In the simulations of receptive-field development (Figures <xref ref-type="fig" rid="F4">4</xref>&#x02013;<xref ref-type="fig" rid="F6">6</xref>), &#x003bb; is set to 0 so as not to perturb unnecessarily the prime objective of maximizing information transmission. It is also possible to remove the homeostasis constraint (&#x003b3;&#x02009;=&#x02009;0) in the presence of SFA. As can be seen in Figure <xref ref-type="fig" rid="F2">2</xref>C, the MI has a maximum at 7.5&#x02009;Hz when the neuron adapts, so that firing rate control comes for free in the information maximization objective. We therefore set &#x003b3;&#x02009;=&#x02009;0 when the neuron adapts, and &#x003b3;&#x02009;=&#x02009;1 when is does not. In fact, the homeostasis constraint only slightly impairs the infomax objective: we have checked that the MI reached after learning (Figures <xref ref-type="fig" rid="F4">4</xref> and <xref ref-type="fig" rid="F5">5</xref>) does not vary by more than 0.1 bit when &#x003b3; takes values as large as 20.</p><fig id="F4" position="float"><label>Figure 4</label><caption><p><bold>Triplets are better than pairs when the neuron adapts</bold>. <bold>(A)</bold> Distributions of synaptic efficacies obtained after learning. The weights were all initialized at 1&#x02009;mV before learning (black arrow). When SFA is switched off, the very same bimodal distributions emerge (not shown). <bold>(B)</bold> Evolution of the MI along learning time. Learning time is arbitrarily indexed from 0&#x02009;&#x0003c;&#x02009;&#x003b1;&#x02009;&#x0003c;&#x02009;1. The dashed curves represent the MI when the weights taken from the momentary distribution at time &#x003b1; are shuffled. Each point is obtained from averaging the MI over 10 different shuffled versions of the synaptic weights. Error bars denote standard error of the mean (SEM) over 10 independent learning episodes with different input spike trains. <bold>(C)</bold> Same as in <bold>(B)</bold>, but SFA is switched off. The <italic>y</italic>-scale is the same as in <bold>(B)</bold>. Parameters for those simulations were &#x003bb;&#x02009;=&#x02009;0, &#x003b3;&#x02009;=&#x02009;0 with SFA, and &#x003b3;&#x02009;=&#x02009;1 without SFA. Other parameters took the values given in Table <xref ref-type="table" rid="T1">1</xref>.</p></caption><graphic xlink:href="fncom-04-00143-g004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><p><bold>Results hold for &#x0201c;soft-bounded&#x0201d; STDP</bold>. The experiments of Figure <xref ref-type="fig" rid="F4">4</xref> are repeated with soft-bounds on the synaptic weights (see <xref ref-type="sec" rid="s2">Materials and Methods</xref>). <bold>(A)</bold> Bottom: LTP is weight-independent (black line), whereas the amount of LTD required by each learning rule (&#x00394;<italic>w</italic>&#x02009;&#x0003c;&#x02009;0) is modulated by a growing function of the momentary weight value (orange curve). The LTP and LTD curves cross at <italic>w</italic><sub>0</sub>&#x02009;=&#x02009;1&#x02009;mV, which is also the initial value of the weights in our simulations. Top: this form of weight dependence produces unimodal but skewed distributions of synaptic weights after learning, for all three learning rules. The learning paradigm is the same as in Figure <xref ref-type="fig" rid="F4">4</xref>. Gray lines denote the weight distributions when adaptation is switched off. Note that histograms are computed by binning all weight values from all learning experiments, but the distributions look similar on individual experiments. In these simulations &#x003bb;&#x02009;=&#x02009;0, <italic>a</italic>&#x02009;=&#x02009;9, and &#x003c4;<italic><sub>C</sub></italic>&#x02009;=&#x02009;0.4&#x02009;s. <bold>(B)</bold> The parameter &#x003c4;<italic><sub>C</sub></italic> of the optimal learning rule has been chosen such that the weight distribution after learning stays as close as possible to that of the pair and triplet models. &#x003c4;<italic><sub>C</sub></italic>&#x02009;=&#x02009;0.4&#x02009;s minimizes the KL divergences between the distribution obtained from the optimal model and those from the pair (black-blue) and triplet (black-red) learning rules. The distance is then nearly as small as the triplet-pair distance (red-blue). <bold>(C)</bold> MI along learning time in this weight-dependent STDP scenario (cf. Figures <xref ref-type="fig" rid="F4">4</xref>B,C). <bold>(D)</bold> Normalized information gain (see text for definition).</p></caption><graphic xlink:href="fncom-04-00143-g005"/></fig></sec><sec><title>Triplet-based learning rule</title><p>We use the minimal model developed in Pfister and Gerstner (<xref ref-type="bibr" rid="B35">2006</xref>) with &#x0201c;all-to-all&#x0201d; spike interactions. Presynaptic spikes at synapse <italic>j</italic> leave a trace <italic>r<sub>j</sub></italic> (Figure <xref ref-type="fig" rid="F3">3</xref>B) which jumps by 1 after each spike and otherwise decays exponentially with time constant &#x003c4;<sub>+</sub>. Similarly, the postsynaptic spikes leave two traces, <italic>o</italic><sub>1</sub> and <italic>o</italic><sub>2</sub>, which jump by 1 after each postsynaptic spike and decay exponentially with time constants &#x003c4;<sub>&#x02212;</sub> and &#x003c4;<italic><sub>y</sub></italic> respectively:</p><disp-formula id="E15"><label>(15)</label><mml:math id="M34"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mo>&#x003c4;</mml:mo><mml:mo>+</mml:mo></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02003;</mml:mo><mml:mo>&#x02003;</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>o</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mo>&#x003c4;</mml:mo><mml:mo>&#x02212;</mml:mo></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02003;</mml:mo><mml:mo>&#x02003;</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>o</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mo>&#x003c4;</mml:mo><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><p>where <italic>x<sub>j</sub></italic>(<italic>t</italic>) and <italic>y</italic>(<italic>t</italic>) are sums of &#x00394;-functions at each firing time as introduced above. The synaptic weight <italic>w<sub>j</sub></italic> undergoes LTD proportionally to <italic>o</italic><sub>1</sub> after each presynaptic spike, and LTP proportionally to <italic>r<sub>j</sub>o</italic><sub>2</sub> following each postsynaptic spike:</p><disp-formula id="E16"><label>(16)</label><mml:math id="M35"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mo>&#x003b7;</mml:mo><mml:mn>3</mml:mn></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mn>3</mml:mn><mml:mo>+</mml:mo></mml:msubsup><mml:msub><mml:mi>r</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mo>&#x003b5;</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mn>2</mml:mn><mml:mo>&#x02212;</mml:mo></mml:msubsup><mml:msub><mml:mi>o</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><p>where &#x003b7;<sub>3</sub> denotes the learning rate. Note that <italic>o</italic><sub>2</sub> is taken just before its update. Under the assumption that pre- and postsynaptic spike trains are independent Poisson processes with rates &#x003c1;<italic><sub>x</sub></italic> and &#x003c1;<italic><sub>y</sub></italic> respectively, the average weight change was shown in Pfister and Gerstner (<xref ref-type="bibr" rid="B35">2006</xref>) to be proportional to</p><disp-formula id="E17"><label>(17)</label><mml:math id="M36"><mml:mrow><mml:mrow><mml:mo>&#x02329;</mml:mo><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo>&#x0232a;</mml:mo></mml:mrow><mml:mo>&#x0221d;</mml:mo><mml:msub><mml:mo>&#x003c1;</mml:mo><mml:mi>x</mml:mi></mml:msub><mml:msub><mml:mo>&#x003c1;</mml:mo><mml:mi>y</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mo>&#x003c1;</mml:mo><mml:mi>y</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mo>&#x003c4;</mml:mo><mml:mo>&#x02212;</mml:mo></mml:msub><mml:msubsup><mml:mi>A</mml:mi><mml:mn>2</mml:mn><mml:mo>&#x02212;</mml:mo></mml:msubsup></mml:mrow><mml:mrow><mml:msub><mml:mo>&#x003c4;</mml:mo><mml:mo>+</mml:mo></mml:msub><mml:msub><mml:mo>&#x003c4;</mml:mo><mml:mi>y</mml:mi></mml:msub><mml:msubsup><mml:mi>A</mml:mi><mml:mn>3</mml:mn><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><p>The rule is thus structurally similar to a Bienenstock&#x02013;Cooper&#x02013;Munro (BCM) learning rule (Bienenstock et al., <xref ref-type="bibr" rid="B5">1982</xref>) since it is linear in the presynaptic firing rates and non-linear in the postsynaptic rate. It is possible to roughly stabilize the postsynaptic firing rate at a target value &#x003c1;<sub>targ</sub>, by having <inline-formula><mml:math id="M37"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mn>2</mml:mn><mml:mo>&#x02212;</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> slide in an activity-dependent manner:</p><disp-formula id="E18"><label>(18)</label><mml:math id="M38"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mn>2</mml:mn><mml:mo>&#x02212;</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mn>2</mml:mn><mml:mo>&#x02212;</mml:mo></mml:msubsup><mml:mfrac><mml:mrow><mml:msup><mml:mover accent="true"><mml:mo>&#x003c1;</mml:mo><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mn>3</mml:mn></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mo>&#x003c1;</mml:mo><mml:mrow><mml:mtext>targ</mml:mtext></mml:mrow><mml:mn>3</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><p>where <inline-formula><mml:math id="M39"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mn>2</mml:mn><mml:mo>&#x02212;</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> is a starting value and <inline-formula><mml:math id="M40"><mml:mover accent="true"><mml:mo>&#x003c1;</mml:mo><mml:mo>&#x000af;</mml:mo></mml:mover></mml:math></inline-formula> is an average of the instantaneous firing rate on the timescale of seconds or minutes (time constant &#x003c4;<sub>&#x003c1;</sub>). Finally, <inline-formula><mml:math id="M41"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mn>3</mml:mn><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> is set to make &#x003c1;<sub>targ</sub> an initial fixed point of the dynamics in Eq. <xref ref-type="disp-formula" rid="E17">17</xref>:</p><disp-formula id="E19"><label>(19)</label><mml:math id="M42"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mn>3</mml:mn><mml:mo>+</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mo>&#x003c4;</mml:mo><mml:mo>&#x02212;</mml:mo></mml:msub><mml:msubsup><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mn>2</mml:mn><mml:mo>&#x02212;</mml:mo></mml:msubsup></mml:mrow><mml:mrow><mml:msub><mml:mo>&#x003c1;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo>&#x003c4;</mml:mo><mml:mo>+</mml:mo></mml:msub><mml:msub><mml:mo>&#x003c4;</mml:mo><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><p>The postsynaptic rate should therefore roughly remain equal to its starting value &#x003c1;<sub>targ</sub>. In practice, the Poisson assumption is not valid because of adaptation and refractoriness, and independence becomes violated as learning operates. This causes the postsynaptic firing rate to deviate and stabilize slightly away from the target &#x003c1;<sub>targ</sub>. We therefore always set &#x003c1;<sub>targ</sub> empirically so that the firing rate stabilizes to the true desired target.</p></sec><sec><title>Pair-based learning rule</title><p>We use a pair-based STDP rule structurally similar to the triplet rule described by Eq. <xref ref-type="disp-formula" rid="E16">16</xref> (Figure <xref ref-type="fig" rid="F3">3</xref>B). The mechanism for LTD is identical, but LTP does not take into account previous postsynaptic firing:</p><disp-formula id="E20"><label>(20)</label><mml:math id="M43"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mo>&#x003b7;</mml:mo><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mn>2</mml:mn><mml:mo>+</mml:mo></mml:msubsup><mml:msub><mml:mi>r</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mn>2</mml:mn><mml:mo>&#x02212;</mml:mo></mml:msubsup><mml:msub><mml:mi>o</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><p>where &#x003b7;<sub>2</sub> is the learning rate. <inline-formula><mml:math id="M44"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mn>2</mml:mn><mml:mo>&#x02212;</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> also slides in an activity-dependent manner according to Eq. <xref ref-type="disp-formula" rid="E18">18</xref>, to help stabilizing the output firing rate at a target &#x003c1;<sub>targ</sub>. <inline-formula><mml:math id="M45"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mn>2</mml:mn><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> is set such that LTD initially balances LTP, i.e.,</p><disp-formula id="E21"><label>(21)</label><mml:math id="M46"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mn>2</mml:mn><mml:mo>+</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mn>2</mml:mn><mml:mo>&#x02212;</mml:mo></mml:msubsup><mml:msub><mml:mo>&#x003c4;</mml:mo><mml:mo>&#x02212;</mml:mo></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mo>&#x003c4;</mml:mo><mml:mo>+</mml:mo></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><p>Comparing learning rules in a fair way requires making sure that their learning rates are equivalent. Since the two rules share the same LTD mechanism, we can simply take the same value for <inline-formula><mml:math id="M47"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mn>2</mml:mn><mml:mo>&#x02212;</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> as well as &#x003b7;<sub>2</sub>&#x02009;=&#x02009;&#x003b7;<sub>3</sub>. Since LTD is dynamically regulated to balance LTP on average in both rules, this ensures that they also share the same LTP rate.</p></sec><sec><title>Weight bounds</title><p>In order to prevent the weights from becoming negative or from growing too large, we set hard bounds on the synaptic efficacies for all three learning rules, when not stated otherwise. That is, if the learning rule requires a weight change &#x00394;<italic>w<sub>j</sub></italic>, <italic>w<sub>j</sub></italic> is set to</p><disp-formula id="E22"><label>(22)</label><mml:math id="M48"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&#x02190;</mml:mo><mml:mtext>min</mml:mtext><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext>max</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mo>&#x00394;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><p>This type of bounds, in which the weight change is independent of the initial synaptic weight itself, is known to yield bimodal distributions of synaptic efficacies. In the simulation of Figure <xref ref-type="fig" rid="F5">5</xref>, we also consider the following soft bounds to extend the validity of our results to unimodal distributions of weights:</p><disp-formula id="E23"><label>(23)</label><mml:math id="M49"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mtext>if</mml:mtext><mml:mo>&#x02009;</mml:mo><mml:mo>&#x00394;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&#x02265;</mml:mo><mml:mn>0</mml:mn><mml:mo>&#x02003;</mml:mo><mml:mtext>then</mml:mtext><mml:mo>&#x02003;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&#x02190;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mo>&#x00394;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>if</mml:mtext><mml:mo>&#x02009;</mml:mo><mml:mo>&#x00394;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&#x0003c;</mml:mo><mml:mn>0</mml:mn><mml:mo>&#x02003;</mml:mo><mml:mtext>then</mml:mtext><mml:mo>&#x02003;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&#x02190;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>a</mml:mi><mml:mfrac><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>&#x00394;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>where <italic>a</italic> is a free parameter and <italic>w</italic><sub>0</sub>&#x02009;=&#x02009;1&#x02009;mV is the value at which synaptic weights are initialized at the beginning of all learning experiments. This choice of soft-bounds is further motivated in Section <xref ref-type="sec" rid="s1">&#x0201c;Results.&#x0201d;</xref> The shapes of the LTP and LTD weight-dependent factors are drawn in Figure <xref ref-type="fig" rid="F5">5</xref>A, for <italic>a</italic>&#x02009;=&#x02009;9. Note that the LTD and LTP factors cross at <italic>w</italic><sub>0</sub>, which ensures that the balance between LTP and LTD set by Eqs <xref ref-type="disp-formula" rid="E19">19</xref> and <xref ref-type="disp-formula" rid="E21">21</xref> is initially preserved.</p><p>When the soft-bounds are used, the parameter &#x003c4;<italic><sub>C</sub></italic> of the optimal model is adjusted so that the weight distribution obtained with the optimal rule best matches the weight distributions of the pair and triplet rules. This parameter indeed has an impact on the spread of the weight distribution: the optimal model knows about the generative model that underlies postsynaptic spike generation, and therefore takes optimally the noise into account, as long as &#x003c4;<italic><sub>C</sub></italic> spans no more than the width of the postsynaptic autocorrelation Toyoizumi et al. (<xref ref-type="bibr" rid="B46">2005</xref>). If &#x003c4;<italic><sub>C</sub></italic> is equal to this width (about 20&#x02009;ms), some weights can grow very large (&#x0003e;50&#x02009;mV), which results in non-realistic weight distributions. Increasing &#x003c4;<italic><sub>C</sub></italic> imposes more detrimental noise such that all weights are kept within reasonable bounds. In order to constrain &#x003c4;<italic><sub>C</sub></italic> in a non-arbitrary way, we ran the learning experiment for several values of &#x003c4;<italic><sub>C</sub></italic> and computed the KL divergences between weight distributions (optimal-triplet, optimal-pair). &#x003c4;<italic><sub>C</sub></italic> is chosen to minimize these, as shown in Figure <xref ref-type="fig" rid="F5">5</xref>B.</p></sec></sec><sec><title>Simulation of <italic>in vitro</italic> experiments</title><p>To obtain the predictions of the optimal model on standard <italic>in vitro</italic> STDP experiments, we compute the weight change of a single synapse (<italic>N</italic>&#x02009;=&#x02009;1) according to Eq. <xref ref-type="disp-formula" rid="E12">12</xref>. The effect of the remaining thousands of synapses is concentrated in a large background noise, obtained by adding a <italic>u<sub>b</sub></italic>&#x02009;=&#x02009;19&#x02009;mV baseline to the voltage. The gain becomes <italic>g<sub>b</sub></italic>&#x02009;=&#x02009;<italic>g</italic>(<italic>u<sub>b</sub></italic>) &#x02243;(&#x02009;21.45 Hz, which in combination with adaptation and refractoriness would yield a spontaneous firing rate of about 7.5&#x02009;Hz (see Figure <xref ref-type="fig" rid="F1">1</xref>). Spontaneous firing is artificially blocked, however. Instead, the neuron is forced to fire at precise times as described below.</p><p>The standard pairing protocol is made of a series of pre&#x02013;post spike pairs, the spikes within the same pair being separated by &#x00394;<italic>s</italic>&#x02009;=&#x02009;<italic>t</italic><sub>post</sub>&#x02009;&#x02212;&#x02009;<italic>t</italic><sub>pre</sub>. Pairs are repeated with some frequency <italic>f</italic>. The average <inline-formula><mml:math id="M50"><mml:mover accent="true"><mml:mi>g</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover></mml:math></inline-formula> is taken fixed and equal to <italic>g<sub>b</sub></italic>, considering that STDP is optimal for <italic>in vivo</italic> conditions such that <inline-formula><mml:math id="M51"><mml:mover accent="true"><mml:mi>g</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover></mml:math></inline-formula> should not adapt to the statistics of <italic>in vitro</italic> conditions. The homeostasis is turned off (&#x003b3;&#x02009;=&#x02009;0) in order to consider only the effects of the infomax principle.</p></sec></sec><sec id="s1"><title>Results</title><p>We study information transmission through a neuron modeled as a noisy communication channel. It receives input spike trains from a hundred plastic excitatory synapses, and stochastically generates output spikes according to an instantaneous firing rate modulated by presynaptic activities. Importantly, the firing rate is also modulated by the neuron's own firing history, in a way that captures the SFA mechanism found in a large number of cortical cell types. We investigate the ability of three different learning rules to enhance information transmission in this framework. The first learning rule is the standard pair-based STDP model, whereby every single pre-before-post (resp. post-before-pre) spike pair yields LTP (resp. LTD) according to a standard double exponential asymmetric window (Bi and Poo, <xref ref-type="bibr" rid="B4">1998</xref>; Song et al., <xref ref-type="bibr" rid="B43">2000</xref>). The second one was developed in Pfister and Gerstner (<xref ref-type="bibr" rid="B35">2006</xref>) and is based on triplets of spikes. LTD is obtained similarly to the pair rule, whereas LTP is obtained from pairing a presynaptic spike with two postsynaptic spikes. The third learning rule (Toyoizumi et al., <xref ref-type="bibr" rid="B46">2005</xref>) is derived from the infomax principle, under some metabolic constraints.</p><sec><title>Triplet-STDP is better than pair-STDP when the neuron adapts</title><p>We assess and compare the performance of each learning rule on a simple spatio-temporal receptive field development task, with <italic>N</italic>&#x02009;=&#x02009;100 presynaptic neurons converging onto a single postsynaptic cell (Figure <xref ref-type="fig" rid="F2">2</xref>A).</p><p>For each presynaptic neuron, a 5-s input spike train is generated once and for all (see <xref ref-type="sec" rid="s2">Materials and Methods</xref>). All presynaptic spike trains are then replayed continuously 5,000 times. All synapses undergo STDP according to one of the three learning rules. Synaptic weights are all initially set to 1&#x02009;mV, which yields an initial output firing rate of about 7.5&#x02009;Hz. We set the target firing rate &#x003c1;<sub>targ</sub> of each learning rule such that the output firing rate stays very close to 7.5 Hz. To gather enough statistics, the whole experiment is repeated 10 times independently, each time with different input patterns. All results are therefore reported as mean and standard error of the mean (SEM) over the 10 trials.</p><p>All three learning rules developed very similar bimodal distributions of synaptic efficacies (Figure <xref ref-type="fig" rid="F4">4</xref>A), irrespective of the presence or absence of SFA. This is a well known consequence of additive STDP with hard bounds imposed on the synaptic weights (Kempter et al., <xref ref-type="bibr" rid="B20">1999</xref>; Song et al., <xref ref-type="bibr" rid="B43">2000</xref>). The firing rate stabilizes at 7.5&#x02009;Hz as desired, for all plasticity rules (not shown). In Figure <xref ref-type="fig" rid="F4">4</xref>B, we show the evolution of the MI (solid lines) as a function of learning time. It is computed as described in Section <xref ref-type="sec" rid="s2">&#x0201c;Materials and Methods,&#x0201d;</xref> from the postsynaptic activity gathered during 100 periods (500&#x02009;s). Since we are interested in quantifying the ability of different learning rules to enhance information transmission, we look at the information gain [defined as MI(&#x003b1;&#x02009;=&#x02009;1)&#x02009;&#x02212; MI(&#x003b1;&#x02009;=&#x02009;0)] rather than the absolute value of the MI after learning. The triplet model reaches 98% of the &#x0201c;optimal&#x0201d; information gain while the pair model reaches 86% of it. Note that we call &#x0201c;optimal&#x0201d; what comes from the optimality model, but it is not necessarily the optimum in the space of solutions, because (i) a stochastic gradient ascent may not always lead to the global maximum, (ii) Toyoizumi et al.&#x02019;s (<xref ref-type="bibr" rid="B46">2005</xref>) optimal learning rule involves a couple of approximations that may result in a sub-optimal algorithm, and (iii) their learning rule does not specifically optimize information transmission for our periodic input scenario, but rather in a more general setting where input spike trains are drawn continuously from a fixed distribution (stationarity).</p><p>It is instructive to compare how much information is lost for each learning rule when the synaptic weights are shuffled. Shuffling means that the distribution stays exactly the same, while the detailed assignment of each <italic>w<sub>j</sub></italic> is randomized. The dashed lines in Figure <xref ref-type="fig" rid="F4">4</xref>B depict the MI under these shuffling conditions. Each point is obtained from averaging the MI over 10 different shuffled versions of the weights. The optimal and triplet model lose respectively 33 and 32% of their information gains, while the pair model loses only 23%. This means that the optimal and triplet learning rules make a better choice in terms of the detailed assignment of each synaptic weight. For the pair learning rule, a larger part of the information gain is a mere side-effect of the weight distribution becoming bimodal. As an aside, we observe that the MI is the same (4.5 bits) in the &#x0201c;shuffled&#x0201d; condition for all three learning rules. This is an indication that we can trust our information comparisons. The result is also compatible with the value found by randomly setting 20 weights to the maximum value and the others to 0 (Figure <xref ref-type="fig" rid="F2">2</xref>B, square mark).</p><p>How is adaptation involved in this increased channel capacity? In Figure <xref ref-type="fig" rid="F2">2</xref>C, the MI is plotted as a function of the postsynaptic firing rate, for an adaptive (black dots) and a non-adaptive (gray dots) neuron, irrespective of synaptic plasticity. Each point in the figure is obtained by setting randomly a given fraction &#x003c7; of synaptic weights to the upper bound (4&#x02009;mV), and the rest to 0 mV. The weight distribution stays bimodal, which leaves the neuron in a high information transmission state. &#x003c7; is varied in order to cover a wide range of firing rates. We see that adaptation enhances information transmission at low firing rates (&#x0003c;10 Hz). The MI has a maximum at 7.5&#x02009;Hz when the neuron is adapting (black circles). If adaptation is removed, the peak broadens and shifts to about 15 Hz (green circles). If the energetic cost of firing spikes is also taken into account, the best performance is achieved at 3&#x02009;Hz, whether adaptation is enabled or not. This is illustrated in Figure <xref ref-type="fig" rid="F2">2</xref>C (lower plot) where the information per spike is reported as a function of the firing rate.</p><p>Is adaptation beneficial in a general sense only, or does it differentially affect the three learning rules? To answer this question, we have the neuron learn again from the beginning, SFA being switched off. The temporal evolution of the MI for each learning rule is shown in Figure <xref ref-type="fig" rid="F4">4</xref>C. Overall, the MI is lower when the neuron does not adapt (compare Figure <xref ref-type="fig" rid="F4">4</xref>B and Figure <xref ref-type="fig" rid="F4">4</xref>C), which is in agreement with the previous paragraph and Figure <xref ref-type="fig" rid="F2">2</xref>C. Importantly, the triplet model loses its advantage over the pair model when adaptation is removed (compared red and blue lines in Figure <xref ref-type="fig" rid="F4">4</xref>C). This suggests a specific interaction between synaptic plasticity and the intrinsic postsynaptic dynamics in the optimal and triplet models. This is further investigated in later sections.</p><p>Finally, the main results of Figure <xref ref-type="fig" rid="F4">4</xref> also hold when the distribution of weights remains unimodal. To achieve unimodal distributions with STDP, the hypothesis of hard-bounded synaptic efficacies must be relaxed. We implemented a form of weight-dependence of the weight change, such that LTP stays independent of the synaptic efficacy, while stronger synapses are depressed more strongly (see <xref ref-type="sec" rid="s2">Materials and Methods</xref>). The weight-dependent factor for LTD had traditionally been modeled as being directly proportional to <italic>w<sub>j</sub></italic> (e.g., van Rossum et al., <xref ref-type="bibr" rid="B49">2000</xref>), which provides a good fit to the data obtained from cultured hippocampal neurons by Bi and Poo (<xref ref-type="bibr" rid="B4">1998</xref>). Morrison et al. (<xref ref-type="bibr" rid="B30">2007</xref>) proposed an alternative fit of the same data with a different form of weight-dependence of LTP. Here we use a further alternative (see <xref ref-type="sec" rid="s2">Materials and Methods</xref>, and Figure <xref ref-type="fig" rid="F5">5</xref>A). We require that the multiplicative factors for LTP and LTD exactly match at <italic>w<sub>j</sub></italic>&#x02009;=&#x02009;<italic>w</italic><sub>0</sub>&#x02009;=&#x02009;1&#x02009;mV, where initial weights are set in our simulations. Further, we found it necessary that the slope of the LTD modulation around <italic>w</italic><sub>0</sub> be less than 1. Indeed, our neuron model is very noisy, such that reproducible pre&#x02013;post pairs that need to be reinforced actually occur among a sea of random pre&#x02013;post and post&#x02013;pre pairs. If LTD too rapidly overcomes LTP above <italic>w</italic><sub>0</sub>, there is no chance for the correlated pre&#x02013;post spikes to evoke sustainable LTP. The slope must be small enough for correlations to be picked up. This motivates our choice of weight dependence for LTD as depicted in Figure <xref ref-type="fig" rid="F5">5</xref>A. The weight distributions for all three learning rules stay indeed unimodal, but highly positively skewed, such that the neuron can really &#x0201c;learn&#x0201d; by giving some relevant synapses large weights (tails of the distributions in Figure <xref ref-type="fig" rid="F5">5</xref>A). Note that the obtained weight distributions resemble those recorded by Sj&#x000f6;str&#x000f6;m et al. (<xref ref-type="bibr" rid="B41">2001</xref>) (see e.g., Figure <xref ref-type="fig" rid="F3">3</xref>C in their paper).</p><p>The evolution of the MI along learning time is reported in Figure <xref ref-type="fig" rid="F5">5</xref>C. Overall, MI values are lower than those of Figure <xref ref-type="fig" rid="F4">4</xref>B. Unimodal distributions of synaptic efficacies are less informative than purely bimodal distributions, reflecting the lower degree of specialization to input features. Such distributions may however be advantageous in a memory storage task where old memories which are not recalled often need to be erased to store new ones. In this scenario, strong weights which become irrelevant can quickly be sent back from the tail to the main weight pool around 1&#x02009;mV. For a detailed study of the impact of the weight-dependence on memory retention, see Billings and van Rossum (<xref ref-type="bibr" rid="B6">2009</xref>).</p><p>We see that it is difficult to directly compare absolute values of the MI in Figure <xref ref-type="fig" rid="F5">5</xref>C, since the &#x0201c;shuffled&#x0201d; MIs (dashed lines) do not converge to the same value. This is because some weight distributions are more skewed than others (compare red and blue distributions in Figure <xref ref-type="fig" rid="F5">5</xref>A). In the present study, we are more interested in knowing how good plasticity rules are at selecting individual weights for up- or down-regulation, on the basis of the input structure. We would like our performance measure to be free of the actual weight distribution, which is mainly shaped by the weight-dependence of Eq. <xref ref-type="disp-formula" rid="E23">23</xref>. We therefore compare the normalized information gain, i.e., [MI(&#x003b1;&#x02009;=&#x02009;1) &#x02212;&#x02009;MI(&#x003b1;&#x02009;=&#x02009;0)] / [MI<sub>sh</sub>(&#x003b1;&#x02009;=&#x02009;1) &#x02212;&#x02009;MI(&#x003b1;&#x02009;=&#x02009;0)], where MI<sub>sh</sub> denotes the MI for shuffled weights. The result is shown in Figure <xref ref-type="fig" rid="F5">5</xref>D: the triplet is again better than the pair model, provided the postsynaptic neuron adapts.</p><p>Our simulations show that when SFA modulates the postsynaptic firing rate, the triplet model yields a better gain in information transmission than pair-STDP does. When adaptation is removed, this advantage vanishes. There must be a specific interaction between triplet-STDP and adaptation that we now seek to unravel.</p></sec><sec><title>Triplet-STDP increases the response entropy when the neuron adapts</title><p>Information transmission improves if the neuron learns to produce more diverse spike trains [<italic>H</italic>(<bold>Y<sup>K</sup></bold>) increases], and if the neuron becomes more reliable [<italic>H</italic>(<bold>Y<sup>K</sup></bold>|&#x003c6;) decreases] In Figure <xref ref-type="fig" rid="F6">6</xref>A we perform a differential analysis of both entropies, on the same data as presented in Figure <xref ref-type="fig" rid="F4">4</xref> (i.e., for hard-bounded STDP). Whether the postsynaptic neuron adapts (top) or not (bottom), the noise entropy (right) is drastically reduced, and the triplet learning rule does so better than the pair model (compare red and blue). The differential impact of adaptation on the two models can only be seen in the behavior of the response entropy <italic>H</italic>(<bold>Y<sup>K</sup>)</bold> (left). When the postsynaptic neuron adapts, triplet- and optimal STDP both increase the response entropy, while it decreases with the pair model. This behavior is reflected in the interspike-interval (ISI) distributions, shown in Figure <xref ref-type="fig" rid="F6">6</xref>B. With adaptation, the optimal and triplet rules produce distributions that are close to an exponential (which would be a straight line in the logarithmic <italic>y</italic>-scale). In contrast, the ISI distribution obtained from pair-STDP stays almost flat for ISIs between 25 and 120&#x02009;ms. Without adaptation, the optimal and triplet models further sparsifies the ISI distribution which then becomes sparser than an exponential, reducing the response entropy.</p><fig id="F6" position="float"><label>Figure 6</label><caption><p><bold>Differential analysis of the entropies</bold>. The learning experiments are the same as in Figure <xref ref-type="fig" rid="F4">4</xref>, using hard-bounds on the synaptic weights. <bold>(A)</bold> Response entropy (left) and noise entropy (right) with (top) and without (bottom) postsynaptic SFA. Entropies are calculated at the end of the learning process, except for the gray boxes which denote the entropies prior to learning. <bold>(B)</bold> Interspike-interval distributions with (left) and without (right) SFA, after learning (except gray line, before learning). The main plots have a logarithmic <italic>y</italic>-scale, whereas the insets have a linear one. <bold>(C)</bold> Peri-stimulus time histograms (PSTHs) prior to learning (top) and after learning for each learning rule, over a full 5-s period. All plots share the same <italic>y</italic>-scale. <bold>(D)</bold> Power spectra of the PSTHs shown in <bold>(C)</bold>, averaged over the 10 independent learning experiments.</p></caption><graphic xlink:href="fncom-04-00143-g006"/></fig><p>Qualitative similarities between the optimal and triplet models can also be found in the power spectrum of the peri-stimulus time histogram (PSTH). The PSTHs are plotted in Figure <xref ref-type="fig" rid="F6">6</xref>C over a full 5-s period, and their average power spectra are displayed in Figure <xref ref-type="fig" rid="F6">6</xref>D. The PSTH is almost flat prior to learning, reflecting the absence of feature selection in the input. Learning in all three learning rules creates sharp peaks in the PSTH, which illustrates the drop in noise entropy seen in Figure <xref ref-type="fig" rid="F6">6</xref>A (right). The pair learning rule produces PSTHs with almost no power at low frequencies (below 5&#x02009;Hz). In contrast, these low frequencies are strongly boosted by the optimal and triplet models. This is however not specific to SFA being on or off (not shown). We give an intuitive account for this in Section <xref ref-type="sec" rid="s3">&#x0201c;Discussion.&#x0201d;</xref></p><p>This section has shed light on qualitative similarities in the way the optimal and triplet learning rules enhance information transmission in an adaptive neuron. We now seek to understand the reason why taking account of triplets of spikes would be close-to-optimal in the presence of postsynaptic SFA.</p></sec><sec><title>The optimal model exhibits a triplet effect</title><p>How similar is the optimal model to the triplet learning rule? In essence, the optimal model is a stochastic gradient learning rule, which updates the synaptic weights at every time step depending on the recent input&#x02013;output correlations and the current relevance of the postsynaptic state. In contrast to this, phenomenological models require changing the synaptic efficacy upon spike occurrence only. It is difficult to compress what happens between spikes in the optimal model down to a single weight change at spike times. However we know that the dependence of LTP on previous postsynaptic firing is a hallmark of the triplet rule, and is absent in the pair rule. We therefore investigate the behavior of the optimal learning rule on post&#x02013;pre&#x02013;post triplets of spikes, and find a clear triplet effect (Figure <xref ref-type="fig" rid="F7">7</xref>).</p><fig id="F7" position="float"><label>Figure 7</label><caption><p><bold>The optimal model incorporates a triplet effect when the postsynaptic neuron adapts</bold>. <bold>(A)</bold> A pre&#x02013;post pair (&#x00394;<italic>s</italic>&#x02009;=&#x02009;15&#x02009;ms interval, black lines in the first two rows) is preceded by another postsynaptic spike. The post&#x02013;post interval &#x00394;<italic>p</italic> is made either 16 (red line), 100 (purple), and 200&#x02009;ms (blue). The time course of <italic>C<sub>j</sub></italic>, <italic>B</italic><sub>post</sub>, and the cumulative weight change <inline-formula><mml:math id="M52"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo stretchy="true">&#x002dc;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> are plotted in the bottom rows. <bold>(B)</bold> Total weight change (optimal model) as a function of the post&#x02013;post interval, for various adaptation time constants, and without adaptation (dashed line).</p></caption><graphic xlink:href="fncom-04-00143-g007"/></fig><p>We consider an isolated post&#x02013;pre&#x02013;post triplet of spikes, in this order (Figure <xref ref-type="fig" rid="F7">7</xref>A). Isolated means that the last pre- and postsynaptic spikes occurred a very long time before this triplet. Let <inline-formula><mml:math id="M53"><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow><mml:mn>1</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>, <italic>t</italic><sub>pre</sub>, and <inline-formula><mml:math id="M54"><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> denote the spike times. The pre&#x02013;post interval is kept constant equal to <inline-formula><mml:math id="M55"><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext>pre</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>15</mml:mn><mml:mo>&#x02009;</mml:mo><mml:mtext>ms</mml:mtext></mml:mrow></mml:math></inline-formula>. We vary the length of the post&#x02013;post interval <inline-formula><mml:math id="M56"><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow><mml:mn>1</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> from 16 to 500&#x02009;ms. The resulting weight change is depicted in Figure <xref ref-type="fig" rid="F7">7</xref>B. For comparison, the triplet model would produce &#x02013; by construction &#x02013; a decaying exponential with time constant &#x003c4;<italic><sub>y</sub></italic>. In the optimal model, potentiation decreases as the post&#x02013;post interval increases. Two times constants show up in this decay, which reflect that of refractoriness (2&#x02009;ms) and adaptation (150&#x02009;ms). The same curve is drawn for two other adaptation time constants (see red and blue curves). When adaptation is removed, the triplet effect vanishes (dashed curve). It should be noted that the isolated pre&#x02013;post pair itself (i.e., large post&#x02013;post interval) results in a baseline amount of LTP, which is not the case in the triplet model. Figure <xref ref-type="fig" rid="F7">7</xref>A shows how this effect arises mechanistically. Three different triplets are shown, with the pre&#x02013;post pair being fixed, and the post&#x02013;post interval being either 16, 100, or 200&#x02009;ms (red, purple, and blue respectively).</p><p>To further highlight the similarity between the optimal learning rule and the triplet model, we now derive an analytical expression for the optimal weight change that follows a post&#x02013;pre&#x02013;post triplet of spikes. Let us observe that the final cumulated weight change evoked by the triplet is dominated by the jump that occurs just following the second postsynaptic spike (Figure <xref ref-type="fig" rid="F7">7</xref>A) &#x02013; except for the negative jump of size &#x003bb; that follows the presynaptic spike arrival, but this is a constant. Our analysis therefore concentrates on the values of <inline-formula><mml:math id="M57"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M58"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Let us denote by <inline-formula><mml:math id="M59"><mml:mrow><mml:msub><mml:mo>&#x003b5;</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext>exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mo>&#x00394;</mml:mo><mml:mi>s</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mo>&#x003c4;</mml:mo><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> the value of the unitary synaptic PSP at time <inline-formula><mml:math id="M60"><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>. Around the baseline potential <italic>u<sub>b</sub></italic>&#x02009;=&#x02009;19&#x02009;mV, the gain function is approximately linear (cf. Figure <xref ref-type="fig" rid="F1">1</xref>A), i.e., <inline-formula><mml:math id="M61"><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mo>&#x003b5;</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02243;</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:msup><mml:mi>g</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mi>b</mml:mi></mml:msub><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mo>&#x003b5;</mml:mo><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> where <italic>g<sub>b</sub></italic>&#x02009;=&#x02009;<italic>g</italic>(<italic>u<sub>b</sub></italic>) and <inline-formula><mml:math id="M62"><mml:mrow><mml:msub><mml:msup><mml:mi>g</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mi>b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mi>g</mml:mi><mml:mo>/</mml:mo><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:msub><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are constants. From Eq. <xref ref-type="disp-formula" rid="E14">14</xref>, we read <inline-formula><mml:math id="M63"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>log[1</mml:mtext><mml:mo>+</mml:mo><mml:mo>&#x02009;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:msup><mml:mi>g</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mi>b</mml:mi></mml:msub><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mo>&#x003b5;</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>&#x02009;</mml:mo><mml:mo>&#x003b4;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula>which is approximately equal to</p><disp-formula id="E24"><label>(24)</label><mml:math id="M64"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02243;</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:msup><mml:mi>g</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mi>b</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mo>&#x003b5;</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:mo>&#x003b4;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><p>assuming the contribution of <italic>w<sub>j</sub></italic>&#x003b5;<italic><sub>j</sub></italic> is small compared to the baseline gain <italic>g<sub>b</sub></italic>. The term proportional to <italic>M</italic> in Eq. <xref ref-type="disp-formula" rid="E14">14</xref> is negligible compared to the &#x00394;-function. From Eq. <xref ref-type="disp-formula" rid="E13">13</xref>, we see that</p><disp-formula id="E25"><label>(25)</label><mml:math id="M65"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mo>&#x003b5;</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:msup><mml:mi>g</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mi>b</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:msup><mml:mi>g</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mi>b</mml:mi></mml:msub><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mo>&#x003b5;</mml:mo><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:mo>&#x003b5;</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><p>The total weight change following the second postsynaptic spike is therefore</p><disp-formula id="E26"><label>(26)</label><mml:math id="M66"><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02243;</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:msup><mml:mi>g</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mi>b</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msubsup><mml:mo>&#x003b5;</mml:mo><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:msup><mml:mi>g</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mi>b</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mo>&#x003b5;</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mi>C</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:mo>&#x003b5;</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><p>where</p><disp-formula id="E27"><label>(27)</label><mml:math id="M67"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:mo>&#x003b5;</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>&#x0222b;</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext>pre</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:mo>&#x003b5;</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mo>&#x003c4;</mml:mo><mml:mi>C</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mstyle><mml:mo>&#x0200a;</mml:mo><mml:mo>&#x0200a;</mml:mo><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext>pre</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mo>&#x003c4;</mml:mo><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:msup><mml:mi>g</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mi>b</mml:mi></mml:msub><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></disp-formula><p>Since we have taken &#x003c4;<italic><sub>C</sub></italic>&#x02009;=&#x02009;&#x003c4;<italic><sub>m</sub></italic>, the first two exponentials collapse into &#x003b5;<italic><sub>j</sub></italic>. To carry out the integration, let us further simplify the adaptation model into <inline-formula><mml:math id="M68"><mml:mrow><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mtext>exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:msub><mml:mo>&#x003c4;</mml:mo><mml:mi>A</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, assuming that <inline-formula><mml:math id="M69"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext>pre</mml:mtext></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:mo>&#x0003e;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> ms so that the refractoriness has already vanished at the time of the presynaptic spike, while adaptation remains. It is also assumed that the triplet is isolated, so that we can neglect the cumulative effect of adaptation. Eq. <xref ref-type="disp-formula" rid="E27">27</xref> becomes</p><disp-formula id="E28"><label>(28)</label><mml:math id="M70"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:mo>&#x003b5;</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mi>b</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:msub><mml:mo>&#x003b5;</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02009;</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mo>&#x003c4;</mml:mo><mml:mi>A</mml:mi></mml:msub><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mo>&#x003c4;</mml:mo><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mo>&#x003c4;</mml:mo><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><p>If &#x00394;<italic>s</italic>&#x02009;&#x0226a; &#x003c4;<italic><sub>A</sub></italic>, the last term into square brackets is approximately &#x00394;<italic>s</italic>/&#x003c4;<italic><sub>A</sub></italic>. If not, <inline-formula><mml:math id="M71"><mml:mrow><mml:msub><mml:mo>&#x003b5;</mml:mo><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> becomes so small that the whole r.h.s of Eq. <xref ref-type="disp-formula" rid="E26">26</xref> vanishes. To sum up, the total weight change following the second postsynaptic spike is given by</p><disp-formula id="E29"><label>(29)</label><mml:math id="M72"><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:msup><mml:mi>g</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mi>b</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msubsup><mml:mo>&#x003b5;</mml:mo><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>&#x02212;</mml:mo><mml:mo>&#x00394;</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:msup><mml:mi>g</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mi>b</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>&#x00394;</mml:mo><mml:mi>s</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msubsup><mml:mo>&#x003b5;</mml:mo><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mo>&#x003c4;</mml:mo><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><p>The first term on the r.h.s of Eq. <xref ref-type="disp-formula" rid="E29">29</xref> is a pair term, i.e., a weight change that depends only on the pre&#x02013;post interval &#x00394;<italic>s</italic>. We note that it is proportional to <inline-formula><mml:math id="M73"><mml:mrow><mml:msubsup><mml:mo>&#x003b5;</mml:mo><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>, meaning that the time constant of the causal part of the STDP learning window is half the membrane time constant. The second term exactly matches the triplet model, when &#x003c4;<italic><sub>A</sub></italic>&#x02009;=&#x02009;&#x003c4;<italic><sub>y</sub></italic> and &#x003c4;<sub>+</sub>&#x02009;=&#x02009;&#x003c4;<italic><sub>m</sub></italic>/2. Indeed, the triplet model would yield the following weight change:</p><disp-formula id="E30"><label>(30)</label><mml:math id="M74"><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mtext>triplet</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02243;</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mn>3</mml:mn><mml:mo>+</mml:mo></mml:msubsup><mml:msub><mml:mo>&#x003b5;</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mo>&#x003c4;</mml:mo><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><p>From this we conclude that the triplet effect, which primarily arose from phenomenological minimal modeling of experimental data, also emerges from an optimal learning rule when the postsynaptic neuron adapts. To understand in more intuitive terms how the triplet mechanism relates to optimal information transmission, let us consider the case where the postsynaptic neuron is fully deterministic. If so, the noise entropy is null, so that maximizing information transfer means producing output spike trains with maximum entropy. If the mean firing rate &#x003c1;<sub>targ</sub> is a further constraint, output spike trains should be Poisson processes, which as a by-product would produce exponentially distributed ISIs. If the neuron is endowed with refractory and adapting mechanisms, there is a natural tendency for short ISIs to appear rarely. Therefore, plasticity has to fight against adaptation and refractoriness to bind more and more stimulus features to short ISIs. The triplet effect is precisely what is needed to achieve this: if a presynaptic spike is found to be responsible for a short ISI, it should be reinforced more than if the ISI was longer. This issue is further developed in Section <xref ref-type="sec" rid="s3">&#x0201c;Discussion.&#x0201d;</xref></p></sec><sec><title>Optimal STDP is target-cell specific</title><p>The results of the previous sections suggest that STDP may optimally interact with adaptation to enhance the channel capacity. In principle, if STDP is optimized for information transmission, it cannot ignore the intrinsic dynamics of the postsynaptic cell which influences the mapping between input and output spikes. The cortex is known to exhibit a rich diversity of cell types, with the corresponding range of intrinsic dynamics, and in parallel, STDP is target-cell specific (Tzounopoulos et al., <xref ref-type="bibr" rid="B48">2004</xref>; Lu et al., <xref ref-type="bibr" rid="B26">2007</xref>). Within the optimality framework, we should therefore be able to predict this target-cell specificity of STDP by investigating the predictions of the optimal model in the context of <italic>in vitro</italic> pairing experiments. Predictions should be made for different types of postsynaptic neurons, and be compared to experimental data. The optimal learning rule was shown in Toyoizumi et al. (<xref ref-type="bibr" rid="B47">2007</xref>) to share some features with STDP. We here extend this work to a couple of additional features including the frequency dependence. We also apply it to another type of postsynaptic cell, an inhibitory FS interneuron, for which <italic>in vitro</italic> data exist.</p><p>Only one synapse is investigated, with unit weight <italic>w</italic><sub>0</sub>&#x02009;=&#x02009;1&#x02009;mV before the start of the experiment. Sixty pre&#x02013;post pairs with given interspike time &#x00394;<italic>s</italic> are repeated in time with frequency <italic>f</italic>. The subsequent weight change given by Eq. <xref ref-type="disp-formula" rid="E12">12</xref> is reported as a function of both parameters (Figures <xref ref-type="fig" rid="F8">8</xref>A,B).</p><fig id="F8" position="float"><label>Figure 8</label><caption><p><bold>Optimal plasticity shares features with target-cell specific STDP</bold>. <bold>(A)</bold> The optimal model applied on 60 pre&#x02013;post pairs repeating at 1 (black line), 20 (red thick), and 50 Hz (green) yields STDP learning windows that qualitatively match those recorded in Sj&#x000f6;str&#x000f6;m et al. (<xref ref-type="bibr" rid="B41">2001</xref>). For comparison, the <italic>in vitro</italic> data has been redrawn with permission. <bold>(B)</bold> LTP dominates when the pairing frequency is increased. The optimal frequency window is plotted for post-before-pre (&#x02212;10&#x02009;ms, solid green curve) and pre-before-post pairs (+10&#x02009;ms, solid blue) repeated with frequency <italic>f</italic> (<italic>x</italic>-axis). Points and error bars are the experimental data, redrawn from Sj&#x000f6;str&#x000f6;m et al. (<xref ref-type="bibr" rid="B41">2001</xref>) with permission. <bold>(C)</bold> Learning window that minimizes information transmission at an excitatory synapse onto a fast-spiking (FS) inhibitory interneuron. The procedure is the same as in <bold>(A)</bold>. The spike-triggered adaptation kernel was updated to better match that of a FS cell (see <bold>D</bold>). Dots are redrawn from Lu et al. (<xref ref-type="bibr" rid="B26">2007</xref>). <bold>(D)</bold> Left: after-spike kernels of firing rate suppression for the principal excitatory cell (red, same as the one we used throughout the article, see <xref ref-type="sec" rid="s2">Materials and Methods</xref>) and the fast-spiking interneuron (blue). The latter was modeled by adding a third variable <italic>q<sub>B</sub></italic>&#x02009;&#x0003c;&#x02009;0 with time constant &#x003c4;<italic><sub>B</sub></italic>&#x02009;=&#x02009;30&#x02009;ms to the initial kernel. Solid blue line: <italic>q<sub>B</sub></italic>&#x02009;=&#x02009;&#x02212;9. Dashed blue line: <italic>q<sub>B</sub></italic>&#x02009;=&#x02009;&#x02212;8. Right: schematic of a feed-forward inhibition microcircuit. A first principal cell (PC) makes an excitatory connection to another PC. It also inhibits it indirectly through a FS interneuron. The example spike trains illustrate the benefit of having LTD for pre-before-post pairing at the PC&#x02013;FS synapse (see text).</p></caption><graphic xlink:href="fncom-04-00143-g008"/></fig><p>The optimal model features asymmetric timing windows at 1, 20, and 50&#x02009;Hz pairing frequencies (Figure <xref ref-type="fig" rid="F8">8</xref>A). At 1 and 20 Hz, pre-before-post yields LTP and post-before-pre leads to LTD. At 50&#x02009;Hz the whole curve is shifted upwards, resulting in LTP on both sides. The model qualitatively agrees with the experimental data reported in Sj&#x000f6;str&#x000f6;m et al. (<xref ref-type="bibr" rid="B41">2001</xref>), redrawn for comparison (Figure <xref ref-type="fig" rid="F8">8</xref>A, circles).</p><p>The frequency dependence experimentally found in Markram et al. (<xref ref-type="bibr" rid="B29">1997</xref>) and Sj&#x000f6;str&#x000f6;m et al. (<xref ref-type="bibr" rid="B41">2001</xref>) is also qualitatively reproduced (Figure <xref ref-type="fig" rid="F8">8</xref>B). Post&#x02013;pre pairing (&#x00394;<italic>s</italic>&#x02009;=&#x02009;&#x02212;10&#x02009;ms, green curve) switches from LTD at low frequency to LTP at higher frequencies, which is consistent with the timing windows in Figure <xref ref-type="fig" rid="F8">8</xref>A. For pre&#x02013;post pairing (&#x00394;<italic>s</italic>&#x02009;=&#x02009;+10&#x02009;ms, blue curve), LTP also increases with the pairing frequency. We also found that when SFA was removed, it was impossible to have a good fit for both the time window and the frequency dependence (not shown).</p><p>To further elucidate the link between optimal STDP and the after-spike kernel (<italic>g<sub>R</sub></italic>&#x02009;+&#x02009;<italic>g<sub>A</sub></italic> in Eq. <xref ref-type="disp-formula" rid="E5">5</xref>), we ask whether plasticity at excitatory synapses onto FS interneurons can be accounted for in the same principled manner. In general, the intrinsic dynamics of inhibitory interneurons are very different from that of principal cells in cortex. STDP at synapses onto those cells is also different from STDP at excitatory-to-excitatory synapses (Tzounopoulos et al., <xref ref-type="bibr" rid="B48">2004</xref>; Lu et al., <xref ref-type="bibr" rid="B26">2007</xref>). The dynamics of FS cells are well modeled using a kernel which is shown in Figure <xref ref-type="fig" rid="F8">8</xref>D (Mensi et al., <xref ref-type="bibr" rid="B28">2010</xref>). We augment the after-spike kernel with an additional variable <italic>g<sub>B</sub></italic> governed by</p><disp-formula id="E31"><label>(31)</label><mml:math id="M75"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mo>&#x003c4;</mml:mo><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mi>Y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><p>Parameters were set to &#x003c4;<italic><sub>B</sub></italic>&#x02009;=&#x02009;30&#x02009;ms, &#x003c4;<italic><sub>A</sub></italic>&#x02009;=&#x02009;150&#x02009;ms, <italic>q<sub>B</sub></italic>&#x02009;=&#x02009;&#x02212;9, and <italic>q<sub>A</sub></italic>&#x02009;=&#x02009;4. The resulting kernel (i.e., <italic>g<sub>R</sub></italic>&#x02009;+&#x02009;<italic>g<sub>A</sub></italic>&#x02009;+&#x02009;<italic>g<sub>B</sub></italic> &#x02013; Figure <xref ref-type="fig" rid="F8">8</xref>D, blue kernel) exhibits after-spike refractoriness followed by a short facilitating period before adaptation takes over (note that the kernel is suppressive, meaning that positive values correspond to suppression of activity while negative values mean facilitation). Since interneurons do not project over long distances to other areas, the infomax objective function might not appear as well justified. Instead, let us consider the simple microcircuit shown in Figure <xref ref-type="fig" rid="F8">8</xref>D. A first principal cell (PC) makes an excitatory synapse onto a second PC, and we assume the infomax principle is at work. The first PC inhibits the second PC via a FS interneuron. How, intuitively, should the PC-to-FS synapse change so that the FS cell also contributes to the overall information maximization between the two PCs? In a very crude understanding of the infomax principle, if a pre-before-post pair of spikes is evoked at the PC&#x02013;PC synapse (see spike trains in Figure <xref ref-type="fig" rid="F8">8</xref>D), the probability of having this pair again should be increased. If a similar pre-before-post pair is simultaneously evoked at the PC&#x02013;FS synapse, then decreasing its weight will make it less likely that the FS spike again after the first PC. This in turn makes it more likely that the first PC&#x02013;PC pair of spike will occur again. Therefore, PC&#x02013;FS synapses should undergo some sort of anti-Hebbian learning. In fact, we found information minimization (i.e., the optimal model with opposite learning rate) to yield a good match between the simulated STDP time window (Figure <xref ref-type="fig" rid="F8">8</xref>C) and that found in Lu et al. (<xref ref-type="bibr" rid="B26">2007</xref>), which also exhibits LTD on both sides with some LTP at large intervals (see orange dots, superimposed). The post-before-pre part of the window can be understood intuitively: when a presynaptic spike arrives a few milliseconds after a postsynaptic spike, it falls in the period where postsynaptic firing is facilitated (<italic>q<sub>B</sub></italic>&#x02009;&#x0003c;&#x02009;0). Therefore, it still has some influence on the subsequent postsynaptic activity. In order to avoid later causal pre&#x02013;post events, the weight should be decreased. We see that the optimal STDP window depends on the after-spike kernel that describes the dynamical properties of the postsynaptic cell: <italic>q<sub>B</sub></italic> directly modulates the post&#x02013;pre part of the window (see dashed curve in Figure <xref ref-type="fig" rid="F8">8</xref>C).</p><p>Together, these results suggest that if STDP is considered as arising from an optimality principle, it naturally interacts with the dynamics of the postsynaptic cell. This might underlie the target-cell specificity of STDP (Tzounopoulos et al., <xref ref-type="bibr" rid="B48">2004</xref>; Lu et al., <xref ref-type="bibr" rid="B26">2007</xref>).</p></sec></sec><sec sec-type="discussion" id="s3"><title>Discussion</title><p>Experiments (Markram et al., <xref ref-type="bibr" rid="B29">1997</xref>; Sj&#x000f6;str&#x000f6;m et al., <xref ref-type="bibr" rid="B41">2001</xref>; Froemke et al., <xref ref-type="bibr" rid="B14">2006</xref>) as well as phenomenological models of STDP (Senn et al., <xref ref-type="bibr" rid="B39">2001</xref>; Froemke et al., <xref ref-type="bibr" rid="B14">2006</xref>; Pfister and Gerstner, <xref ref-type="bibr" rid="B35">2006</xref>; Clopath et al., <xref ref-type="bibr" rid="B10">2010</xref>) point to the fact that LTP is not accurately described by independent contributions from neighboring postsynaptic spikes. In order to reproduce the results of recent STDP experiments, at least two postsynaptic spikes must interact in the LTP process. We have shown that this key feature (&#x0201c;triplet effect&#x0201d; in Pfister and Gerstner, <xref ref-type="bibr" rid="B35">2006</xref>; Clopath et al., <xref ref-type="bibr" rid="B10">2010</xref>; and similarly in Senn et al., <xref ref-type="bibr" rid="B39">2001</xref>) happens to be optimal for an adapting neuron to learn to maximize information transmission. We have compared the performance of an optimal model (Toyoizumi et al., <xref ref-type="bibr" rid="B46">2005</xref>) to that of two minimal STDP models. One of them incorporated the triplet effect (Pfister and Gerstner, <xref ref-type="bibr" rid="B35">2006</xref>), while the second one did not (standard pair-based learning rule; Gerstner et al., <xref ref-type="bibr" rid="B15">1996</xref>; Kempter et al., <xref ref-type="bibr" rid="B20">1999</xref>; Song et al., <xref ref-type="bibr" rid="B43">2000</xref>). The triplet-based model performs very close to the optimal one, and this advantage over pair-STDP disappears when SFA is removed from the intrinsic dynamics of the postsynaptic cell.</p><p>Our results are not restricted to additive STDP in which the amount of weight change is independent of the weight itself. It also holds when the amount of LTD increases with the efficacy of the synapse, a form which better reflects experimental observations (Bi and Poo, <xref ref-type="bibr" rid="B4">1998</xref>; Sj&#x000f6;str&#x000f6;m et al., <xref ref-type="bibr" rid="B41">2001</xref>). In the model introduced here, the amount of LTD is modulated by a sub-linear function of the synaptic weight. The deviation from linearity is set by a single parameter <italic>a</italic>&#x02009;&#x0003e;&#x02009;0, with the purely multiplicative dependence of van Rossum et al. (<xref ref-type="bibr" rid="B49">2000</xref>) being recovered when <italic>a</italic>&#x02009;=&#x02009;0. Since we modeled only a fraction of the total input synapses, we assumed a certain level of noise in the postsynaptic cell to account for the activity of the remaining synapses, thereby staying consistent with the framework of information theory in which communication channels are generally considered noisy. Because of this noise level, we found a large <italic>a</italic> was required for the weight distribution to become positively skewed as reported by Sj&#x000f6;str&#x000f6;m et al. (<xref ref-type="bibr" rid="B41">2001</xref>) (cortex layer V). For both the pair and triplet learning rules, the noisier the postsynaptic neuron, the weaker the LTD weight-dependence (i.e., the larger <italic>a</italic>) must be to keep a significant spread of the weight distribution. This means that other (possibly simpler) forms of weight dependence for LTD would work equally well, provided the noise level is adjusted accordingly. For example, in a nearly deterministic neuron, input&#x02013;output correlations are strong enough for the weight-distribution to spread even when LTD depends linearly on the synaptic weight (<italic>a</italic>&#x02009;=&#x02009;0, not shown).</p><p>In the original papers where the optimal and triplet rule were first described, it was pointed out that both rules could be mapped onto the BCM learning rule (Bienenstock et al., <xref ref-type="bibr" rid="B5">1982</xref>). Both learning rules are quadratic in the postsynaptic activity. In turn, the link between the BCM rule and ICA has also already been researched (Intrator and Cooper, <xref ref-type="bibr" rid="B18">1992</xref>; Blais et al., <xref ref-type="bibr" rid="B7">1998</xref>; Clopath et al., <xref ref-type="bibr" rid="B10">2010</xref>), as has the relationship between the infomax principle and ICA (Bell and Sejnowski, <xref ref-type="bibr" rid="B2">1995</xref>). It therefore does not come as a surprise that the triplet model performs close to the infomax optimal learning rule. What is novel is the link to adaptation and spike after-potential.</p><p>We have also shown that when the optimal or triplet plasticity models are at work, the postsynaptic neuron learns to transmit information in a wider frequency band (Figure <xref ref-type="fig" rid="F6">6</xref>D): both rules evoke postsynaptic responses that have substantial power below 5 Hz, in contrast to the pair-based STDP rule. This is intuitively understood from the triplet effect combined with adaptation. Let us imagine STDP starts creating a peak in the PSTH so that we have, with high probability, a first postsynaptic spike at time <italic>t</italic><sub>0</sub>. If a presynaptic spike at time <italic>t</italic><sub>0</sub>&#x02009;+ (&#x00394;/2) is followed by a further postsynaptic spike at time <italic>t</italic><sub>0</sub>&#x02009;+&#x02009;&#x00394; (&#x00394; on the order of 10&#x02009;ms), the triplet effect reinforces the connection from this presynaptic unit. In turn, it will create another peak at time <italic>t</italic><sub>0</sub>&#x02009;+&#x02009;&#x00394;, and this process can continue. Peaks thus extend and become broader, until adaptation becomes strong enough to prevent further immediate firing. The next series of peaks will then be delayed by a few hundred milliseconds. Broadening of peak widths and ISIs together introduce more power at lower frequencies in the PSTH.</p><p>One should bear in mind that neurons process incoming signals in order to convey them to other receivers. Although the information content of the output spike train really is an important quantity with respect to information processing, the way it can be decoded by downstream neurons should also be taken into account. Some &#x0201c;words&#x0201d; in the output spike train may be more suited for subsequent transmission than others. It has been suggested (Lisman, <xref ref-type="bibr" rid="B25">1997</xref>) that since cortical synapses are intrinsically unreliable, isolated incoming spikes cannot be received properly, whereas bursts of action potentials evoke a reliable response in the receiving neuron. There is a lot of evidence for burst firing in many sensory systems (see Krahe and Gabbiani, <xref ref-type="bibr" rid="B22">2004</xref> for a review). As shown in Figure <xref ref-type="fig" rid="F6">6</xref>, the optimal and triplet STDP models tend to sparsify the distribution of ISIs, meaning that the neuron learns to respond vigorously (very short ISIs) to a larger number of features in the input stream, while remaining silent for longer portions of the stimulus. The neuron thus overcomes the effects of adaptation, which in baseline conditions (before learning) gives the ISI distribution a broad peak and a Gaussian-like drop-off. Our results therefore suggest that reliable occurrence of short ISIs can arise from STDP in adaptive neurons that are not intrinsic bursters. This is in line with Eyherabide et al. (<xref ref-type="bibr" rid="B12">2008</xref>), which recently provided evidence for high information transmission through burst activity in an insect auditory system (<italic>Locusta migratoria</italic>). The recorded neurons encoded almost half of the total transmitted information in bursts, and this was also shown not to require intrinsic burst dynamics.</p><p>Since our results rely on the outcome of a couple of numerical experiments, one might be concerned about the validity of the findings outside the range of parameter values we have used. There are for example a couple of free parameters in the neuron model. It is obviously difficult to browse the full high-dimensional parameter space and search for regions where the results would break down. We therefore tried to constrain our neuron parameters in a sensible manner. For example, the parameters of the SFA mechanism (<italic>q<sub>A</sub></italic> and &#x003c4;<italic><sub>A</sub></italic>) were chosen such that the response properties to a step in input firing rate would look plausible (Figure <xref ref-type="fig" rid="F1">1</xref>C). The noise parameter <italic>r</italic><sub>0</sub> and the threshold value <italic>u<sub>T</sub></italic> were chosen so as to achieve an output rate of 7.5 Hz when all synaptic weights are at 1&#x02009;mV. We acknowledge, though, that <italic>r</italic><sub>0</sub> could be made arbitrarily large (reducing the amount of noise) since <italic>u<sub>T</sub></italic> can compensate for it. In the limit of very low noise, information transmission cannot be improved by increasing the neuron's reliability anymore, since the noise entropy would already be minimal. We have shown however that a substantial part of the information gain found in the optimal and triplet models are due to an increased response entropy. This qualitative similarity, together with the structural similarities highlighted in Figures <xref ref-type="fig" rid="F7">7</xref> and <xref ref-type="fig" rid="F8">8</xref>, lead us to believe that our results would still hold in the deterministic limit, and for noise levels in between. The optimal plasticity rule becoming ill-defined in this limit, we did not investigate this further.</p><p>To what extent can we extrapolate our results to the optimality of synaptic plasticity in the real brain? It obviously depends on the amount of trust one can put into this triplet model. Phenomenological models of STDP are usually constructed based on the results of <italic>in vitro</italic> experiments. They end up reproducing the quantitative outcome of only a few pre&#x02013;post pairing schemes which are far from spanning the full complexity of real spike trains. To what extent can these models be trusted in more natural situations? From a machine learning perspective, a minimal model is likely to generalize better than a more detailed model, because its small number of free parameters might prevent it from overfitting the experimental data at the expense of its interpolation/extrapolation power. In this study, we have put the emphasis on an extrapolation of recent minimal models (Pfister and Gerstner, <xref ref-type="bibr" rid="B35">2006</xref>; Clopath et al., <xref ref-type="bibr" rid="B10">2010</xref>): the amount of LTP obtained from a pre-before-post pair increases with the recent postsynaptic firing frequency. By construction, the models account for the frequency dependence of the classical pairing experiment (they are fitted on this, among other things). However, they are seriously challenged by a more detailed study of spike interactions at L2/3 pyramidal cells (Froemke et al., <xref ref-type="bibr" rid="B14">2006</xref>). There, it was explicitly shown that (<italic>n</italic>-posts)&#x02013;pre&#x02013;post bursts yield an amount of LTD which grows with <italic>n</italic>, the number of postsynaptic spikes in the burst preceding the pair. In contrast, post&#x02013;pre&#x02013;post triplets in hippocampal slices lead to LTP in a way that is consistent with the triplet model (Wang et al., <xref ref-type="bibr" rid="B50">2005</xref>). The results of our study should therefore be interpreted bearing in mind the variability in experimental results. The recurrent <italic>in vitro</italic> versus <italic>in vivo</italic> debate should also be considered: synaptic plasticity depends on a lot of biochemical parameters for which the slice conditions do not faithfully reflect the normal operating mode of the brain.</p><p>A second controversy lies in our optimality model itself. While efficient coding of presynaptic spike trains may seem a reasonable goal to achieve at, say, thalamocortical synapses in sensory cortices, many other objectives could well be considered when it comes to other brain areas. Some examples are optimal decision making through risk balancing, reinforcement learning via reward maximization, or optimal memory storage and recall in autoassociative memories. It will be interesting to see more STDP learning rules in functionally different areas and how these relate to optimality principles.</p><p>Finally, while we investigated information transmission through a single postsynaptic cell, it remains to be elucidated how local information maximization in large recurrent networks of spiking neurons translates into a better information flow through the network.</p></sec><sec><title>Conflict of Interest Statement</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec></body><back><ack><p>This work was partially supported by the European Union Framework 7 ICT Project 215910 (BIOTACT, <uri xlink:type="simple" xlink:href="http://www.biotact.org">www.biotact.org</uri>). Jean-Pascal Pfister was supported by the Wellcome Trust. We thank Dr. Eilif Muller for having motivated the use of graphics cards (GPU) to compute the MI.</p></ack><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bell</surname><given-names>A. J.</given-names></name><name><surname>Parra</surname><given-names>L. C.</given-names></name></person-group> (<year>2005</year>). <article-title>&#x0201c;Maximising sensitivity in a spiking network,&#x0201d;</article-title> in <source>Advances in Neural Information Processing Systems</source>, Vol. <volume>17</volume>, eds <person-group person-group-type="editor"><name><surname>Saul</surname><given-names>L. K.</given-names></name><name><surname>Weiss</surname><given-names>Y.</given-names></name><name><surname>Bottou</surname><given-names>L.</given-names></name></person-group> (<publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>), <fpage>121</fpage>&#x02013;<lpage>128</lpage></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bell</surname><given-names>A. J.</given-names></name><name><surname>Sejnowski</surname><given-names>T. J.</given-names></name></person-group> (<year>1995</year>). <article-title>An information maximization approach to blind separation and blind deconvolution</article-title>. <source>Neural Comput.</source><volume>7</volume>, <fpage>1129</fpage>&#x02013;<lpage>1159</lpage><pub-id pub-id-type="doi">10.1162/neco.1995.7.6.1129</pub-id><pub-id pub-id-type="pmid">7584893</pub-id></mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bell</surname><given-names>C. C.</given-names></name><name><surname>V</surname><given-names>H.</given-names></name><name><surname>Sugawara</surname><given-names>Y.</given-names></name><name><surname>Grant</surname><given-names>K.</given-names></name></person-group> (<year>1997</year>). <article-title>Synaptic plasticity in a cerebellum-like structure depends on temporal order</article-title>. <source>Nature</source><volume>387</volume>, <fpage>278</fpage>&#x02013;<lpage>281</lpage><pub-id pub-id-type="doi">10.1038/387278a0</pub-id><pub-id pub-id-type="pmid">9153391</pub-id></mixed-citation></ref><ref id="B4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bi</surname><given-names>G. Q.</given-names></name><name><surname>Poo</surname><given-names>M. M.</given-names></name></person-group> (<year>1998</year>). <article-title>Synaptic modifications in cultured hippocampal neurons: dependence on spike timing, synaptic strength, and postsynaptic cell type</article-title>. <source>J. Neurosci.</source><volume>18</volume>, <fpage>10464</fpage>&#x02013;<lpage>10472</lpage><pub-id pub-id-type="pmid">9852584</pub-id></mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bienenstock</surname><given-names>E. L.</given-names></name><name><surname>Cooper</surname><given-names>L. N.</given-names></name><name><surname>Munro</surname><given-names>P. W.</given-names></name></person-group> (<year>1982</year>). <article-title>Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex</article-title>. <source>J. Neurosci.</source><volume>2</volume>, <fpage>32</fpage>&#x02013;<lpage>48</lpage><pub-id pub-id-type="pmid">7054394</pub-id></mixed-citation></ref><ref id="B6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Billings</surname><given-names>G.</given-names></name><name><surname>van Rossum</surname><given-names>M. C. W.</given-names></name></person-group> (<year>2009</year>). <article-title>Memory retention and spike-timing dependent plasticity</article-title>. <source>J. Neurophysiol.</source><volume>101</volume>, <fpage>2775</fpage>&#x02013;<lpage>2788</lpage><pub-id pub-id-type="doi">10.1152/jn.91007.2008</pub-id><pub-id pub-id-type="pmid">19297513</pub-id></mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blais</surname><given-names>B. S.</given-names></name><name><surname>Intrator</surname><given-names>N.</given-names></name><name><surname>Shouval</surname><given-names>H.</given-names></name><name><surname>Cooper</surname><given-names>L. N.</given-names></name></person-group> (<year>1998</year>). <article-title>Receptive field formation in natural scene environments: comparison of single-cell learning rules</article-title>. <source>Neural Comput.</source><volume>10</volume>, <fpage>1797</fpage>&#x02013;<lpage>1813</lpage><pub-id pub-id-type="doi">10.1162/089976698300017142</pub-id><pub-id pub-id-type="pmid">9744898</pub-id></mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bohte</surname><given-names>S. M.</given-names></name><name><surname>Mozer</surname><given-names>M. C.</given-names></name></person-group> (<year>2007</year>). <article-title>Reducing the variability of neural responses: a computational theory of spike-timing-dependent plasticity</article-title>. <source>Neural Comput.</source><volume>19</volume>, <fpage>371</fpage>&#x02013;<lpage>403</lpage><pub-id pub-id-type="doi">10.1162/neco.2007.19.2.371</pub-id><pub-id pub-id-type="pmid">17206869</pub-id></mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chechik</surname><given-names>G.</given-names></name></person-group> (<year>2003</year>). <article-title>Spike timing-dependent plasticity and relevant mutual information maximization</article-title>. <source>Neural Comput.</source><volume>15</volume>, <fpage>1481</fpage>&#x02013;<lpage>1510</lpage><pub-id pub-id-type="doi">10.1162/089976603321891774</pub-id><pub-id pub-id-type="pmid">12816563</pub-id></mixed-citation></ref><ref id="B10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clopath</surname><given-names>C.</given-names></name><name><surname>B&#x000fc;sing</surname><given-names>L.</given-names></name><name><surname>Vasilaki</surname><given-names>E.</given-names></name><name><surname>Gerstner</surname><given-names>W.</given-names></name></person-group> (<year>2010</year>). <article-title>Connectivity reflects coding: a model of voltage-based STDP with homeostasis</article-title>. <source>Nat. Neurosci.</source><volume>13</volume>, <fpage>344</fpage>&#x02013;<lpage>352</lpage><pub-id pub-id-type="doi">10.1038/nn.2479</pub-id><pub-id pub-id-type="pmid">20098420</pub-id></mixed-citation></ref><ref id="B11"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Clopath</surname><given-names>C.</given-names></name><name><surname>Longtin</surname><given-names>A.</given-names></name><name><surname>Gerstner</surname><given-names>W.</given-names></name></person-group> (<year>2008</year>). <article-title>&#x0201c;An online Hebbian learning rule that performs independent component analysis,&#x0201d;</article-title> in <source>Advances in Neural Information Processing Systems</source>, Vol. <volume>20</volume>, eds <person-group person-group-type="editor"><name><surname>Platt</surname><given-names>J.</given-names></name><name><surname>Koller</surname><given-names>D.</given-names></name><name><surname>Singer</surname><given-names>Y.</given-names></name><name><surname>Roweis</surname><given-names>S.</given-names></name></person-group> (<publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>), <fpage>321</fpage>&#x02013;<lpage>328</lpage></mixed-citation></ref><ref id="B12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eyherabide</surname><given-names>H. G.</given-names></name><name><surname>Rokem</surname><given-names>A.</given-names></name><name><surname>Herz</surname><given-names>A. V. M.</given-names></name><name><surname>I</surname><given-names>S.</given-names></name></person-group> (<year>2008</year>). <article-title>Burst firing is a neural code in an insect auditory system</article-title>. <source>Front. Comput. Neurosci.</source><volume>2</volume>, <fpage>1</fpage>&#x02013;<lpage>17</lpage><pub-id pub-id-type="doi">10.3389/neuro.10.003.2008</pub-id><pub-id pub-id-type="pmid">18946531</pub-id></mixed-citation></ref><ref id="B13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Florian</surname><given-names>R. V.</given-names></name></person-group> (<year>2007</year>). <article-title>Reinforcement learning through modulation of spike timing-dependent synaptic plasticity</article-title>. <source>Neural Comput.</source><volume>19</volume>, <fpage>1468</fpage>&#x02013;<lpage>1502</lpage><pub-id pub-id-type="doi">10.1162/neco.2007.19.6.1468</pub-id><pub-id pub-id-type="pmid">17444757</pub-id></mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Froemke</surname><given-names>R. C.</given-names></name><name><surname>Tsay</surname><given-names>I.</given-names></name><name><surname>Raad</surname><given-names>M.</given-names></name><name><surname>Long</surname><given-names>J.</given-names></name><name><surname>Dan</surname><given-names>Y.</given-names></name></person-group> (<year>2006</year>). <article-title>Contribution of individual spikes in burst-induced long-term synaptic modification</article-title>. <source>J. Neurophysiol.</source><volume>95</volume>, <fpage>1620</fpage>&#x02013;<lpage>1629</lpage><pub-id pub-id-type="doi">10.1152/jn.00910.2005</pub-id><pub-id pub-id-type="pmid">16319206</pub-id></mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gerstner</surname><given-names>W.</given-names></name><name><surname>Kempter</surname><given-names>R.</given-names></name><name><surname>van Hemmen</surname><given-names>J.</given-names></name><name><surname>Wagner</surname><given-names>H.</given-names></name></person-group> (<year>1996</year>). <article-title>A neuronal learning rule for sub-millisecond temporal coding</article-title>. <source>Nature</source><volume>383</volume>, <fpage>76</fpage>&#x02013;<lpage>78</lpage><pub-id pub-id-type="doi">10.1038/383076a0</pub-id><pub-id pub-id-type="pmid">8779718</pub-id></mixed-citation></ref><ref id="B16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gerstner</surname><given-names>W.</given-names></name><name><surname>Kistler</surname><given-names>W. K.</given-names></name></person-group> (<year>2002a</year>). <article-title>Mathematical formulations of Hebbian learning</article-title>. <source>Biol. Cybern.</source><volume>87</volume>, <fpage>404</fpage>&#x02013;<lpage>415</lpage><pub-id pub-id-type="doi">10.1007/s00422-002-0353-y</pub-id><pub-id pub-id-type="pmid">12461630</pub-id></mixed-citation></ref><ref id="B17"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gerstner</surname><given-names>W.</given-names></name><name><surname>Kistler</surname><given-names>W.</given-names></name></person-group> (<year>2002b</year>). <source>Spiking Neuron Models</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name></mixed-citation></ref><ref id="B18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Intrator</surname><given-names>N.</given-names></name><name><surname>Cooper</surname><given-names>L.</given-names></name></person-group> (<year>1992</year>). <article-title>Objective function formulation of the BCM theory of visual cortical plasticity &#x02013; statistical connections, stability conditions</article-title>. <source>Neural Netw.</source><volume>5</volume>, <fpage>3</fpage>&#x02013;<lpage>17</lpage><pub-id pub-id-type="doi">10.1016/S0893-6080(05)80003-6</pub-id></mixed-citation></ref><ref id="B19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Izhikevich</surname><given-names>E.</given-names></name><name><surname>Edelman</surname><given-names>G. M.</given-names></name></person-group> (<year>2008</year>). <article-title>Large-scale model of mammalian thalamocortical systems</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source><volume>105</volume>, <fpage>3593</fpage>&#x02013;<lpage>3598</lpage><pub-id pub-id-type="doi">10.1073/pnas.0712231105</pub-id><pub-id pub-id-type="pmid">18292226</pub-id></mixed-citation></ref><ref id="B20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kempter</surname><given-names>R.</given-names></name><name><surname>Gerstner</surname><given-names>W.</given-names></name><name><surname>van Hemmen</surname><given-names>J. L.</given-names></name></person-group> (<year>1999</year>). <article-title>Hebbian learning and spiking neurons</article-title>. <source>Phys. Rev. E</source><volume>59</volume>, <fpage>4498</fpage>&#x02013;<lpage>4514</lpage><pub-id pub-id-type="doi">10.1103/PhysRevE.59.4498</pub-id></mixed-citation></ref><ref id="B21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klampfl</surname><given-names>S.</given-names></name><name><surname>Legenstein</surname><given-names>R.</given-names></name><name><surname>Maass</surname><given-names>W.</given-names></name></person-group> (<year>2009</year>). <article-title>Spiking neurons can learn to solve information bottleneck problems and to extract independent components</article-title>. <source>Neural Comput.</source><volume>21</volume>, <fpage>911</fpage>&#x02013;<lpage>959</lpage><pub-id pub-id-type="doi">10.1162/neco.2008.01-07-432</pub-id><pub-id pub-id-type="pmid">19018708</pub-id></mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krahe</surname><given-names>R.</given-names></name><name><surname>Gabbiani</surname><given-names>F.</given-names></name></person-group> (<year>2004</year>). <article-title>Burst firing in sensory systems</article-title>. <source>Nat. Rev. Neurosci.</source><volume>5</volume>, <fpage>13</fpage>&#x02013;<lpage>23</lpage><pub-id pub-id-type="doi">10.1038/nrn1296</pub-id><pub-id pub-id-type="pmid">14661065</pub-id></mixed-citation></ref><ref id="B23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lengyel</surname><given-names>M.</given-names></name><name><surname>Kwag</surname><given-names>J.</given-names></name><name><surname>Paulsen</surname><given-names>O.</given-names></name><name><surname>Dayan</surname><given-names>P.</given-names></name></person-group> (<year>2005</year>). <article-title>Matching storage and recall: hippocampal spike timing-dependent plasticity and phase response curves</article-title>. <source>Nat. Neurosci.</source><volume>8</volume>, <fpage>1677</fpage>&#x02013;<lpage>1683</lpage><pub-id pub-id-type="doi">10.1038/nn1561</pub-id><pub-id pub-id-type="pmid">16261136</pub-id></mixed-citation></ref><ref id="B24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Linsker</surname><given-names>R.</given-names></name></person-group> (<year>1989</year>). <article-title>How to generate ordered maps by maximizing the mutual information between input and output signals</article-title>. <source>Neural Comput.</source><volume>1</volume>, <fpage>402</fpage>&#x02013;<lpage>411</lpage><pub-id pub-id-type="doi">10.1162/neco.1989.1.3.402</pub-id></mixed-citation></ref><ref id="B25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lisman</surname><given-names>J.</given-names></name></person-group> (<year>1997</year>). <article-title>Bursts as a unit of neural information: making unreliable synapses reliable</article-title>. <source>Trends Neurosci.</source><volume>20</volume>, <fpage>38</fpage>&#x02013;<lpage>43</lpage><pub-id pub-id-type="doi">10.1016/S0166-2236(96)10070-9</pub-id><pub-id pub-id-type="pmid">9004418</pub-id></mixed-citation></ref><ref id="B26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>J.</given-names></name><name><surname>Li</surname><given-names>C.</given-names></name><name><surname>Zhao</surname><given-names>J.-P.</given-names></name><name><surname>Poo</surname><given-names>M.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name></person-group> (<year>2007</year>). <article-title>Spike-timing-dependent plasticity of neocortical excitatory synapses on inhibitory interneurons depends on target cell type</article-title>. <source>J. Neurosci.</source><volume>27</volume>, <fpage>9711</fpage>&#x02013;<lpage>9720</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2513-07.2007</pub-id><pub-id pub-id-type="pmid">17804631</pub-id></mixed-citation></ref><ref id="B27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Magee</surname><given-names>J. C.</given-names></name><name><surname>Johnston</surname><given-names>D.</given-names></name></person-group> (<year>1997</year>). <article-title>A synaptically controlled associative signal for Hebbian plasticity in hippocampal neurons</article-title>. <source>Science</source><volume>275</volume>, <fpage>209</fpage>&#x02013;<lpage>213</lpage><pub-id pub-id-type="doi">10.1126/science.275.5297.209</pub-id><pub-id pub-id-type="pmid">8985013</pub-id></mixed-citation></ref><ref id="B28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mensi</surname><given-names>S.</given-names></name><name><surname>Naud</surname><given-names>R.</given-names></name><name><surname>Avermann</surname><given-names>M.</given-names></name><name><surname>Peterson</surname><given-names>C.</given-names></name><name><surname>Gerstner</surname><given-names>W.</given-names></name></person-group> (<year>2010</year>). <article-title>Complexity and performance in simple neuron models</article-title>. <source>Front. Neurosci</source>.<pub-id pub-id-type="doi">10.3389/conf. fnins.2010.03.00064</pub-id></mixed-citation></ref><ref id="B29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markram</surname><given-names>H.</given-names></name><name><surname>L&#x000fc;bke</surname><given-names>J.</given-names></name><name><surname>Frotscher</surname><given-names>M.</given-names></name><name><surname>Sakmann</surname><given-names>B.</given-names></name></person-group> (<year>1997</year>). <article-title>Regulation of synaptic efficacy by coincidence of postsynaptic AP and EPSPs</article-title>. <source>Science</source><volume>275</volume>, <fpage>213</fpage>&#x02013;<lpage>215</lpage><pub-id pub-id-type="doi">10.1126/science.275.5297.213</pub-id><pub-id pub-id-type="pmid">8985014</pub-id></mixed-citation></ref><ref id="B30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morrison</surname><given-names>A.</given-names></name><name><surname>Aertsen</surname><given-names>A.</given-names></name><name><surname>Diesmann</surname><given-names>M.</given-names></name></person-group> (<year>2007</year>). <article-title>Spike-timing dependent plasticity in balanced random networks</article-title>. <source>Neural Comput.</source><volume>19</volume>, <fpage>1437</fpage>&#x02013;<lpage>1467</lpage><pub-id pub-id-type="doi">10.1162/neco.2007.19.6.1437</pub-id><pub-id pub-id-type="pmid">17444756</pub-id></mixed-citation></ref><ref id="B31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morrison</surname><given-names>A.</given-names></name><name><surname>Diesmann</surname><given-names>M.</given-names></name><name><surname>Gerstner</surname><given-names>W.</given-names></name></person-group> (<year>2008</year>). <article-title>Phenomenological models of synaptic plasticity based on spike timing</article-title>. <source>Biol. Cybern.</source><volume>98</volume>, <fpage>459</fpage>&#x02013;<lpage>478</lpage><pub-id pub-id-type="doi">10.1007/s00422-008-0233-1</pub-id><pub-id pub-id-type="pmid">18491160</pub-id></mixed-citation></ref><ref id="B32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oja</surname><given-names>E.</given-names></name></person-group> (<year>1982</year>). <article-title>A simplified neuron model as a principal component analyzer</article-title>. <source>J. Math. Biol.</source><volume>15</volume>, <fpage>267</fpage>&#x02013;<lpage>273</lpage><pub-id pub-id-type="doi">10.1007/BF00275687</pub-id><pub-id pub-id-type="pmid">7153672</pub-id></mixed-citation></ref><ref id="B33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oja</surname><given-names>E.</given-names></name></person-group> (<year>1989</year>). <article-title>Neural networks, principal components, and subspaces</article-title>. <source>Int. J. Neural Syst.</source><volume>1</volume>, <fpage>61</fpage>&#x02013;<lpage>68</lpage><pub-id pub-id-type="doi">10.1142/S0129065789000475</pub-id></mixed-citation></ref><ref id="B34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname><given-names>B. A.</given-names></name><name><surname>Field</surname><given-names>D. J.</given-names></name></person-group> (<year>1996</year>). <article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title>. <source>Nature</source><volume>381</volume>, <fpage>607</fpage>&#x02013;<lpage>609</lpage><pub-id pub-id-type="doi">10.1038/381607a0</pub-id><pub-id pub-id-type="pmid">8637596</pub-id></mixed-citation></ref><ref id="B35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfister</surname><given-names>J.-P.</given-names></name><name><surname>Gerstner</surname><given-names>W.</given-names></name></person-group> (<year>2006</year>). <article-title>Triplets of spikes in a model of spike timing-dependent plasticity</article-title>. <source>J. Neurosci.</source><volume>26</volume>, <fpage>9673</fpage>&#x02013;<lpage>9682</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1425-06.2006</pub-id><pub-id pub-id-type="pmid">16988038</pub-id></mixed-citation></ref><ref id="B36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfister</surname><given-names>J.-P.</given-names></name><name><surname>Toyoizumi</surname><given-names>T.</given-names></name><name><surname>Barber</surname><given-names>D.</given-names></name><name><surname>Gerstner</surname><given-names>W.</given-names></name></person-group> (<year>2006</year>). <article-title>Optimal spike-timing dependent plasticity for precise action potential firing in supervised learning</article-title>. <source>Neural Comput.</source><volume>18</volume>, <fpage>1309</fpage>&#x02013;<lpage>1339</lpage><pub-id pub-id-type="doi">10.1162/neco.2006.18.6.1318</pub-id></mixed-citation></ref><ref id="B37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>R. P.</given-names></name><name><surname>Ballard</surname><given-names>D. H.</given-names></name></person-group> (<year>1999</year>). <article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title>. <source>Nat. Neurosci.</source><volume>2</volume>, <fpage>79</fpage>&#x02013;<lpage>87</lpage><pub-id pub-id-type="doi">10.1038/4580</pub-id><pub-id pub-id-type="pmid">10195184</pub-id></mixed-citation></ref><ref id="B38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubin</surname><given-names>J.</given-names></name><name><surname>Lee</surname><given-names>D. D.</given-names></name><name><surname>Sompolinsky</surname><given-names>H.</given-names></name></person-group> (<year>2001</year>). <article-title>Equilibrium properties of temporally asymmetric Hebbian plasticity</article-title>. <source>Phys. Rev. Lett.</source><volume>86</volume>, <fpage>364</fpage>&#x02013;<lpage>367</lpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.86.364</pub-id><pub-id pub-id-type="pmid">11177832</pub-id></mixed-citation></ref><ref id="B39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Senn</surname><given-names>W.</given-names></name><name><surname>Tsodyks</surname><given-names>M.</given-names></name><name><surname>Markram</surname><given-names>H.</given-names></name></person-group> (<year>2001</year>). <article-title>An algorithm for modifying neurotransmitter release probability based on pre- and postsynaptic spike timing</article-title>. <source>Neural Comput.</source><volume>13</volume>, <fpage>35</fpage>&#x02013;<lpage>67</lpage><pub-id pub-id-type="doi">10.1162/089976601300014628</pub-id><pub-id pub-id-type="pmid">11177427</pub-id></mixed-citation></ref><ref id="B40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seung</surname><given-names>H. S.</given-names></name></person-group> (<year>2003</year>). <article-title>Learning in spiking neural networks by reinforcement of stochastic synaptic transmission</article-title>. <source>Neuron</source><volume>40</volume>, <fpage>1063</fpage>&#x02013;<lpage>1073</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(03)00761-X</pub-id><pub-id pub-id-type="pmid">14687542</pub-id></mixed-citation></ref><ref id="B41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sj&#x000f6;str&#x000f6;m</surname><given-names>P.</given-names></name><name><surname>Turrigiano</surname><given-names>G.</given-names></name><name><surname>Nelson</surname><given-names>S.</given-names></name></person-group> (<year>2001</year>). <article-title>Rate, timing, and cooperativity jointly determine cortical synaptic plasticity</article-title>. <source>Neuron</source><volume>32</volume>, <fpage>1149</fpage>&#x02013;<lpage>1164</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(01)00542-6</pub-id><pub-id pub-id-type="pmid">11754844</pub-id></mixed-citation></ref><ref id="B42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>E. C.</given-names></name><name><surname>Lewicki</surname><given-names>M. S.</given-names></name></person-group> (<year>2006</year>). <article-title>Efficient auditory coding</article-title>. <source>Nature</source><volume>439</volume>, <fpage>978</fpage>&#x02013;<lpage>982</lpage><pub-id pub-id-type="doi">10.1038/nature04485</pub-id><pub-id pub-id-type="pmid">16495999</pub-id></mixed-citation></ref><ref id="B43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>S.</given-names></name><name><surname>Miller</surname><given-names>K. D.</given-names></name><name><surname>Abbott</surname><given-names>L. F.</given-names></name></person-group> (<year>2000</year>). <article-title>Competitive Hebbian learning through spike-time-dependent synaptic plasticity</article-title>. <source>Nat. Neurosci.</source><volume>3</volume>, <fpage>919</fpage>&#x02013;<lpage>926</lpage><pub-id pub-id-type="doi">10.1038/78829</pub-id><pub-id pub-id-type="pmid">10966623</pub-id></mixed-citation></ref><ref id="B44"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sprekeler</surname><given-names>H.</given-names></name><name><surname>Hennequin</surname><given-names>G.</given-names></name><name><surname>Gerstner</surname><given-names>W.</given-names></name></person-group> (<year>2009</year>). <article-title>&#x0201c;Code-specific policy gradient rules for spiking neurons,&#x0201d;</article-title> in <source>Advances in Neural Information Processing Systems</source>, Vol. <volume>22</volume>, eds <person-group person-group-type="editor"><name><surname>Bengio</surname><given-names>Y.</given-names></name><name><surname>Schuurmans</surname><given-names>D.</given-names></name><name><surname>Lafferty</surname><given-names>J.</given-names></name><name><surname>Williams</surname><given-names>C. K. I.</given-names></name><name><surname>Culotta</surname><given-names>A.</given-names></name></person-group> (<publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>), <fpage>1741</fpage>&#x02013;<lpage>1749</lpage></mixed-citation></ref><ref id="B45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sprekeler</surname><given-names>H.</given-names></name><name><surname>Michaelis</surname><given-names>C.</given-names></name><name><surname>Wiskott</surname><given-names>L.</given-names></name></person-group> (<year>2007</year>). <article-title>Slowness: an objective for spike-timing-dependent plasticity?</article-title><source>PLoS Comput. Biol.</source><volume>3</volume>, <fpage>e112</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.0030112</pub-id><pub-id pub-id-type="pmid">17604445</pub-id></mixed-citation></ref><ref id="B46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Toyoizumi</surname><given-names>T.</given-names></name><name><surname>Pfister</surname><given-names>J.-P.</given-names></name><name><surname>Aihara</surname><given-names>K.</given-names></name><name><surname>Gerstner</surname><given-names>W.</given-names></name></person-group> (<year>2005</year>). <article-title>Generalized Bienenstock&#x02013;Cooper&#x02013;Munro rule for spiking neurons that maximizes information transmission</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source><volume>102</volume>, <fpage>5239</fpage>&#x02013;<lpage>5244</lpage><pub-id pub-id-type="doi">10.1073/pnas.0500495102</pub-id><pub-id pub-id-type="pmid">15795376</pub-id></mixed-citation></ref><ref id="B47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Toyoizumi</surname><given-names>T.</given-names></name><name><surname>Pfister</surname><given-names>J.-P.</given-names></name><name><surname>Aihara</surname><given-names>K.</given-names></name><name><surname>Gerstner</surname><given-names>W.</given-names></name></person-group> (<year>2007</year>). <article-title>Optimality model of unsupervised spike-timing-dependent plasticity: synaptic memory and weight distribution</article-title>. <source>Neural Comput.</source><volume>19</volume>, <fpage>639</fpage><pub-id pub-id-type="doi">10.1162/neco.2007.19.3.639</pub-id><pub-id pub-id-type="pmid">17298228</pub-id></mixed-citation></ref><ref id="B48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tzounopoulos</surname><given-names>T.</given-names></name><name><surname>Kim</surname><given-names>Y.</given-names></name><name><surname>Oertel</surname><given-names>D.</given-names></name><name><surname>Trussell</surname><given-names>L.</given-names></name></person-group> (<year>2004</year>). <article-title>Cell-specific, spike timing-dependent plasticity in the dorsal cochlear nucleus</article-title>. <source>Nat. Neurosci.</source><volume>7</volume>, <fpage>719</fpage>&#x02013;<lpage>725</lpage><pub-id pub-id-type="doi">10.1038/nn1272</pub-id><pub-id pub-id-type="pmid">15208632</pub-id></mixed-citation></ref><ref id="B49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Rossum</surname><given-names>M. C. W.</given-names></name><name><surname>Bi</surname><given-names>G. Q.</given-names></name><name><surname>Turrigiano</surname><given-names>G. G.</given-names></name></person-group> (<year>2000</year>). <article-title>Stable Hebbian learning from spike timing-dependent plasticity</article-title>. <source>J. Neurosci.</source><volume>20</volume>, <fpage>8812</fpage>&#x02013;<lpage>8821</lpage><pub-id pub-id-type="pmid">11102489</pub-id></mixed-citation></ref><ref id="B50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>H.-X.</given-names></name><name><surname>Gerkin</surname><given-names>R. C.</given-names></name><name><surname>Nauen</surname><given-names>D. W.</given-names></name><name><surname>Wang</surname><given-names>G.-Q.</given-names></name></person-group> (<year>2005</year>). <article-title>Coactivation and timing-dependent integration of synaptic potentiation and depression</article-title>. <source>Nat. Neurosci.</source><volume>8</volume>, <fpage>187</fpage>&#x02013;<lpage>193</lpage><pub-id pub-id-type="doi">10.1038/nn1387</pub-id><pub-id pub-id-type="pmid">15657596</pub-id></mixed-citation></ref><ref id="B51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>X.</given-names></name><name><surname>Seung</surname><given-names>H. S.</given-names></name></person-group> (<year>2004</year>). <article-title>Learning in neural networks by reinforcement of irregular spiking</article-title>. <source>Phys. Rev. E</source><volume>69</volume>, <fpage>041909</fpage><pub-id pub-id-type="doi">10.1103/PhysRevE.69.041909</pub-id></mixed-citation></ref><ref id="B52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>L. I.</given-names></name><name><surname>Tao</surname><given-names>H. W.</given-names></name><name><surname>Holt</surname><given-names>C. E.</given-names></name><name><surname>Harris</surname><given-names>W. A.</given-names></name><name><surname>Poo</surname><given-names>M. M.</given-names></name></person-group> (<year>1998</year>). <article-title>A critical window for cooperation and competition among developing retinotectal synapses</article-title>. <source>Nature</source><volume>395</volume>, <fpage>37</fpage>&#x02013;<lpage>44</lpage><pub-id pub-id-type="doi">10.1038/25665</pub-id><pub-id pub-id-type="pmid">9738497</pub-id></mixed-citation></ref></ref-list></back></article>
